[
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "SA-TIED Geospatial Workshop",
    "section": "",
    "text": "Welcome to the workbook for the SA-TIED Geospatial Workshop. Over the course of this two-day introductory workshop, you will be introduced to the R programming language, learn how to map socio-economic and demographic data, and gain an understanding of the ideas behind spatial models. We will explore the following topics:\n\nFundamentals of using R for data analysis with the tidyverse library\nCreating thematic maps using R with the tmap library\nQuantifying the degree of spatial dependence in a dataset\nIncorporating space into statistical models\n\n\n\n\nWhile there are no specific prerequisites for this workshop, basic familiarity with R, Python or Stata do-files is recommended. You will require a computer with admin rights to install the necessary software.\n\n\n\nThe schedule of the workshop is as follows:\n\n\n\nDay\nType\nTime\nDetails\n\n\n\n\n1\nLecture\n09h00-09h45\nLecture Notes #1\n\n\n1\nComputer tutorial\n10h00-12h00\nR for Data Analysis\n\n\n1\nLecture\n13h00-13h45\nLecture Notes #2\n\n\n1\nComputer tutorial\n14h00-16h00\nR for Spatial Analysis\n\n\n\n\n\n\n\n\n2\nLecture\n09h00-09h45\nLecture Notes #3\n\n\n2\nComputer tutorial\n10h00-12h00\nSpatial Autocorrelation\n\n\n2\nLecture\n13h00-13h45\nLecture Notes #4\n\n\n2\nComputer tutorial\n14h00-16h00\nSpatial Models\n\n\n\n\n\n\nThis workbook is created using the Quarto publishing system. The workbook’s content is partially based on content from:\n\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nThe Mapping and Modelling Geographic Data in R course by Richard Harris\n\nThe workbooks contains data sourced from the 2011 South African Census of Population:\n\nStatistics South Africa. South African Census Community Profiles 2011 [dataset]. Version 1. Pretoria: Statistics SA [producer], 2014. Cape Town: DataFirst [distributor], 2015. DOI: https://doi.org/10.25828/6n0m-7m52"
  },
  {
    "objectID": "00-index.html#welcome",
    "href": "00-index.html#welcome",
    "title": "SA-TIED Geospatial Workshop",
    "section": "",
    "text": "Welcome to the workbook for the SA-TIED Geospatial Workshop. Over the course of this two-day introductory workshop, you will be introduced to the R programming language, learn how to map socio-economic and demographic data, and gain an understanding of the ideas behind spatial models. We will explore the following topics:\n\nFundamentals of using R for data analysis with the tidyverse library\nCreating thematic maps using R with the tmap library\nQuantifying the degree of spatial dependence in a dataset\nIncorporating space into statistical models"
  },
  {
    "objectID": "00-index.html#prerequisites",
    "href": "00-index.html#prerequisites",
    "title": "SA-TIED Geospatial Workshop",
    "section": "",
    "text": "While there are no specific prerequisites for this workshop, basic familiarity with R, Python or Stata do-files is recommended. You will require a computer with admin rights to install the necessary software."
  },
  {
    "objectID": "00-index.html#workshop-overview",
    "href": "00-index.html#workshop-overview",
    "title": "SA-TIED Geospatial Workshop",
    "section": "",
    "text": "The schedule of the workshop is as follows:\n\n\n\nDay\nType\nTime\nDetails\n\n\n\n\n1\nLecture\n09h00-09h45\nLecture Notes #1\n\n\n1\nComputer tutorial\n10h00-12h00\nR for Data Analysis\n\n\n1\nLecture\n13h00-13h45\nLecture Notes #2\n\n\n1\nComputer tutorial\n14h00-16h00\nR for Spatial Analysis\n\n\n\n\n\n\n\n\n2\nLecture\n09h00-09h45\nLecture Notes #3\n\n\n2\nComputer tutorial\n10h00-12h00\nSpatial Autocorrelation\n\n\n2\nLecture\n13h00-13h45\nLecture Notes #4\n\n\n2\nComputer tutorial\n14h00-16h00\nSpatial Models"
  },
  {
    "objectID": "00-index.html#acknowledgements",
    "href": "00-index.html#acknowledgements",
    "title": "SA-TIED Geospatial Workshop",
    "section": "",
    "text": "This workbook is created using the Quarto publishing system. The workbook’s content is partially based on content from:\n\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nThe Mapping and Modelling Geographic Data in R course by Richard Harris\n\nThe workbooks contains data sourced from the 2011 South African Census of Population:\n\nStatistics South Africa. South African Census Community Profiles 2011 [dataset]. Version 1. Pretoria: Statistics SA [producer], 2014. Cape Town: DataFirst [distributor], 2015. DOI: https://doi.org/10.25828/6n0m-7m52"
  },
  {
    "objectID": "01-getting-started.html",
    "href": "01-getting-started.html",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "R is a programming language originally designed for conducting statistical analysis and creating graphics. The major advantage of using R is that it can be used on any computer operating system, and is free for anyone to use and contribute to. Because of this, it has rapidly become the statistical language of choice for many academics and has a large user community with people constantly contributing new packages to carry out all manner of statistical, graphical, and importantly for us, geographical tasks.\nInstalling R takes a few relatively simple steps involving two pieces of software. First there is the R programme itself. Follow these steps to get it installed on your computer:\n\nNavigate in your browser to the download page: [Link]\nIf you use a Windows computer, click on Download R for Windows. Then click on base. Download and install R 4.4.x for Windows. If you use a Mac computer, click on Download R for macOS and download and install R-4.4.x.arm64.pkg for Apple silicon Macs and R-4.4.x.x86_64.pkg for older Intel-based Macs.\n\nThat is it! You now have successfully installed R onto your computer. To make working with the R language a little bit easier we also need to install something called an Integrated Development Environment (IDE). We will use RStudio Desktop:\n\nNavigate to the official webpage of RStudio: [Link]\nDownload and install RStudio on your computer.\n\nAfter this, start RStudio to see if the installation was successful. Your screen should something that looks like what is shown in Figure 1.\n\n\n\n\n\nFigure 1: The RStudio interface.\n\n\n\n\nThe main windows that we will be using are:\n\n\n\n\n\n\n\nWindow\nPurpose\n\n\n\n\nConsole\nWhere we write one-off code such as installing packages.\n\n\nFiles\nWhere we can see where our files are stored on our computer system.\n\n\nEnvironment\nWhere our variables or objects are kept in memory.\n\n\nPlots\nWhere the outputs of our graphs, charts and maps are shown.\n\n\n\n\n\n\nNow we have installed R and RStudio, we need to customise R. Many useful R functions come in packages, these are free libraries of code written and made available by other R users. This includes packages specifically developed for data cleaning, data wrangling, visualisation, mapping, and spatial analysis. To save us some time, we will install all R packages that we will need for the workshop in one go. Start RStudio, and copy and paste the following code into the console window. You can execute the code by pressing the Return button on your keyboard. Depending on your computer’s specifications and the internet connection, this may take a short while.\n\n\n\nR code\n\n# install packages\ninstall.packages(c(\"tidyverse\", \"haven\", \"sf\", \"tmap\", \"spdep\"))\n\n\n\n\n\n\n\n\nFor Linux and macOS users who are new to working with spatial data in R, this installation of some of these libraries may fail since additional (non-R) libraries are required (which are automatically installed for Windows users). If this is the case, please refer to the information pages of the sf library for instructions.\n\n\n\nOnce you have installed the packages, we need to check whether we can in fact load them into R. Copy and paste the following code into the console, and execute by pressing Return on your keyboard again.\n\n\n\nR code\n\n# load packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\n\n\nYou will see some information printed to your console but as long as you do not get any of the messages below, the installation was successful. If you do get any of the messages below it means that the package was not properly installed, so try to install the package in question again.\n\nError: package or namespace load failed for &lt;packagename&gt;\nError: package '&lt;packagename&gt;' could not be loaded\nError in library(&lt;packagename&gt;) : there is no package called '&lt;packagename&gt;'\n\n\n\n\n\n\n\nMany packages require additional software components, known as dependencies, to function properly. Occasionally, when you install a package, some of these dependencies might not be installed automatically. If you encounter errors related to a package that you did not explicitly install, it is likely due to a missing dependency. To resolve this, identify the missing dependency and install it using the command install.packages('&lt;dependencyname&gt;'). Afterward, try loading your packages again.\n\n\n\n\n\n\nUnlike traditional statistical analysis software like Microsoft Excel or Stata, which often rely on point-and-click interfaces, R requires users to input commands to perform tasks such as loading datasets and fitting models. This command-based approach is typically done by writing scripts, which not only document your workflow but also allow for easy repetition of tasks.\n\n\n\n\n\n\nIf you are familiar with Stata’s do-file you will find that the approach to programming in R is comparable.\n\n\n\nLet us begin by exploring some of R’s built-in functionality through a simple exercise: creating a few variables and performing basic calculations. Open your console, and let us start with some quick math. At its core, every programming language can be used as a powerful calculator.\n\n\n\n\n\n\nIn your RStudio console, you will notice a prompt sign &gt; on the left-hand side. This is where you can directly interact with R. If any text appears in red, it indicates an error or warning. When you see the &gt;, it means R is ready for your next command. However, if you see a +, it means you have not completed the previous line of code. This often happens when brackets are left open or a command is not properly finished as R expected.\n\n\n\nType in 10 * 12 into the console and execute.\n\n\n\nR code\n\n# multiplication\n10 * 12\n\n\n[1] 120\n\n\nOnce you press return, you should see the answer of 120 returned below.\n\n\nInstead of using raw numbers or standalone values, it is more effective to store these values in variables, which allows for easy reference later. In R, this process is known as creating an object, and the object is stored as a variable. To assign a value to a variable, use the &lt;- symbol. Let us create two variables to experiment with this concept.\nType in ten &lt;- 10 into the console and execute.\n\n\n\nR code\n\n# store a variable\nten &lt;- 10\n\n\nYou will see nothing is returned in the console. However, if you check your environment window, you will see that a new variable has appeared, containing the value you assigned.\nType in twelve &lt;- 12 into the console and execute.\n\n\n\nR code\n\n# store a variable\ntwelve &lt;- 12\n\n\nAgain, nothing will be returned to the console, but be sure to check your environment window for the newly created variable. We have now stored two numbers in our environment and assigned them variable names for easy reference. R stores these objects as variables in your computer’s RAM memory, allowing for quick processing. Keep in mind that without saving your environment, these variables will be lost when you close R. Now that we have our variables, let us proceed with a simple multiplication:\nType in ten * twelve into the console and execute.\n\n\n\nR code\n\n# using variables\nten * twelve\n\n\n[1] 120\n\n\nYou should see the output in the console of 120. While this calculation may seem trivial, it demonstrates a powerful concept: these variables can be treated just like the values they contain.\nNext, type in ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# using variables and values\nten * twelve * 8\n\n\n[1] 960\n\n\nYou should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable.\nType output &lt;- ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# store output\noutput &lt;- ten * twelve * 8\n\n\nBecause we are storing the output of our maths to a new variable, the answer is not returned to the screen but is kept in memory.\n\n\n\nWe can ask R to return the value of the output variable by simply typing its name into the console. You should see that it returns the same value as the earlier calculation.\n\n\n\nR code\n\n# return value\noutput\n\n\n[1] 960\n\n\n\n\n\nWe can also store variables of different data types, not just numbers but text as well.\nType in str_variable &lt;- \"Hello Pretoria\" into the console and execute.\n\n\n\nR code\n\n# store a variable\nstr_variable &lt;- \"Hello Pretoria\"\n\n\nWe have just stored our sentence made from a combination of characters. A variable that stores text is known as a string. A string is always denoted by the use of single ('') or double (\"\") quotation marks.\nType in str_variable into the console and execute.\n\n\n\nR code\n\n# return variable\nstr_variable\n\n\n[1] \"Hello Pretoria\"\n\n\nYou should see our entire sentence returned, enclosed in quotation marks (\"\").\n\n\n\nWe can also call a function on our variable. For example, we can ask R to print our variable, which will give us the same output as accessing it directly via the console.\nType in print(str_variable) into the console and execute.\n\n\n\nR code\n\n# printing a variable\nprint(str_variable)\n\n\n[1] \"Hello Pretoria\"\n\n\nYou can type ?print into the console to learn more about the print() function. This method works with any function, giving you access to its documentation. Understanding this documentation is crucial for using the function correctly and interpreting its output.\n\n\n\nR code\n\n# open documentation of the print function\n?print\n\n\n\n\n\n\n\n\nIn many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one required argument although most functions will also have several optional or default parameters.\n\n\n\n\n\n\nWithin the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is.\nType in typeof(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(str_variable)\n\n\n[1] \"character\"\n\n\nYou should see the answer: character. As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables too.\nType in typeof(ten) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(ten)\n\n\n[1] \"double\"\n\n\nYou should see the answer: double. Alternatively, we can check the class of a variable by using the class() function.\nType in class(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(str_variable)\n\n\n[1] \"character\"\n\n\nIn this case, you will see the same result as before because, in R, both the class and type of a string are character. Other programming languages might use the term string instead, but it essentially means the same thing.\nType in class(ten) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(ten)\n\n\n[1] \"numeric\"\n\n\nIn this case, you will get a different result because the class of this variable is numeric. Numeric objects in R can be either doubles (decimals) or integers (whole numbers). You can test whether the ten variable is an integer by using specific functions designed for this purpose.\nType in is.integer(ten) into the console and execute.\n\n\n\nR code\n\n# call the integer() function\nis.integer(ten)\n\n\n[1] FALSE\n\n\nYou should see the result FALSE. As we know from the typeof() function, the ten variable is stored as a double, so it cannot be an integer.\n\n\n\n\n\n\nWhilst knowing how to distinguish between different data types might not seem important now, the difference betwee a double and an integer can quite easily lead to unexpected errors.\n\n\n\nWe can also check the length of our a variable.\nType in length(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the length() function\nlength(str_variable)\n\n\n[1] 1\n\n\nYou should get the answer 1 because we only have one set of characters. We can also determine the length of each set of characters, which tells us the length of the string contained in the variable.\nType in nchar(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the nchar() function\nnchar(str_variable)\n\n\n[1] 14\n\n\nYou should get an answer of 14.\n\n\n\nVariables are not constricted to one value, but can be combined to create larger objects. Type in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute.\n\n\n\nR code\n\n# store a new variable\ntwo_str_variable &lt;- c(\"This is our second string variable\", \"It has two parts to it\")\n\n\nIn this code, we have created a new variable using the c() function, which combines values into a vector or list. We provided the c() function with two sets of strings, separated by a comma and enclosed within the function’s parentheses.\nLet us now try both our length() and nchar() on our new variable and see what the results are:\n\n\n\nR code\n\n# call the length() function\nlength(two_str_variable)\n\n\n[1] 2\n\n# call the nchar() function\nnchar(two_str_variable)\n\n[1] 34 22\n\n\nYou should notice that the length() function now returned a 2 and the nchar() function returned two values of 34 and 22.\n\n\n\n\n\n\nYou may have noticed that each line of code in the examples includes a comment explaining its purpose. In R, comments are created using the hash symbol #. This symbol instructs R to ignore the commented line when executing the code. Comments are useful for understanding your code when you revisit it later or when sharing it with others.\n\n\n\nExtending the concept of multi-value objects to two dimensions results in a dataframe. A dataframe is the de facto data structure for most tabular data. We will use functions from the tidyverse library, a suite of packages—to load a data file and conduct exploratory data analysis. Within the tidyverse, dataframes are referred to as tibbles. Some of the most important and useful functions come from the tidyr and dplyr packages, including:\n\n\n\n\n\n\n\n\nPackage\nFunction\nUse to\n\n\n\n\ndplyr\nselect()\nselect columns\n\n\ndplyr\nfilter()\nselect rows\n\n\ndplyr\nmutate()\ntransform or recode variables\n\n\ndplyr\nsummarise()\nsummarise data\n\n\ndplyr\ngroup_by()\ngroup data into subgroups for further processing\n\n\ntidyr\npivot_longer()\nconvert data from wide format to long format\n\n\ntidyr\npivot_wider()\nconvert long format dataset to wide format\n\n\n\n\n\n\n\n\n\nFor more information on the tidyverse you can refer to www.tidyverse.org.\n\n\n\n\n\n\n\nIn RStudio, scripts allow us to build and save code that can be run repeatedly. We can organise these scripts into RStudio projects, which consolidate all files related to an analysis such as input data, R scripts, results, figures, and more. This organisation helps keep track of all data, input, and output, while enabling us to create standalone scripts for each part of our analysis. Additionally, it simplifies managing directories and filepaths.\nNavigate to File -&gt; New Project -&gt; New Directory, and create a folder with the name GeoSpatial-Workshop24. Click on Create Project. You should now see your main window switch to this new project and when you check your files window, you should see a new R Project called GeoSpatial-Workshop24.\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore _ or hyphen - if you like.\n\n\n\n\n\n\nWith the basics covered, let us dive into loading a real dataset, performing data cleaning, and conducting some exploratory data analysis on the isiXhosa speaking population. We will work with a dataset that contains counts of the primary languages spoken in South African, sourced from the South African Census Community Profiles 2011 and made available through the DataFirst data service. You can download a copy of the file through the link below and save it in your project folder under data/attributes.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 Language Table\n.dta\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Create a folder named scripts, and save your script as 01-language-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(haven)\n\n\n\n\n\n\n\n\nIn RStudio, there are two primary ways to run a script: all at once or by executing individual lines or chunks of code. As a beginner, it is often beneficial to use the line-by-line approach, as it allows you to test your code interactively and catch errors early.\nTo run line-by-line:\n\nBy clicking: Highlight the line or chunk of code you want to run, then go to Code and select Run selected lines.\nBy key commands: Highlight the code, then press Ctl (or Cmd on Mac) + Return.\n\nTo run the whole script:\n\nBy clicking: In the scripting window, click Run in the top-right corner and choose Run All.\nBy key commands: Press Option + Ctrl (or Cmd) + R.\n\nIf a script gets stuck or you realise there is an error in your code, you may need to interrupt R. To do this, go to Session -&gt; Interrupt R. If the interruption doesn’t work, you might need to terminate and restart R.\n\n\n\n\n\nNext, we can load the sa-language.dta file into R. Using the haven library, R is capable to read a large range of different filetypes. This includes Stata .dta files:\n\n\n\nR code\n\n# load data\natt &lt;- read_dta(\"data/attributes/sa-language.dta\")\n\n\n\n\n\n\n\n\nIf using a Windows machine, you may need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nLet us have a look at the dataframe:\n\n\n\nR code\n\n# inspect columns, rows\nncol(att)\n\n\n[1] 28\n\nnrow(att)\n\n[1] 84908\n\n# inspect data\nhead(att)\n\n# A tibble: 6 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1600001 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n2  1600002 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n3  1600003 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n4  1600004 160010001 Vredenda…  160010 Vreden… WC011        160 Matzik… DC1     \n5  1600005 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n6  1600006 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n# inspect column names\nnames(att)\n\n [1] \"sal_code\" \"sp_code\"  \"sp_name\"  \"mp_code\"  \"mp_name\"  \"mn_mdb_c\"\n [7] \"mn_code\"  \"mn_name\"  \"dc_mdb_c\" \"dc_code\"  \"dc_name\"  \"pr_code\" \n[13] \"pr_name\"  \"lng_1\"    \"lng_2\"    \"lng_3\"    \"lng_4\"    \"lng_5\"   \n[19] \"lng_6\"    \"lng_7\"    \"lng_8\"    \"lng_9\"    \"lng_10\"   \"lng_11\"  \n[25] \"lng_12\"   \"lng_13\"   \"lng_14\"   \"lng_15\"  \n\n\nTo access specific columns or rows in a dataframe, we can use indexing. Indexing refers to the numbering assigned to each element within a data structure, allowing us to precisely select and manipulate data.\nTo access the first row of a dataframe, you would use dataframe[1, ], and to access the first column, you would use dataframe[, 1]. The comma separates the row and column indices, with the absence of a number indicating all rows or columns respectively.\n\n\n\n\n\n\nIn R, indexing begins at 1, meaning that the first element of any data structure is accessed with the index [1]. This is different from many other programming languages, such as Python or Java, where indexing typically starts at 0.\n\n\n\n\n\n\nR code\n\n# index columns\natt[, 1]\n\n\n# A tibble: 84,908 × 1\n   sal_code\n      &lt;dbl&gt;\n 1  1600001\n 2  1600002\n 3  1600003\n 4  1600004\n 5  1600005\n 6  1600006\n 7  1600007\n 8  1600008\n 9  1600009\n10  1600010\n# ℹ 84,898 more rows\n\n# index rows\natt[1, ]\n\n# A tibble: 1 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1600001 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n# index cell\natt[1, 1]\n\n# A tibble: 1 × 1\n  sal_code\n     &lt;dbl&gt;\n1  1600001\n\n\n\n\n\n\n\n\nAlternatively, you can access the data within individual columns by referring to their names using the $ operator. This allows you to easily extract and work with a specific column without needing to know its position in the dataframe. For example, if your dataframe is named dataframe and you want to access a column named age, you would use dataframe$age. This method is especially useful when your data has many columns or when the column positions may change, because it relies on the column names rather than their index numbers.\n\n\n\n\n\n\nNow that we have loaded and inspected our data, we can examine its distribution. We will focus on the column that contains the counts of individuals who speak isiXhosa as their primary language, aggregated by Small Area Layer — the most granular geographic level available in the 2011 South African Census of Population. This data is stored in the lng_4 column of the dataframe.\n\n\n\nR code\n\n# mean\nmean(att$lng_4)\n\n\n[1] 95.99431\n\n# median\nmedian(att$lng_4)\n\n[1] 9\n\n# range\nrange(att$lng_4)\n\n[1]    0 6465\n\n# summary\nsummary(att$lng_4)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    9.00   95.99   72.00 6465.00 \n\n\nWe can also call some functions to quickly draw a boxplot, histogram, or scatterplot:\n\n\n\nR code\n\n# boxplot\nboxplot(att$lng_4, horizontal = TRUE)\n\n\n\n\n\nFigure 2: Quick boxplot.\n\n\n\n\n\n\n\nR code\n\n# histogram\nhist(att$lng_4, breaks = 50, xlab = \"Number of Small Area Layers\", main = \"Number of isiXhosa speakers\")\n\n\n\n\n\nFigure 3: Quick histogram.\n\n\n\n\n\n\n\nR code\n\n# bivariate plot\nplot(att$lng_4, att$lng_2, xlab = \"isiXhosa\", ylab = \"English\")\n\n\n\n\n\nFigure 4: Quick bivariate plot.\n\n\n\n\n\n\n\nThe dataset is quite extensive, containing counts for all 84,908 Small Area Layers. To make our analysis more manageable, let’s focus on a subset of the data, specifically zooming in on Cape Town. We can achieve this by filtering the data using the dc_name column, which corresponds to the District Municipality Name.\n\n\n\nR code\n\n# prepare data: filter out Cape Town\natt &lt;- att |&gt;\n    filter(dc_name == \"City of Cape Town\")\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might seem a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\nIn this case, we have overwritten our initial object, though this is not strictly necessary. Our object now contains a subset of the full language dataset that we initially loaded.\n\n\n\nR code\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1990001 199014025 Paarden …  199014 Milner… CPT          199 City o… CPT     \n2  1990002 199016089 Mimosa     199016 Bellvi… CPT          199 City o… CPT     \n3  1990003 199041008 Signal H…  199041 Cape T… CPT          199 City o… CPT     \n4  1990004 199017021 Durbanvi…  199017 Durban… CPT          199 City o… CPT     \n5  1990005 199053012 Pine Hav…  199053 Simon'… CPT          199 City o… CPT     \n6  1990006 199030007 Guguleth…  199030 Gugule… CPT          199 City o… CPT     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n\nOur code appears to have worked successfully.\n\n\n\nThe next thing that we probably want to do is standardise our data. A common challenge with spatial data is that the spatial units aren not always the same size — some Small Area Layers may have larger populations, while others have fewer residents. This makes it difficult to compare absolute counts fairly. To address this, we can sum the counts across all relevant columns to determine the total number of speakers in each area. Then, we can calculate the proportion of isiXhosa speakers within each Small Area Layer:\n\n\n\nR code\n\n# prepare data: sum across\natt &lt;- att |&gt;\n    rowwise() |&gt;\n    mutate(sal_pop = sum(across(starts_with(\"lng\")), na.rm = TRUE))\n\n# prepare data: calculate percentages\natt &lt;- att |&gt;\n    mutate(sal_prop_xhosa = lng_4/sal_pop)\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 30\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1990001 199014025 Paarden …  199014 Milner… CPT          199 City o… CPT     \n2  1990002 199016089 Mimosa     199016 Bellvi… CPT          199 City o… CPT     \n3  1990003 199041008 Signal H…  199041 Cape T… CPT          199 City o… CPT     \n4  1990004 199017021 Durbanvi…  199017 Durban… CPT          199 City o… CPT     \n5  1990005 199053012 Pine Hav…  199053 Simon'… CPT          199 City o… CPT     \n6  1990006 199030007 Guguleth…  199030 Gugule… CPT          199 City o… CPT     \n# ℹ 21 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;, sal_pop &lt;dbl&gt;, sal_prop_xhosa &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe now have the percentage of people who speak isiXhosa as their primary language for each Small Area Layer. To provide a more meaningful representation, however, we can aggregate the data by sub-places. This involves grouping our data by sp_code, which contains unique sub-place codes. We will then:\n\nSum the number of isiXhosa speakers within each sub-place and store this in a new variable.\nSum the total number of people within each sub-place and store this in another new variable.\nExtract the distinct values for each sub-place.\n\nBy using the pipe function again, we can chain these steps together efficiently as follows:\n\n\n\nR code\n\n# prepare data: aggregate small areas to sub places\natt &lt;- att |&gt;\n    group_by(sp_code) |&gt;\n    mutate(sp_pop = sum(sal_pop)) |&gt;\n    mutate(sp_xhosa = sum(lng_4)) |&gt;\n    ungroup() |&gt;\n    distinct(sp_code, sp_pop, sp_xhosa)\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 3\n    sp_code sp_pop sp_xhosa\n      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 199014025     12        0\n2 199016089   1212        0\n3 199041008     12        0\n4 199017021   7893      105\n5 199053012      9        0\n6 199030007  55242    47385\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can save this dataset so that we can easily load it the next time we want to work with this by writing it to a .csv file.\n\n\n\nR code\n\n# write data\nwrite_csv(x = att, file = \"data/attributes/sa-language.csv\")\n\n\n\n\n\n\nThis concludes this session. Please try to complete the following tasks:\n\nCreate a histogram showing the distribution of the number of isiXhosa speakers in Cape Town, grouped by sub-place.\nCreate a boxplot of the proportion of isiXhosa-speakers in Cape Town, grouped by sub place.\n\nNext, use the link below to download another dataset sourced from the South African Census Community Profiles 2011 that contains information on internet access. Save the file in your project folder under data/attributes.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 Internet Access Table\n.dta\nDownload\n\n\n\nUse this dataset to:\n\nCreate a histogram of the distribution of the number of people with internet access at home in the City of Johannesburg, grouped by sub-place.\nCreate a boxplot of the proportion of people with internet access at home in the City of Johannesburg, grouped by sub-place."
  },
  {
    "objectID": "01-getting-started.html#installation-of-r",
    "href": "01-getting-started.html#installation-of-r",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "R is a programming language originally designed for conducting statistical analysis and creating graphics. The major advantage of using R is that it can be used on any computer operating system, and is free for anyone to use and contribute to. Because of this, it has rapidly become the statistical language of choice for many academics and has a large user community with people constantly contributing new packages to carry out all manner of statistical, graphical, and importantly for us, geographical tasks.\nInstalling R takes a few relatively simple steps involving two pieces of software. First there is the R programme itself. Follow these steps to get it installed on your computer:\n\nNavigate in your browser to the download page: [Link]\nIf you use a Windows computer, click on Download R for Windows. Then click on base. Download and install R 4.4.x for Windows. If you use a Mac computer, click on Download R for macOS and download and install R-4.4.x.arm64.pkg for Apple silicon Macs and R-4.4.x.x86_64.pkg for older Intel-based Macs.\n\nThat is it! You now have successfully installed R onto your computer. To make working with the R language a little bit easier we also need to install something called an Integrated Development Environment (IDE). We will use RStudio Desktop:\n\nNavigate to the official webpage of RStudio: [Link]\nDownload and install RStudio on your computer.\n\nAfter this, start RStudio to see if the installation was successful. Your screen should something that looks like what is shown in Figure 1.\n\n\n\n\n\nFigure 1: The RStudio interface.\n\n\n\n\nThe main windows that we will be using are:\n\n\n\n\n\n\n\nWindow\nPurpose\n\n\n\n\nConsole\nWhere we write one-off code such as installing packages.\n\n\nFiles\nWhere we can see where our files are stored on our computer system.\n\n\nEnvironment\nWhere our variables or objects are kept in memory.\n\n\nPlots\nWhere the outputs of our graphs, charts and maps are shown."
  },
  {
    "objectID": "01-getting-started.html#customisation-of-r",
    "href": "01-getting-started.html#customisation-of-r",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "Now we have installed R and RStudio, we need to customise R. Many useful R functions come in packages, these are free libraries of code written and made available by other R users. This includes packages specifically developed for data cleaning, data wrangling, visualisation, mapping, and spatial analysis. To save us some time, we will install all R packages that we will need for the workshop in one go. Start RStudio, and copy and paste the following code into the console window. You can execute the code by pressing the Return button on your keyboard. Depending on your computer’s specifications and the internet connection, this may take a short while.\n\n\n\nR code\n\n# install packages\ninstall.packages(c(\"tidyverse\", \"haven\", \"sf\", \"tmap\", \"spdep\"))\n\n\n\n\n\n\n\n\nFor Linux and macOS users who are new to working with spatial data in R, this installation of some of these libraries may fail since additional (non-R) libraries are required (which are automatically installed for Windows users). If this is the case, please refer to the information pages of the sf library for instructions.\n\n\n\nOnce you have installed the packages, we need to check whether we can in fact load them into R. Copy and paste the following code into the console, and execute by pressing Return on your keyboard again.\n\n\n\nR code\n\n# load packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\n\n\nYou will see some information printed to your console but as long as you do not get any of the messages below, the installation was successful. If you do get any of the messages below it means that the package was not properly installed, so try to install the package in question again.\n\nError: package or namespace load failed for &lt;packagename&gt;\nError: package '&lt;packagename&gt;' could not be loaded\nError in library(&lt;packagename&gt;) : there is no package called '&lt;packagename&gt;'\n\n\n\n\n\n\n\nMany packages require additional software components, known as dependencies, to function properly. Occasionally, when you install a package, some of these dependencies might not be installed automatically. If you encounter errors related to a package that you did not explicitly install, it is likely due to a missing dependency. To resolve this, identify the missing dependency and install it using the command install.packages('&lt;dependencyname&gt;'). Afterward, try loading your packages again."
  },
  {
    "objectID": "01-getting-started.html#getting-started-with-r",
    "href": "01-getting-started.html#getting-started-with-r",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "Unlike traditional statistical analysis software like Microsoft Excel or Stata, which often rely on point-and-click interfaces, R requires users to input commands to perform tasks such as loading datasets and fitting models. This command-based approach is typically done by writing scripts, which not only document your workflow but also allow for easy repetition of tasks.\n\n\n\n\n\n\nIf you are familiar with Stata’s do-file you will find that the approach to programming in R is comparable.\n\n\n\nLet us begin by exploring some of R’s built-in functionality through a simple exercise: creating a few variables and performing basic calculations. Open your console, and let us start with some quick math. At its core, every programming language can be used as a powerful calculator.\n\n\n\n\n\n\nIn your RStudio console, you will notice a prompt sign &gt; on the left-hand side. This is where you can directly interact with R. If any text appears in red, it indicates an error or warning. When you see the &gt;, it means R is ready for your next command. However, if you see a +, it means you have not completed the previous line of code. This often happens when brackets are left open or a command is not properly finished as R expected.\n\n\n\nType in 10 * 12 into the console and execute.\n\n\n\nR code\n\n# multiplication\n10 * 12\n\n\n[1] 120\n\n\nOnce you press return, you should see the answer of 120 returned below.\n\n\nInstead of using raw numbers or standalone values, it is more effective to store these values in variables, which allows for easy reference later. In R, this process is known as creating an object, and the object is stored as a variable. To assign a value to a variable, use the &lt;- symbol. Let us create two variables to experiment with this concept.\nType in ten &lt;- 10 into the console and execute.\n\n\n\nR code\n\n# store a variable\nten &lt;- 10\n\n\nYou will see nothing is returned in the console. However, if you check your environment window, you will see that a new variable has appeared, containing the value you assigned.\nType in twelve &lt;- 12 into the console and execute.\n\n\n\nR code\n\n# store a variable\ntwelve &lt;- 12\n\n\nAgain, nothing will be returned to the console, but be sure to check your environment window for the newly created variable. We have now stored two numbers in our environment and assigned them variable names for easy reference. R stores these objects as variables in your computer’s RAM memory, allowing for quick processing. Keep in mind that without saving your environment, these variables will be lost when you close R. Now that we have our variables, let us proceed with a simple multiplication:\nType in ten * twelve into the console and execute.\n\n\n\nR code\n\n# using variables\nten * twelve\n\n\n[1] 120\n\n\nYou should see the output in the console of 120. While this calculation may seem trivial, it demonstrates a powerful concept: these variables can be treated just like the values they contain.\nNext, type in ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# using variables and values\nten * twelve * 8\n\n\n[1] 960\n\n\nYou should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable.\nType output &lt;- ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# store output\noutput &lt;- ten * twelve * 8\n\n\nBecause we are storing the output of our maths to a new variable, the answer is not returned to the screen but is kept in memory.\n\n\n\nWe can ask R to return the value of the output variable by simply typing its name into the console. You should see that it returns the same value as the earlier calculation.\n\n\n\nR code\n\n# return value\noutput\n\n\n[1] 960\n\n\n\n\n\nWe can also store variables of different data types, not just numbers but text as well.\nType in str_variable &lt;- \"Hello Pretoria\" into the console and execute.\n\n\n\nR code\n\n# store a variable\nstr_variable &lt;- \"Hello Pretoria\"\n\n\nWe have just stored our sentence made from a combination of characters. A variable that stores text is known as a string. A string is always denoted by the use of single ('') or double (\"\") quotation marks.\nType in str_variable into the console and execute.\n\n\n\nR code\n\n# return variable\nstr_variable\n\n\n[1] \"Hello Pretoria\"\n\n\nYou should see our entire sentence returned, enclosed in quotation marks (\"\").\n\n\n\nWe can also call a function on our variable. For example, we can ask R to print our variable, which will give us the same output as accessing it directly via the console.\nType in print(str_variable) into the console and execute.\n\n\n\nR code\n\n# printing a variable\nprint(str_variable)\n\n\n[1] \"Hello Pretoria\"\n\n\nYou can type ?print into the console to learn more about the print() function. This method works with any function, giving you access to its documentation. Understanding this documentation is crucial for using the function correctly and interpreting its output.\n\n\n\nR code\n\n# open documentation of the print function\n?print\n\n\n\n\n\n\n\n\nIn many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one required argument although most functions will also have several optional or default parameters.\n\n\n\n\n\n\nWithin the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is.\nType in typeof(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(str_variable)\n\n\n[1] \"character\"\n\n\nYou should see the answer: character. As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables too.\nType in typeof(ten) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(ten)\n\n\n[1] \"double\"\n\n\nYou should see the answer: double. Alternatively, we can check the class of a variable by using the class() function.\nType in class(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(str_variable)\n\n\n[1] \"character\"\n\n\nIn this case, you will see the same result as before because, in R, both the class and type of a string are character. Other programming languages might use the term string instead, but it essentially means the same thing.\nType in class(ten) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(ten)\n\n\n[1] \"numeric\"\n\n\nIn this case, you will get a different result because the class of this variable is numeric. Numeric objects in R can be either doubles (decimals) or integers (whole numbers). You can test whether the ten variable is an integer by using specific functions designed for this purpose.\nType in is.integer(ten) into the console and execute.\n\n\n\nR code\n\n# call the integer() function\nis.integer(ten)\n\n\n[1] FALSE\n\n\nYou should see the result FALSE. As we know from the typeof() function, the ten variable is stored as a double, so it cannot be an integer.\n\n\n\n\n\n\nWhilst knowing how to distinguish between different data types might not seem important now, the difference betwee a double and an integer can quite easily lead to unexpected errors.\n\n\n\nWe can also check the length of our a variable.\nType in length(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the length() function\nlength(str_variable)\n\n\n[1] 1\n\n\nYou should get the answer 1 because we only have one set of characters. We can also determine the length of each set of characters, which tells us the length of the string contained in the variable.\nType in nchar(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the nchar() function\nnchar(str_variable)\n\n\n[1] 14\n\n\nYou should get an answer of 14.\n\n\n\nVariables are not constricted to one value, but can be combined to create larger objects. Type in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute.\n\n\n\nR code\n\n# store a new variable\ntwo_str_variable &lt;- c(\"This is our second string variable\", \"It has two parts to it\")\n\n\nIn this code, we have created a new variable using the c() function, which combines values into a vector or list. We provided the c() function with two sets of strings, separated by a comma and enclosed within the function’s parentheses.\nLet us now try both our length() and nchar() on our new variable and see what the results are:\n\n\n\nR code\n\n# call the length() function\nlength(two_str_variable)\n\n\n[1] 2\n\n# call the nchar() function\nnchar(two_str_variable)\n\n[1] 34 22\n\n\nYou should notice that the length() function now returned a 2 and the nchar() function returned two values of 34 and 22.\n\n\n\n\n\n\nYou may have noticed that each line of code in the examples includes a comment explaining its purpose. In R, comments are created using the hash symbol #. This symbol instructs R to ignore the commented line when executing the code. Comments are useful for understanding your code when you revisit it later or when sharing it with others.\n\n\n\nExtending the concept of multi-value objects to two dimensions results in a dataframe. A dataframe is the de facto data structure for most tabular data. We will use functions from the tidyverse library, a suite of packages—to load a data file and conduct exploratory data analysis. Within the tidyverse, dataframes are referred to as tibbles. Some of the most important and useful functions come from the tidyr and dplyr packages, including:\n\n\n\n\n\n\n\n\nPackage\nFunction\nUse to\n\n\n\n\ndplyr\nselect()\nselect columns\n\n\ndplyr\nfilter()\nselect rows\n\n\ndplyr\nmutate()\ntransform or recode variables\n\n\ndplyr\nsummarise()\nsummarise data\n\n\ndplyr\ngroup_by()\ngroup data into subgroups for further processing\n\n\ntidyr\npivot_longer()\nconvert data from wide format to long format\n\n\ntidyr\npivot_wider()\nconvert long format dataset to wide format\n\n\n\n\n\n\n\n\n\nFor more information on the tidyverse you can refer to www.tidyverse.org."
  },
  {
    "objectID": "01-getting-started.html#rstudio-projects",
    "href": "01-getting-started.html#rstudio-projects",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "In RStudio, scripts allow us to build and save code that can be run repeatedly. We can organise these scripts into RStudio projects, which consolidate all files related to an analysis such as input data, R scripts, results, figures, and more. This organisation helps keep track of all data, input, and output, while enabling us to create standalone scripts for each part of our analysis. Additionally, it simplifies managing directories and filepaths.\nNavigate to File -&gt; New Project -&gt; New Directory, and create a folder with the name GeoSpatial-Workshop24. Click on Create Project. You should now see your main window switch to this new project and when you check your files window, you should see a new R Project called GeoSpatial-Workshop24.\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore _ or hyphen - if you like."
  },
  {
    "objectID": "01-getting-started.html#bathetha-isixhosa",
    "href": "01-getting-started.html#bathetha-isixhosa",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "With the basics covered, let us dive into loading a real dataset, performing data cleaning, and conducting some exploratory data analysis on the isiXhosa speaking population. We will work with a dataset that contains counts of the primary languages spoken in South African, sourced from the South African Census Community Profiles 2011 and made available through the DataFirst data service. You can download a copy of the file through the link below and save it in your project folder under data/attributes.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 Language Table\n.dta\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Create a folder named scripts, and save your script as 01-language-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(haven)\n\n\n\n\n\n\n\n\nIn RStudio, there are two primary ways to run a script: all at once or by executing individual lines or chunks of code. As a beginner, it is often beneficial to use the line-by-line approach, as it allows you to test your code interactively and catch errors early.\nTo run line-by-line:\n\nBy clicking: Highlight the line or chunk of code you want to run, then go to Code and select Run selected lines.\nBy key commands: Highlight the code, then press Ctl (or Cmd on Mac) + Return.\n\nTo run the whole script:\n\nBy clicking: In the scripting window, click Run in the top-right corner and choose Run All.\nBy key commands: Press Option + Ctrl (or Cmd) + R.\n\nIf a script gets stuck or you realise there is an error in your code, you may need to interrupt R. To do this, go to Session -&gt; Interrupt R. If the interruption doesn’t work, you might need to terminate and restart R.\n\n\n\n\n\nNext, we can load the sa-language.dta file into R. Using the haven library, R is capable to read a large range of different filetypes. This includes Stata .dta files:\n\n\n\nR code\n\n# load data\natt &lt;- read_dta(\"data/attributes/sa-language.dta\")\n\n\n\n\n\n\n\n\nIf using a Windows machine, you may need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nLet us have a look at the dataframe:\n\n\n\nR code\n\n# inspect columns, rows\nncol(att)\n\n\n[1] 28\n\nnrow(att)\n\n[1] 84908\n\n# inspect data\nhead(att)\n\n# A tibble: 6 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1600001 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n2  1600002 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n3  1600003 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n4  1600004 160010001 Vredenda…  160010 Vreden… WC011        160 Matzik… DC1     \n5  1600005 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n6  1600006 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n# inspect column names\nnames(att)\n\n [1] \"sal_code\" \"sp_code\"  \"sp_name\"  \"mp_code\"  \"mp_name\"  \"mn_mdb_c\"\n [7] \"mn_code\"  \"mn_name\"  \"dc_mdb_c\" \"dc_code\"  \"dc_name\"  \"pr_code\" \n[13] \"pr_name\"  \"lng_1\"    \"lng_2\"    \"lng_3\"    \"lng_4\"    \"lng_5\"   \n[19] \"lng_6\"    \"lng_7\"    \"lng_8\"    \"lng_9\"    \"lng_10\"   \"lng_11\"  \n[25] \"lng_12\"   \"lng_13\"   \"lng_14\"   \"lng_15\"  \n\n\nTo access specific columns or rows in a dataframe, we can use indexing. Indexing refers to the numbering assigned to each element within a data structure, allowing us to precisely select and manipulate data.\nTo access the first row of a dataframe, you would use dataframe[1, ], and to access the first column, you would use dataframe[, 1]. The comma separates the row and column indices, with the absence of a number indicating all rows or columns respectively.\n\n\n\n\n\n\nIn R, indexing begins at 1, meaning that the first element of any data structure is accessed with the index [1]. This is different from many other programming languages, such as Python or Java, where indexing typically starts at 0.\n\n\n\n\n\n\nR code\n\n# index columns\natt[, 1]\n\n\n# A tibble: 84,908 × 1\n   sal_code\n      &lt;dbl&gt;\n 1  1600001\n 2  1600002\n 3  1600003\n 4  1600004\n 5  1600005\n 6  1600006\n 7  1600007\n 8  1600008\n 9  1600009\n10  1600010\n# ℹ 84,898 more rows\n\n# index rows\natt[1, ]\n\n# A tibble: 1 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1600001 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n# index cell\natt[1, 1]\n\n# A tibble: 1 × 1\n  sal_code\n     &lt;dbl&gt;\n1  1600001\n\n\n\n\n\n\n\n\nAlternatively, you can access the data within individual columns by referring to their names using the $ operator. This allows you to easily extract and work with a specific column without needing to know its position in the dataframe. For example, if your dataframe is named dataframe and you want to access a column named age, you would use dataframe$age. This method is especially useful when your data has many columns or when the column positions may change, because it relies on the column names rather than their index numbers.\n\n\n\n\n\n\nNow that we have loaded and inspected our data, we can examine its distribution. We will focus on the column that contains the counts of individuals who speak isiXhosa as their primary language, aggregated by Small Area Layer — the most granular geographic level available in the 2011 South African Census of Population. This data is stored in the lng_4 column of the dataframe.\n\n\n\nR code\n\n# mean\nmean(att$lng_4)\n\n\n[1] 95.99431\n\n# median\nmedian(att$lng_4)\n\n[1] 9\n\n# range\nrange(att$lng_4)\n\n[1]    0 6465\n\n# summary\nsummary(att$lng_4)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    9.00   95.99   72.00 6465.00 \n\n\nWe can also call some functions to quickly draw a boxplot, histogram, or scatterplot:\n\n\n\nR code\n\n# boxplot\nboxplot(att$lng_4, horizontal = TRUE)\n\n\n\n\n\nFigure 2: Quick boxplot.\n\n\n\n\n\n\n\nR code\n\n# histogram\nhist(att$lng_4, breaks = 50, xlab = \"Number of Small Area Layers\", main = \"Number of isiXhosa speakers\")\n\n\n\n\n\nFigure 3: Quick histogram.\n\n\n\n\n\n\n\nR code\n\n# bivariate plot\nplot(att$lng_4, att$lng_2, xlab = \"isiXhosa\", ylab = \"English\")\n\n\n\n\n\nFigure 4: Quick bivariate plot.\n\n\n\n\n\n\n\nThe dataset is quite extensive, containing counts for all 84,908 Small Area Layers. To make our analysis more manageable, let’s focus on a subset of the data, specifically zooming in on Cape Town. We can achieve this by filtering the data using the dc_name column, which corresponds to the District Municipality Name.\n\n\n\nR code\n\n# prepare data: filter out Cape Town\natt &lt;- att |&gt;\n    filter(dc_name == \"City of Cape Town\")\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might seem a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\nIn this case, we have overwritten our initial object, though this is not strictly necessary. Our object now contains a subset of the full language dataset that we initially loaded.\n\n\n\nR code\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1990001 199014025 Paarden …  199014 Milner… CPT          199 City o… CPT     \n2  1990002 199016089 Mimosa     199016 Bellvi… CPT          199 City o… CPT     \n3  1990003 199041008 Signal H…  199041 Cape T… CPT          199 City o… CPT     \n4  1990004 199017021 Durbanvi…  199017 Durban… CPT          199 City o… CPT     \n5  1990005 199053012 Pine Hav…  199053 Simon'… CPT          199 City o… CPT     \n6  1990006 199030007 Guguleth…  199030 Gugule… CPT          199 City o… CPT     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n\nOur code appears to have worked successfully.\n\n\n\nThe next thing that we probably want to do is standardise our data. A common challenge with spatial data is that the spatial units aren not always the same size — some Small Area Layers may have larger populations, while others have fewer residents. This makes it difficult to compare absolute counts fairly. To address this, we can sum the counts across all relevant columns to determine the total number of speakers in each area. Then, we can calculate the proportion of isiXhosa speakers within each Small Area Layer:\n\n\n\nR code\n\n# prepare data: sum across\natt &lt;- att |&gt;\n    rowwise() |&gt;\n    mutate(sal_pop = sum(across(starts_with(\"lng\")), na.rm = TRUE))\n\n# prepare data: calculate percentages\natt &lt;- att |&gt;\n    mutate(sal_prop_xhosa = lng_4/sal_pop)\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 30\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1990001 199014025 Paarden …  199014 Milner… CPT          199 City o… CPT     \n2  1990002 199016089 Mimosa     199016 Bellvi… CPT          199 City o… CPT     \n3  1990003 199041008 Signal H…  199041 Cape T… CPT          199 City o… CPT     \n4  1990004 199017021 Durbanvi…  199017 Durban… CPT          199 City o… CPT     \n5  1990005 199053012 Pine Hav…  199053 Simon'… CPT          199 City o… CPT     \n6  1990006 199030007 Guguleth…  199030 Gugule… CPT          199 City o… CPT     \n# ℹ 21 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;, sal_pop &lt;dbl&gt;, sal_prop_xhosa &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe now have the percentage of people who speak isiXhosa as their primary language for each Small Area Layer. To provide a more meaningful representation, however, we can aggregate the data by sub-places. This involves grouping our data by sp_code, which contains unique sub-place codes. We will then:\n\nSum the number of isiXhosa speakers within each sub-place and store this in a new variable.\nSum the total number of people within each sub-place and store this in another new variable.\nExtract the distinct values for each sub-place.\n\nBy using the pipe function again, we can chain these steps together efficiently as follows:\n\n\n\nR code\n\n# prepare data: aggregate small areas to sub places\natt &lt;- att |&gt;\n    group_by(sp_code) |&gt;\n    mutate(sp_pop = sum(sal_pop)) |&gt;\n    mutate(sp_xhosa = sum(lng_4)) |&gt;\n    ungroup() |&gt;\n    distinct(sp_code, sp_pop, sp_xhosa)\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 3\n    sp_code sp_pop sp_xhosa\n      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 199014025     12        0\n2 199016089   1212        0\n3 199041008     12        0\n4 199017021   7893      105\n5 199053012      9        0\n6 199030007  55242    47385\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can save this dataset so that we can easily load it the next time we want to work with this by writing it to a .csv file.\n\n\n\nR code\n\n# write data\nwrite_csv(x = att, file = \"data/attributes/sa-language.csv\")"
  },
  {
    "objectID": "01-getting-started.html#internet-at-home",
    "href": "01-getting-started.html#internet-at-home",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "This concludes this session. Please try to complete the following tasks:\n\nCreate a histogram showing the distribution of the number of isiXhosa speakers in Cape Town, grouped by sub-place.\nCreate a boxplot of the proportion of isiXhosa-speakers in Cape Town, grouped by sub place.\n\nNext, use the link below to download another dataset sourced from the South African Census Community Profiles 2011 that contains information on internet access. Save the file in your project folder under data/attributes.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 Internet Access Table\n.dta\nDownload\n\n\n\nUse this dataset to:\n\nCreate a histogram of the distribution of the number of people with internet access at home in the City of Johannesburg, grouped by sub-place.\nCreate a boxplot of the proportion of people with internet access at home in the City of Johannesburg, grouped by sub-place."
  },
  {
    "objectID": "02-mapping-data.html",
    "href": "02-mapping-data.html",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "Open a new script within your Geospatial-Workshop24 project and save this as 02-language-maps.r. We will start again by loading the libraries that we will need. You have been introduced to the tidyverse library last session, but now we are adding the sf library to read and load our spatial data as well as the tmap library to visualise our spatial data:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\nWe will continue working with the .csv dataset that we prepared and saved in the previous session, so let us make sure it is loaded properly:\n\n\n\nR code\n\n# load data\natt &lt;- read_csv(\"data/attributes/sa-language.csv\")\n\n\nRows: 878 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): sp_code, sp_pop, sp_xhosa\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nNext, we need a corresponding spatial dataset that contains the Cape Town’s sub places and save it in your data/spatial folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nCape Town Sub Places\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nYou may have used spatial data before and noticed that we did not download a collection of files known as a shapefile but a GeoPackage instead. Whilst shapefiles are still being used, GeoPackage is a more modern and portable file format. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles where possible: [Link]\n\n\n\nLet us load the file and store it into an object called cpt. We can do this as follows:\n\n\n\nR code\n\n# load data\ncpt &lt;- st_read(\"data/spatial/subplace-cape-town-2013.gpkg\")\n\n\nReading layer `subplace-cape-town-2013' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/SA-TIED/data/spatial/subplace-cape-town-2013.gpkg' \n  using driver `GPKG'\nSimple feature collection with 921 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -64020.67 ymin: -3803551 xmax: 430.9835 ymax: -3705149\nProjected CRS: WGS_1984_Transverse_Mercator\n\n\nYou should also see the cpt variable appear in your environment window.\n\n\n\nAs this is the first time we have loaded spatial data into R, let’s go for a little exploration of how we can interact with our spatial dataframe. The first thing we want to do when we load spatial data is to quickly plot the data to check whether everything is in order. To do this, we can use the same function we used before: plot().\n\n\n\nR code\n\n# plot data\nplot(cpt, max.plot = 1)\n\n\n\n\n\nYou should see your cpt plot appear in your Plots window.\n\n\n\n\n\n\nThe plot() function should not to be used to make publishable maps but can be used as a quick way of inspecting your spatial data.\n\n\n\nJust as with a tabular dataframe, we can inspect the spatial data frame:\n\n\n\nR code\n\n# inspect columns\nncol(cpt)\n\n\n[1] 17\n\n# inspect rows\nnrow(cpt)\n\n[1] 921\n\n# inspect data\nhead(cpt)\n\n# A tibble: 6 × 17\n  sp_code   sp_name    mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c dc_mn_c\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;\n1 199035011 Greenfield  199035 Blue D… CPT          199 City o… CPT          199\n2 199035012 Wesbank     199035 Blue D… CPT          199 City o… CPT          199\n3 199035013 Kleinvlei   199035 Blue D… CPT          199 City o… CPT          199\n4 199035014 Palm Park   199035 Blue D… CPT          199 City o… CPT          199\n5 199035015 Park Vill…  199035 Blue D… CPT          199 City o… CPT          199\n6 199035016 Hill View   199035 Blue D… CPT          199 City o… CPT          199\n# ℹ 8 more variables: dc_name &lt;chr&gt;, pr_mdb_c &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, albers_are &lt;dbl&gt;, shape_leng &lt;dbl&gt;, shape_area &lt;dbl&gt;,\n#   geom &lt;MULTIPOLYGON [m]&gt;\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"geom\"      \n\n\nWe can also again establish the class of our data:\n\n\n\nR code\n\n# inspect\nclass(cpt)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe should see our data is an sf dataframe, which is what we want and we can move on.\n\n\n\nWe now have our language dataset (att) with the number of isiXhosa speakers in Cape Town, organised by sub-place, as well as a spatial dataset containing the boundaries of these sub-places (cpt). We can now join this table data to our spatial data using an Attribute Join.\n\n\n\n\n\n\nAn attribute join links two datasets based on a common attribute, enabling the ‘matching’ of rows between them.\n\n\n\n\n\nFigure 1: Attribute Joins.\n\n\n\n\nTo perform a successful join, each dataset must contain a unique identifying (UID) field. This could be a code, a name, or any other consistent identifier. It is crucial that the ID field is accurate across both datasets, with no typos or inconsistencies (e.g., “City of Cape Town” is not the same as “The City of Cape Town”). Whenever possible, it is preferable to use unique codes rather than names, as codes reduce the likelihood of errors and mismatches.\n\n\n\nBefore proceeding with the join, we need to verify that a matching UID exists in both datasets. Let’s look at the column names in our datasets again:\n\n\n\nR code\n\n# inspect column names\nnames(att)\n\n\n[1] \"sp_code\"  \"sp_pop\"   \"sp_xhosa\"\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"geom\"      \n\n\nThe sp_code columns looks promising as it features in both datasets. We can quickly sort both columns and have a peek at the data:\n\n\n\nR code\n\n# inspect att\nhead(sort(att$sp_code))\n\n\n[1] 199001001 199002001 199002002 199002003 199003001 199004001\n\n# inspect cpt\nhead(sort(cpt$sp_code))\n\n[1] \"199001001\" \"199002001\" \"199002002\" \"199002003\" \"199003001\" \"199004001\"\n\n\nThey seem to contain similar values, so that is promising. Let us try to join the attribute data onto the spatial data:\n\n\n\nR code\n\n# join attribute data onto spatial data\ncpt &lt;- cpt |&gt; \n  left_join(att, by = c(\"sp_code\" = \"sp_code\"))\n\n\nYou will notice that the join results in an error.\nWhere the sub place codes in the att object are stored as numbers, the sub place codes in the cpt object are stored as strings. We can fix this by casting the number to characters:\n\n\n\nR code\n\n# change data type\natt &lt;- att |&gt;\n    mutate(sp_code = as.character(sp_code))\n\n# inspect\ntypeof(att$sp_code)\n\n\n[1] \"character\"\n\n\nWe can now try to join the datasets together again:\n\n\n\nR code\n\n# join attribute data onto spatial data\ncpt &lt;- cpt |&gt; \n  left_join(att, by = c(\"sp_code\" = \"sp_code\"))\n\n\nWe can explore the joined data in usual fashion:\n\n\n\nR code\n\n# inspect columns\nncol(cpt)\n\n\n[1] 19\n\n# inspect rows\nnrow(cpt)\n\n[1] 921\n\n# inspect data\nhead(cpt)\n\n# A tibble: 6 × 19\n  sp_code   sp_name    mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c dc_mn_c\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;\n1 199035011 Greenfield  199035 Blue D… CPT          199 City o… CPT          199\n2 199035012 Wesbank     199035 Blue D… CPT          199 City o… CPT          199\n3 199035013 Kleinvlei   199035 Blue D… CPT          199 City o… CPT          199\n4 199035014 Palm Park   199035 Blue D… CPT          199 City o… CPT          199\n5 199035015 Park Vill…  199035 Blue D… CPT          199 City o… CPT          199\n6 199035016 Hill View   199035 Blue D… CPT          199 City o… CPT          199\n# ℹ 10 more variables: dc_name &lt;chr&gt;, pr_mdb_c &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, albers_are &lt;dbl&gt;, shape_leng &lt;dbl&gt;, shape_area &lt;dbl&gt;,\n#   sp_pop &lt;dbl&gt;, sp_xhosa &lt;dbl&gt;, geom &lt;MULTIPOLYGON [m]&gt;\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"sp_pop\"     \"sp_xhosa\"   \"geom\"      \n\n\nAlways inspect your join to ensure everything looks as expected. A good way to do this is by using the View() function to check for any unexpected missing values, which are marked as NA. We can further compare the total number of rows in the spatial dataset with the total number of non-NA values in the joined columns:\n\n\n\nR code\n\n# inspect\nnrow(cpt)\n\n\n[1] 921\n\n# inspect attribute data: sp_pop\nsum(!is.na(cpt$sp_pop))\n\n[1] 878\n\n# inspect attribute data: sp_xhosas\nsum(!is.na(cpt$sp_xhosa))\n\n[1] 878\n\n\nOut of the 921 sub-places, only 878 have data on the number of isiXhosa speakers and the total population. However, this discrepancy may not be due to the join; it could simply be that the original dataset lacks data for some units. Let’s confirm this:\n\n\n\nR code\n\n# inspect\nnrow(att)\n\n\n[1] 878\n\n\nThis confirms that all our full attribute dataset has been linked to the spatial dataset.\n\n\n\n\n\n\nIt is important to confirm the source of any NA values that are introduced to the dataset to ensure these are genuine. In our case the South African Census Community Profiles 2011 dataset includes sub places with no population, which typically correspond to industrial areas.\n\n\n\nSince we know these NA values are not truly missing but represent structural zeroes, we can replace them with 0. However, this step is not strictly necessary.\n\n\n\nR code\n\n# replace NAs\ncpt &lt;- cpt |&gt;\n    mutate(sp_pop = if_else(is.na(sp_pop), 0, sp_pop), sp_xhosa = if_else(is.na(sp_xhosa),\n        0, sp_xhosa))\n\n\nWe are almost ready to map the data. We only need to add the proportion of isiXhosa speakers within each sub place to the data frame:\n\n\n\nR code\n\n# calculate percentages\ncpt &lt;- cpt |&gt;\n    mutate(sp_prop_xhosa = sp_xhosa/sp_pop)\n\n\nWe can save this dataset so that we can easily load it the next time we want to work with this by writing it to a GeoPackage:\n\n\n\nR code\n\n# write data\nst_write(obj = cpt, dsn = \"data/spatial/subplace-cape-town-2013-xhosa.gpkg\")\n\n\n\n\n\nFor our map-making, we will use one of the two primary visualisation libraries for spatial data: tmap. tmap offers a flexible, layer-based approach that makes it easy to create various types of thematic maps, such as choropleths and bubble maps. One of the standout features of tmap is its quick plotting function, qtm(), which allows you to generate basic maps with minimal effort.\n\n\n\nR code\n\n# quick thematic map\nqtm(cpt, fill = \"sp_prop_xhosa\")\n\n\n\n\n\nFigure 2: Quick thematic map.\n\n\n\n\nIn this case, the fill() argument in tmap is how we instruct the library to create a choropleth map based on the values in the specified column. If we set fill() to NULL, only the borders of our polygons will be drawn, without any colour fill. The qtm() function in tmap is versatile, allowing us to pass various parameters to customise the aesthetics of our map. By checking the function’s documentation, you can explore the full list of available parameters. For instance, to set the borders of our Cape Town polygons to white, we can use the borders parameter:\n\n\n\nR code\n\n# quick thematic map\nqtm(cpt, fill = \"sp_prop_xhosa\", borders = \"white\")\n\n\n\n\n\nFigure 3: Quick thematic map with white borders.\n\n\n\n\nThe map does not look quite right yet. While we can continue tweaking parameters in the qtm() function to improve it, qtm() is somewhat limited in its functionality and is primarily intended for quickly inspecting your data and creating basic maps. For more complex and refined map-making with the tmap library, it is better to use the main plotting method that starts with the tm_shape() function.\n\n\n\n\n\n\nThe primary approach to creating maps in tmap involves using a layered grammar of graphics to build up your map, starting with the tm_shape() function. This function, when provided with a spatial dataframe, captures the spatial information of your data, including its projection and geometry, and creates a spatial object. While you can override certain aspects of the spatial data (such as its projection) using the function’s parameters, the essential role of tm_shape() is to instruct R to “use this object as the basis for drawing the shapes.”\nTo actually render the shapes, you need to add a layer that specifies the type of shape you want R to draw from this spatial information—such as polygons for our data. This layer function tells R to “draw my spatial object as X,” where X represents the type of shape. Within this layer, you can also provide additional details to control how R draws your shapes. Further, you can add more layers to include other spatial objects and their corresponding shapes on your map. Finally, layout options can be specified through a layout layer, allowing you to customize the overall appearance and arrangement of your map.\n\n\n\nLet us build a map using tmap:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) + tm_polygons()\n\n\n\n\n\nFigure 4: Building up a map layer by layer.\n\n\n\n\nAs you can now see, we have mapped the spatial polygons of our cpt spatial dataframe. However, this is not quite the map we want; we need a choropleth map where the polygons are colored based on the proportion of isiXhosa speakers. To achieve this, we use the col parameter within the tm_polygons() function.\n\n\n\n\n\n\nThe col parameter within tm_polygons() allows you to fill polygons with colours based on:\n\nA single colour value (e.g. red).\nThe name of a data variable within the spatial data file. This variable can either contain specific colour values or numeric/categorical values that will be mapped to a colour palette.\n\n\n\n\nLet us go ahead and pass our sp_prop_xhosa variable within the col() parameter and see what we get:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n  # specify column\n  tm_polygons(\n    col = \"sp_prop_xhosa\"\n  )\n\n\n\n\n\nFigure 5: Building up a map layer by layer.\n\n\n\n\nWe are making progress, but there are two immediate issues with our map. First, the classification breaks do not adequately reflect the variation in our dataset. By default, tmap uses pretty breaks, which may not be the most effective for our data. An alternative, such as natural breaks (or Jenks), might better reveal the data’s variation.\nTo customise the classification breaks, refer to the tm_polygons() documentation. The following parameters are relevant:\n\nn: Specifies the number of classification breaks.\nstyle: Defines the method for classification breaks, such as fixed, standard deviation, equal, or quantile.\nbreaks: Allows you to set specific numeric breaks when using the fixed style.\n\nFor example, if we want to adjust our choropleth map to use five classes determined by the natural breaks method, we need to add the n and style parameters to our tm_polygons() layer:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n  # specify column, classes\n  tm_polygons(\n    col = \"sp_prop_xhosa\",\n    n = 5,\n    style = \"jenks\"\n  )\n\n\n\n\n\nFigure 6: Building up a map layer by layer.\n\n\n\n\n\n\n\nStyling a map in tmap requires a deeper understanding and familiarity with the library, which is something you will develop best through hands-on practice. Here are the key functions to be aware of:\n\ntm_layout(): Customize titles, fonts, legends, and other layout elements.\ntm_compass(): Add and style a North arrow or compass.\ntm_scale_bar(): Add and style a scale bar.\n\nTo begin styling your map, explore each of these functions and their parameters. Through trial and error, you can tweak and refine the map until you achieve the desired look:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"sp_prop_xhosa\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Largest share\", \"2nd largest\", \"3rd largest\", \"4th largest\", \"Smallest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population speaking isiXhosa\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  )\n\n\n\n\n\nFigure 7: Building up a map layer by layer.\n\n\n\n\nWe can also have some map labels, if we want, by extracting centroids from selected polygons and adding these as seperate map layer:\n\n\n\nR code\n\n# map labels\ncbd &lt;- cpt |&gt;\n  filter(sp_code == \"199041011\") |&gt;\n  st_centroid()\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# map object\ncpt_xhosa &lt;-\n  # shape, polygons\n  tm_shape(cpt) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"sp_prop_xhosa\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Largest share\", \"2nd largest\", \"3rd largest\", \"4th largest\", \"Smallest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # cbd centroid\n  tm_shape(cbd) +\n\n  # add points\n  tm_dots(size = 0.4, col = \"#000000\") +\n\n  # add labels\n  tm_text(text = \"sp_name\", xmod = 0, ymod = -0.6, col = \"#000000\", size = 0.8) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population speaking isi-Xhosa\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  ) +\n\n  # add credits\n  tm_credits(\"Data source: Census 2011, StatsSA\",\n    fontface = \"italic\",\n    position = c(\"left\", \"bottom\"),\n    size = 0.4\n  )\n\n# plot\ncpt_xhosa\n\n\n\n\nFigure 8: Building up a map layer by layer.\n\n\n\n\nIn the code above, we stored the full map definition as an object. This makes it easy to export the map and save it as a .jpg, .png or .pdf file:\n\n\n\nR code\n\n# write map\ntmap_save(tm = xhosa_map, \"cpt-xhosa.jpg\", width = 15, height = 15, units = c(\"cm\"))\n\n\n\n\n\nThis concludes this session. Please try to complete the following tasks:\n\nIf you have not already done so in the previous session, aggregate the internet access dataset by the internet at home variable for sub places in the City of Johannesburg.\nDownload the spatial boundaries for sub places in the City of Johannesburg using the link below, and save the file in your data/spatial folder.\nCreate a map that shows the proportion of individuals with internet access at home as part of the overall population.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nJohannesburg Sub Places\nGeoPackage\nDownload"
  },
  {
    "objectID": "02-mapping-data.html#loading-spatial-data",
    "href": "02-mapping-data.html#loading-spatial-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "Open a new script within your Geospatial-Workshop24 project and save this as 02-language-maps.r. We will start again by loading the libraries that we will need. You have been introduced to the tidyverse library last session, but now we are adding the sf library to read and load our spatial data as well as the tmap library to visualise our spatial data:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\nWe will continue working with the .csv dataset that we prepared and saved in the previous session, so let us make sure it is loaded properly:\n\n\n\nR code\n\n# load data\natt &lt;- read_csv(\"data/attributes/sa-language.csv\")\n\n\nRows: 878 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): sp_code, sp_pop, sp_xhosa\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nNext, we need a corresponding spatial dataset that contains the Cape Town’s sub places and save it in your data/spatial folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nCape Town Sub Places\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nYou may have used spatial data before and noticed that we did not download a collection of files known as a shapefile but a GeoPackage instead. Whilst shapefiles are still being used, GeoPackage is a more modern and portable file format. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles where possible: [Link]\n\n\n\nLet us load the file and store it into an object called cpt. We can do this as follows:\n\n\n\nR code\n\n# load data\ncpt &lt;- st_read(\"data/spatial/subplace-cape-town-2013.gpkg\")\n\n\nReading layer `subplace-cape-town-2013' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/SA-TIED/data/spatial/subplace-cape-town-2013.gpkg' \n  using driver `GPKG'\nSimple feature collection with 921 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -64020.67 ymin: -3803551 xmax: 430.9835 ymax: -3705149\nProjected CRS: WGS_1984_Transverse_Mercator\n\n\nYou should also see the cpt variable appear in your environment window."
  },
  {
    "objectID": "02-mapping-data.html#exploring-spatial-data",
    "href": "02-mapping-data.html#exploring-spatial-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "As this is the first time we have loaded spatial data into R, let’s go for a little exploration of how we can interact with our spatial dataframe. The first thing we want to do when we load spatial data is to quickly plot the data to check whether everything is in order. To do this, we can use the same function we used before: plot().\n\n\n\nR code\n\n# plot data\nplot(cpt, max.plot = 1)\n\n\n\n\n\nYou should see your cpt plot appear in your Plots window.\n\n\n\n\n\n\nThe plot() function should not to be used to make publishable maps but can be used as a quick way of inspecting your spatial data.\n\n\n\nJust as with a tabular dataframe, we can inspect the spatial data frame:\n\n\n\nR code\n\n# inspect columns\nncol(cpt)\n\n\n[1] 17\n\n# inspect rows\nnrow(cpt)\n\n[1] 921\n\n# inspect data\nhead(cpt)\n\n# A tibble: 6 × 17\n  sp_code   sp_name    mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c dc_mn_c\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;\n1 199035011 Greenfield  199035 Blue D… CPT          199 City o… CPT          199\n2 199035012 Wesbank     199035 Blue D… CPT          199 City o… CPT          199\n3 199035013 Kleinvlei   199035 Blue D… CPT          199 City o… CPT          199\n4 199035014 Palm Park   199035 Blue D… CPT          199 City o… CPT          199\n5 199035015 Park Vill…  199035 Blue D… CPT          199 City o… CPT          199\n6 199035016 Hill View   199035 Blue D… CPT          199 City o… CPT          199\n# ℹ 8 more variables: dc_name &lt;chr&gt;, pr_mdb_c &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, albers_are &lt;dbl&gt;, shape_leng &lt;dbl&gt;, shape_area &lt;dbl&gt;,\n#   geom &lt;MULTIPOLYGON [m]&gt;\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"geom\"      \n\n\nWe can also again establish the class of our data:\n\n\n\nR code\n\n# inspect\nclass(cpt)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe should see our data is an sf dataframe, which is what we want and we can move on."
  },
  {
    "objectID": "02-mapping-data.html#joining-attribute-data",
    "href": "02-mapping-data.html#joining-attribute-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "We now have our language dataset (att) with the number of isiXhosa speakers in Cape Town, organised by sub-place, as well as a spatial dataset containing the boundaries of these sub-places (cpt). We can now join this table data to our spatial data using an Attribute Join.\n\n\n\n\n\n\nAn attribute join links two datasets based on a common attribute, enabling the ‘matching’ of rows between them.\n\n\n\n\n\nFigure 1: Attribute Joins.\n\n\n\n\nTo perform a successful join, each dataset must contain a unique identifying (UID) field. This could be a code, a name, or any other consistent identifier. It is crucial that the ID field is accurate across both datasets, with no typos or inconsistencies (e.g., “City of Cape Town” is not the same as “The City of Cape Town”). Whenever possible, it is preferable to use unique codes rather than names, as codes reduce the likelihood of errors and mismatches.\n\n\n\nBefore proceeding with the join, we need to verify that a matching UID exists in both datasets. Let’s look at the column names in our datasets again:\n\n\n\nR code\n\n# inspect column names\nnames(att)\n\n\n[1] \"sp_code\"  \"sp_pop\"   \"sp_xhosa\"\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"geom\"      \n\n\nThe sp_code columns looks promising as it features in both datasets. We can quickly sort both columns and have a peek at the data:\n\n\n\nR code\n\n# inspect att\nhead(sort(att$sp_code))\n\n\n[1] 199001001 199002001 199002002 199002003 199003001 199004001\n\n# inspect cpt\nhead(sort(cpt$sp_code))\n\n[1] \"199001001\" \"199002001\" \"199002002\" \"199002003\" \"199003001\" \"199004001\"\n\n\nThey seem to contain similar values, so that is promising. Let us try to join the attribute data onto the spatial data:\n\n\n\nR code\n\n# join attribute data onto spatial data\ncpt &lt;- cpt |&gt; \n  left_join(att, by = c(\"sp_code\" = \"sp_code\"))\n\n\nYou will notice that the join results in an error.\nWhere the sub place codes in the att object are stored as numbers, the sub place codes in the cpt object are stored as strings. We can fix this by casting the number to characters:\n\n\n\nR code\n\n# change data type\natt &lt;- att |&gt;\n    mutate(sp_code = as.character(sp_code))\n\n# inspect\ntypeof(att$sp_code)\n\n\n[1] \"character\"\n\n\nWe can now try to join the datasets together again:\n\n\n\nR code\n\n# join attribute data onto spatial data\ncpt &lt;- cpt |&gt; \n  left_join(att, by = c(\"sp_code\" = \"sp_code\"))\n\n\nWe can explore the joined data in usual fashion:\n\n\n\nR code\n\n# inspect columns\nncol(cpt)\n\n\n[1] 19\n\n# inspect rows\nnrow(cpt)\n\n[1] 921\n\n# inspect data\nhead(cpt)\n\n# A tibble: 6 × 19\n  sp_code   sp_name    mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c dc_mn_c\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;\n1 199035011 Greenfield  199035 Blue D… CPT          199 City o… CPT          199\n2 199035012 Wesbank     199035 Blue D… CPT          199 City o… CPT          199\n3 199035013 Kleinvlei   199035 Blue D… CPT          199 City o… CPT          199\n4 199035014 Palm Park   199035 Blue D… CPT          199 City o… CPT          199\n5 199035015 Park Vill…  199035 Blue D… CPT          199 City o… CPT          199\n6 199035016 Hill View   199035 Blue D… CPT          199 City o… CPT          199\n# ℹ 10 more variables: dc_name &lt;chr&gt;, pr_mdb_c &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, albers_are &lt;dbl&gt;, shape_leng &lt;dbl&gt;, shape_area &lt;dbl&gt;,\n#   sp_pop &lt;dbl&gt;, sp_xhosa &lt;dbl&gt;, geom &lt;MULTIPOLYGON [m]&gt;\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"sp_pop\"     \"sp_xhosa\"   \"geom\"      \n\n\nAlways inspect your join to ensure everything looks as expected. A good way to do this is by using the View() function to check for any unexpected missing values, which are marked as NA. We can further compare the total number of rows in the spatial dataset with the total number of non-NA values in the joined columns:\n\n\n\nR code\n\n# inspect\nnrow(cpt)\n\n\n[1] 921\n\n# inspect attribute data: sp_pop\nsum(!is.na(cpt$sp_pop))\n\n[1] 878\n\n# inspect attribute data: sp_xhosas\nsum(!is.na(cpt$sp_xhosa))\n\n[1] 878\n\n\nOut of the 921 sub-places, only 878 have data on the number of isiXhosa speakers and the total population. However, this discrepancy may not be due to the join; it could simply be that the original dataset lacks data for some units. Let’s confirm this:\n\n\n\nR code\n\n# inspect\nnrow(att)\n\n\n[1] 878\n\n\nThis confirms that all our full attribute dataset has been linked to the spatial dataset.\n\n\n\n\n\n\nIt is important to confirm the source of any NA values that are introduced to the dataset to ensure these are genuine. In our case the South African Census Community Profiles 2011 dataset includes sub places with no population, which typically correspond to industrial areas.\n\n\n\nSince we know these NA values are not truly missing but represent structural zeroes, we can replace them with 0. However, this step is not strictly necessary.\n\n\n\nR code\n\n# replace NAs\ncpt &lt;- cpt |&gt;\n    mutate(sp_pop = if_else(is.na(sp_pop), 0, sp_pop), sp_xhosa = if_else(is.na(sp_xhosa),\n        0, sp_xhosa))\n\n\nWe are almost ready to map the data. We only need to add the proportion of isiXhosa speakers within each sub place to the data frame:\n\n\n\nR code\n\n# calculate percentages\ncpt &lt;- cpt |&gt;\n    mutate(sp_prop_xhosa = sp_xhosa/sp_pop)\n\n\nWe can save this dataset so that we can easily load it the next time we want to work with this by writing it to a GeoPackage:\n\n\n\nR code\n\n# write data\nst_write(obj = cpt, dsn = \"data/spatial/subplace-cape-town-2013-xhosa.gpkg\")"
  },
  {
    "objectID": "02-mapping-data.html#mapping-spatial-data",
    "href": "02-mapping-data.html#mapping-spatial-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "For our map-making, we will use one of the two primary visualisation libraries for spatial data: tmap. tmap offers a flexible, layer-based approach that makes it easy to create various types of thematic maps, such as choropleths and bubble maps. One of the standout features of tmap is its quick plotting function, qtm(), which allows you to generate basic maps with minimal effort.\n\n\n\nR code\n\n# quick thematic map\nqtm(cpt, fill = \"sp_prop_xhosa\")\n\n\n\n\n\nFigure 2: Quick thematic map.\n\n\n\n\nIn this case, the fill() argument in tmap is how we instruct the library to create a choropleth map based on the values in the specified column. If we set fill() to NULL, only the borders of our polygons will be drawn, without any colour fill. The qtm() function in tmap is versatile, allowing us to pass various parameters to customise the aesthetics of our map. By checking the function’s documentation, you can explore the full list of available parameters. For instance, to set the borders of our Cape Town polygons to white, we can use the borders parameter:\n\n\n\nR code\n\n# quick thematic map\nqtm(cpt, fill = \"sp_prop_xhosa\", borders = \"white\")\n\n\n\n\n\nFigure 3: Quick thematic map with white borders.\n\n\n\n\nThe map does not look quite right yet. While we can continue tweaking parameters in the qtm() function to improve it, qtm() is somewhat limited in its functionality and is primarily intended for quickly inspecting your data and creating basic maps. For more complex and refined map-making with the tmap library, it is better to use the main plotting method that starts with the tm_shape() function.\n\n\n\n\n\n\nThe primary approach to creating maps in tmap involves using a layered grammar of graphics to build up your map, starting with the tm_shape() function. This function, when provided with a spatial dataframe, captures the spatial information of your data, including its projection and geometry, and creates a spatial object. While you can override certain aspects of the spatial data (such as its projection) using the function’s parameters, the essential role of tm_shape() is to instruct R to “use this object as the basis for drawing the shapes.”\nTo actually render the shapes, you need to add a layer that specifies the type of shape you want R to draw from this spatial information—such as polygons for our data. This layer function tells R to “draw my spatial object as X,” where X represents the type of shape. Within this layer, you can also provide additional details to control how R draws your shapes. Further, you can add more layers to include other spatial objects and their corresponding shapes on your map. Finally, layout options can be specified through a layout layer, allowing you to customize the overall appearance and arrangement of your map.\n\n\n\nLet us build a map using tmap:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) + tm_polygons()\n\n\n\n\n\nFigure 4: Building up a map layer by layer.\n\n\n\n\nAs you can now see, we have mapped the spatial polygons of our cpt spatial dataframe. However, this is not quite the map we want; we need a choropleth map where the polygons are colored based on the proportion of isiXhosa speakers. To achieve this, we use the col parameter within the tm_polygons() function.\n\n\n\n\n\n\nThe col parameter within tm_polygons() allows you to fill polygons with colours based on:\n\nA single colour value (e.g. red).\nThe name of a data variable within the spatial data file. This variable can either contain specific colour values or numeric/categorical values that will be mapped to a colour palette.\n\n\n\n\nLet us go ahead and pass our sp_prop_xhosa variable within the col() parameter and see what we get:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n  # specify column\n  tm_polygons(\n    col = \"sp_prop_xhosa\"\n  )\n\n\n\n\n\nFigure 5: Building up a map layer by layer.\n\n\n\n\nWe are making progress, but there are two immediate issues with our map. First, the classification breaks do not adequately reflect the variation in our dataset. By default, tmap uses pretty breaks, which may not be the most effective for our data. An alternative, such as natural breaks (or Jenks), might better reveal the data’s variation.\nTo customise the classification breaks, refer to the tm_polygons() documentation. The following parameters are relevant:\n\nn: Specifies the number of classification breaks.\nstyle: Defines the method for classification breaks, such as fixed, standard deviation, equal, or quantile.\nbreaks: Allows you to set specific numeric breaks when using the fixed style.\n\nFor example, if we want to adjust our choropleth map to use five classes determined by the natural breaks method, we need to add the n and style parameters to our tm_polygons() layer:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n  # specify column, classes\n  tm_polygons(\n    col = \"sp_prop_xhosa\",\n    n = 5,\n    style = \"jenks\"\n  )\n\n\n\n\n\nFigure 6: Building up a map layer by layer."
  },
  {
    "objectID": "02-mapping-data.html#styling-spatial-data",
    "href": "02-mapping-data.html#styling-spatial-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "Styling a map in tmap requires a deeper understanding and familiarity with the library, which is something you will develop best through hands-on practice. Here are the key functions to be aware of:\n\ntm_layout(): Customize titles, fonts, legends, and other layout elements.\ntm_compass(): Add and style a North arrow or compass.\ntm_scale_bar(): Add and style a scale bar.\n\nTo begin styling your map, explore each of these functions and their parameters. Through trial and error, you can tweak and refine the map until you achieve the desired look:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"sp_prop_xhosa\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Largest share\", \"2nd largest\", \"3rd largest\", \"4th largest\", \"Smallest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population speaking isiXhosa\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  )\n\n\n\n\n\nFigure 7: Building up a map layer by layer.\n\n\n\n\nWe can also have some map labels, if we want, by extracting centroids from selected polygons and adding these as seperate map layer:\n\n\n\nR code\n\n# map labels\ncbd &lt;- cpt |&gt;\n  filter(sp_code == \"199041011\") |&gt;\n  st_centroid()\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# map object\ncpt_xhosa &lt;-\n  # shape, polygons\n  tm_shape(cpt) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"sp_prop_xhosa\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Largest share\", \"2nd largest\", \"3rd largest\", \"4th largest\", \"Smallest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # cbd centroid\n  tm_shape(cbd) +\n\n  # add points\n  tm_dots(size = 0.4, col = \"#000000\") +\n\n  # add labels\n  tm_text(text = \"sp_name\", xmod = 0, ymod = -0.6, col = \"#000000\", size = 0.8) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population speaking isi-Xhosa\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  ) +\n\n  # add credits\n  tm_credits(\"Data source: Census 2011, StatsSA\",\n    fontface = \"italic\",\n    position = c(\"left\", \"bottom\"),\n    size = 0.4\n  )\n\n# plot\ncpt_xhosa\n\n\n\n\nFigure 8: Building up a map layer by layer.\n\n\n\n\nIn the code above, we stored the full map definition as an object. This makes it easy to export the map and save it as a .jpg, .png or .pdf file:\n\n\n\nR code\n\n# write map\ntmap_save(tm = xhosa_map, \"cpt-xhosa.jpg\", width = 15, height = 15, units = c(\"cm\"))"
  },
  {
    "objectID": "02-mapping-data.html#assignment",
    "href": "02-mapping-data.html#assignment",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "This concludes this session. Please try to complete the following tasks:\n\nIf you have not already done so in the previous session, aggregate the internet access dataset by the internet at home variable for sub places in the City of Johannesburg.\nDownload the spatial boundaries for sub places in the City of Johannesburg using the link below, and save the file in your data/spatial folder.\nCreate a map that shows the proportion of individuals with internet access at home as part of the overall population.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nJohannesburg Sub Places\nGeoPackage\nDownload"
  },
  {
    "objectID": "03-spatial-autocorrelation.html",
    "href": "03-spatial-autocorrelation.html",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "Open a new script within your Geospatial-Workshop24 project and save this as 03-autorrelcation.r. We will start again by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\n\n\n\n\n\nIn this session, we will be looking at education, focusing on the number of people with no schooling at the municipal level, as aggregated from the South African Census Community Profiles 2011. Along with this dataset, we also have access to a GeoPackage that contains the spatial boundaries of these municipalities. You can download both files below and save them in your project folder under data/attributes and data/spatial, respectively.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 No Schooling Variable\ncsv\nDownload\n\n\nSA Municipalities\nGeoPackage\nDownload\n\n\n\nNow, we can load both files into memory:\n\n\n\nR code\n\n# load spatial data, transform projection\nsa_municipality &lt;- st_read(\"data/spatial/municipality-south-africa-2013.gpkg\")\n\n\nReading layer `municipality-south-africa-2013' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/SA-TIED/data/spatial/municipality-south-africa-2013.gpkg' \n  using driver `GPKG'\nSimple feature collection with 234 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1831416 ymin: -4141363 xmax: 3667419 ymax: -2526543\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# load attribute data\nno_schooling &lt;- read_csv(\"data/attributes/sa-no-schooling.csv\")\n\nRows: 234 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): mn_name\ndbl (3): mn_code, mn_pop, mn_no_school\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\n\n\n\nWith this dataset, we are interested in analysing the proportion of people without schooling across the country and visualising this information on a map. Let us start by preparing the data for mapping:\n\n\n\nR code\n\n# calculate proportions\nno_schooling &lt;- no_schooling |&gt;\n  mutate(mn_prop_no_schooling = mn_no_school / mn_pop)\n\n# join attribute data onto spatial data\nsa_municipality &lt;- sa_municipality |&gt; \n  left_join(no_schooling, by = c(\"mn_code\" = \"mn_code\"))\n\n\nWe can now create a simple map:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_prop_no_schooling\",\n    n = 5,\n    style = \"jenks\"\n  ) +\n\n  # no legend\n  tm_layout(\n    legend.show = FALSE\n  )\n\n\n\n\n\nFigure 1: Proportions of people having no schooling by municipality.\n\n\n\n\n\n\n\n\n\n\nIf you get the following error when trying to plot the municipality boundaries: Error: Shape contains invalid polygons. Please fix it or set tmap_options(check.and.fix = TRUE) and rerun the plot, run sf_use_s2(FALSE) and try again.\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population that did does not have any schooling appears to be neither random nor uniform, with a tendency for similar values to be found in closely located municipalities. Let us compare our map to a map with the same values which have been randomly permutated:\n\n\n\nR code\n\n# random permutation\nsa_municipality &lt;- sa_municipality |&gt;\n  mutate(mn_prop_no_schooling_random = sample(sa_municipality$mn_prop_no_schooling, replace = FALSE))\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_prop_no_schooling_random\",\n    n = 5,\n    style = \"jenks\"\n  ) +\n\n  # no legend\n  tm_layout(\n    legend.show = FALSE\n  )\n\n\n\n\n\nFigure 2: Proportions of people having no schooling by municipality with randomly permutated values.\n\n\n\n\nLooking at Figure 2, even with the values being randomly permuted, certain patterns seem to emerge. This observation raises an important question: to what extent are the patterns that we see in the actual data actually present? A widely used method to quantify the similarity between neighbouring locations is by calculating Moran’s I statistic. This measure assesses spatial autocorrelation, indicating the degree to which values of a variable cluster spatially — either through similar (positive spatial autocorrelation) or contrasting values (negative spatial autocorrelation, such as the alternating black and white squares on a chessboard).\n\n\n\nIf the purpose of a Moran’s I test is to quantify how similar places are to their neighbours, the first step is to define what constitutes a neighbour. This definition is not necessarily straightforward, because ‘neighbouring’ observations can be determined in various ways, based on either geometry or proximity. The most common methods include:\n\nContiguity: Spatial units are considered neighbours if their polygon boundaries touch.\nFixed Distance: Spatial units are considered neighbours if they fall within a specified distance.\n(K) Nearest Neighbours: Spatial units are considered neighbours if they are among the closest neighbours.\n\nTo capture this information, we need to formalise the spatial relationships within our data by constructing a spatial weights matrix (\\(W_{ij}\\)). This matrix defines which units are neighbours based on our chosen criteria.\n\n\n\n\n\n\nIn the following example, neighbours are defined as places that share a border (i.e., they are contiguous). Currently, it is sufficient for them to meet at a single point — so if two places are triangular, touching corners would count them as neighbours. If, however, you require them to share an edge, rather than just a corner, you can modify the default argument by setting queen = FALSE.\n\n\n\n\n\n\nR code\n\n# create neighbourlist\nsa_mn_nb &lt;- poly2nb(sa_municipality, queen = TRUE)\n\n# inspect\nsummary(sa_mn_nb)\n\n\nNeighbour list object:\nNumber of regions: 234 \nNumber of nonzero links: 1244 \nPercentage nonzero weights: 2.271897 \nAverage number of links: 5.316239 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 1  7 22 35 56 64 40  3  5  1 \n1 least connected region:\n102 with 1 link\n1 most connected region:\n193 with 10 links\n\n\n\n\n\n\n\n\nIt is pretty common for the average number of contiguous neighbours to be about five and the most frequent number of contiguous neighbours to be five or six. It suggests that the spatial configuration of the places is loosely approximating a hexagonal tessellation, although places on the edge of a study region will typically have fewer contiguous neighbours then those at the centre."
  },
  {
    "objectID": "03-spatial-autocorrelation.html#loading-spatial-data",
    "href": "03-spatial-autocorrelation.html#loading-spatial-data",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "Open a new script within your Geospatial-Workshop24 project and save this as 03-autorrelcation.r. We will start again by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\n\n\n\n\n\nIn this session, we will be looking at education, focusing on the number of people with no schooling at the municipal level, as aggregated from the South African Census Community Profiles 2011. Along with this dataset, we also have access to a GeoPackage that contains the spatial boundaries of these municipalities. You can download both files below and save them in your project folder under data/attributes and data/spatial, respectively.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 No Schooling Variable\ncsv\nDownload\n\n\nSA Municipalities\nGeoPackage\nDownload\n\n\n\nNow, we can load both files into memory:\n\n\n\nR code\n\n# load spatial data, transform projection\nsa_municipality &lt;- st_read(\"data/spatial/municipality-south-africa-2013.gpkg\")\n\n\nReading layer `municipality-south-africa-2013' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/SA-TIED/data/spatial/municipality-south-africa-2013.gpkg' \n  using driver `GPKG'\nSimple feature collection with 234 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1831416 ymin: -4141363 xmax: 3667419 ymax: -2526543\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# load attribute data\nno_schooling &lt;- read_csv(\"data/attributes/sa-no-schooling.csv\")\n\nRows: 234 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): mn_name\ndbl (3): mn_code, mn_pop, mn_no_school\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function."
  },
  {
    "objectID": "03-spatial-autocorrelation.html#spatial-dependency",
    "href": "03-spatial-autocorrelation.html#spatial-dependency",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "With this dataset, we are interested in analysing the proportion of people without schooling across the country and visualising this information on a map. Let us start by preparing the data for mapping:\n\n\n\nR code\n\n# calculate proportions\nno_schooling &lt;- no_schooling |&gt;\n  mutate(mn_prop_no_schooling = mn_no_school / mn_pop)\n\n# join attribute data onto spatial data\nsa_municipality &lt;- sa_municipality |&gt; \n  left_join(no_schooling, by = c(\"mn_code\" = \"mn_code\"))\n\n\nWe can now create a simple map:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_prop_no_schooling\",\n    n = 5,\n    style = \"jenks\"\n  ) +\n\n  # no legend\n  tm_layout(\n    legend.show = FALSE\n  )\n\n\n\n\n\nFigure 1: Proportions of people having no schooling by municipality.\n\n\n\n\n\n\n\n\n\n\nIf you get the following error when trying to plot the municipality boundaries: Error: Shape contains invalid polygons. Please fix it or set tmap_options(check.and.fix = TRUE) and rerun the plot, run sf_use_s2(FALSE) and try again.\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population that did does not have any schooling appears to be neither random nor uniform, with a tendency for similar values to be found in closely located municipalities. Let us compare our map to a map with the same values which have been randomly permutated:\n\n\n\nR code\n\n# random permutation\nsa_municipality &lt;- sa_municipality |&gt;\n  mutate(mn_prop_no_schooling_random = sample(sa_municipality$mn_prop_no_schooling, replace = FALSE))\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_prop_no_schooling_random\",\n    n = 5,\n    style = \"jenks\"\n  ) +\n\n  # no legend\n  tm_layout(\n    legend.show = FALSE\n  )\n\n\n\n\n\nFigure 2: Proportions of people having no schooling by municipality with randomly permutated values.\n\n\n\n\nLooking at Figure 2, even with the values being randomly permuted, certain patterns seem to emerge. This observation raises an important question: to what extent are the patterns that we see in the actual data actually present? A widely used method to quantify the similarity between neighbouring locations is by calculating Moran’s I statistic. This measure assesses spatial autocorrelation, indicating the degree to which values of a variable cluster spatially — either through similar (positive spatial autocorrelation) or contrasting values (negative spatial autocorrelation, such as the alternating black and white squares on a chessboard)."
  },
  {
    "objectID": "03-spatial-autocorrelation.html#defining-neighbours",
    "href": "03-spatial-autocorrelation.html#defining-neighbours",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "If the purpose of a Moran’s I test is to quantify how similar places are to their neighbours, the first step is to define what constitutes a neighbour. This definition is not necessarily straightforward, because ‘neighbouring’ observations can be determined in various ways, based on either geometry or proximity. The most common methods include:\n\nContiguity: Spatial units are considered neighbours if their polygon boundaries touch.\nFixed Distance: Spatial units are considered neighbours if they fall within a specified distance.\n(K) Nearest Neighbours: Spatial units are considered neighbours if they are among the closest neighbours.\n\nTo capture this information, we need to formalise the spatial relationships within our data by constructing a spatial weights matrix (\\(W_{ij}\\)). This matrix defines which units are neighbours based on our chosen criteria.\n\n\n\n\n\n\nIn the following example, neighbours are defined as places that share a border (i.e., they are contiguous). Currently, it is sufficient for them to meet at a single point — so if two places are triangular, touching corners would count them as neighbours. If, however, you require them to share an edge, rather than just a corner, you can modify the default argument by setting queen = FALSE.\n\n\n\n\n\n\nR code\n\n# create neighbourlist\nsa_mn_nb &lt;- poly2nb(sa_municipality, queen = TRUE)\n\n# inspect\nsummary(sa_mn_nb)\n\n\nNeighbour list object:\nNumber of regions: 234 \nNumber of nonzero links: 1244 \nPercentage nonzero weights: 2.271897 \nAverage number of links: 5.316239 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 1  7 22 35 56 64 40  3  5  1 \n1 least connected region:\n102 with 1 link\n1 most connected region:\n193 with 10 links\n\n\n\n\n\n\n\n\nIt is pretty common for the average number of contiguous neighbours to be about five and the most frequent number of contiguous neighbours to be five or six. It suggests that the spatial configuration of the places is loosely approximating a hexagonal tessellation, although places on the edge of a study region will typically have fewer contiguous neighbours then those at the centre."
  },
  {
    "objectID": "04-spatial-models.html",
    "href": "04-spatial-models.html",
    "title": "1 Spatial Models",
    "section": "",
    "text": "1 Spatial Models"
  }
]