[
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "SA-TIED Geospatial Workshop",
    "section": "",
    "text": "Welcome to the workbook for the SA-TIED Geospatial Workshop. Over the course of this two-day introductory workshop, you will be introduced to the R programming language, learn how to map socio-economic and demographic data, and gain an understanding of the ideas behind spatial models. We will explore the following topics:\n\nFundamentals of using R for data analysis with the tidyverse library\nCreating thematic maps using R with the tmap library\nQuantifying the degree of spatial dependence in a dataset\nIncorporating space into statistical models\n\n\n\n\nWhile there are no specific prerequisites for this workshop, basic familiarity with R, Python or Stata do-files is recommended. You will require a computer with admin rights to install the necessary software.\n\n\n\nThe schedule of the workshop is as follows:\n\n\n\nDay\nType\nTime\nDetails\n\n\n\n\n1\nLecture\n09h00-09h30\nLecture Notes #1\n\n\n1\nSoftware installation\n09h30-09h45\nR for Data Analysis\n\n\n1\nComputer tutorial\n10h00-12h00\nR for Data Analysis\n\n\n1\nLecture\n13h00-13h45\nLecture Notes #2\n\n\n1\nComputer tutorial\n14h00-16h00\nR for Spatial Analysis\n\n\n\n\n\n\n\n\n2\nLecture\n09h00-09h45\nLecture Notes #3\n\n\n2\nComputer tutorial\n10h00-12h00\nSpatial Autocorrelation\n\n\n2\nLecture\n13h00-13h45\nLecture Notes #4\n\n\n2\nComputer tutorial\n14h00-16h00\nSpatial Models\n\n\n\n\n\n\nThis workbook is created using the Quarto publishing system. Elements of this workbook are partially partially based on and modified from:\n\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nThe Mapping and Modelling Geographic Data in R course by Richard Harris\n\nThe workbooks contains data sourced from the 2011 South African Census of Population:\n\nStatistics South Africa. South African Census Community Profiles 2011 [dataset]. Version 1. Pretoria: Statistics SA [producer], 2014. Cape Town: DataFirst [distributor], 2015. DOI: https://doi.org/10.25828/6n0m-7m52"
  },
  {
    "objectID": "00-index.html#welcome",
    "href": "00-index.html#welcome",
    "title": "SA-TIED Geospatial Workshop",
    "section": "",
    "text": "Welcome to the workbook for the SA-TIED Geospatial Workshop. Over the course of this two-day introductory workshop, you will be introduced to the R programming language, learn how to map socio-economic and demographic data, and gain an understanding of the ideas behind spatial models. We will explore the following topics:\n\nFundamentals of using R for data analysis with the tidyverse library\nCreating thematic maps using R with the tmap library\nQuantifying the degree of spatial dependence in a dataset\nIncorporating space into statistical models"
  },
  {
    "objectID": "00-index.html#prerequisites",
    "href": "00-index.html#prerequisites",
    "title": "SA-TIED Geospatial Workshop",
    "section": "",
    "text": "While there are no specific prerequisites for this workshop, basic familiarity with R, Python or Stata do-files is recommended. You will require a computer with admin rights to install the necessary software."
  },
  {
    "objectID": "00-index.html#workshop-overview",
    "href": "00-index.html#workshop-overview",
    "title": "SA-TIED Geospatial Workshop",
    "section": "",
    "text": "The schedule of the workshop is as follows:\n\n\n\nDay\nType\nTime\nDetails\n\n\n\n\n1\nLecture\n09h00-09h30\nLecture Notes #1\n\n\n1\nSoftware installation\n09h30-09h45\nR for Data Analysis\n\n\n1\nComputer tutorial\n10h00-12h00\nR for Data Analysis\n\n\n1\nLecture\n13h00-13h45\nLecture Notes #2\n\n\n1\nComputer tutorial\n14h00-16h00\nR for Spatial Analysis\n\n\n\n\n\n\n\n\n2\nLecture\n09h00-09h45\nLecture Notes #3\n\n\n2\nComputer tutorial\n10h00-12h00\nSpatial Autocorrelation\n\n\n2\nLecture\n13h00-13h45\nLecture Notes #4\n\n\n2\nComputer tutorial\n14h00-16h00\nSpatial Models"
  },
  {
    "objectID": "00-index.html#acknowledgements",
    "href": "00-index.html#acknowledgements",
    "title": "SA-TIED Geospatial Workshop",
    "section": "",
    "text": "This workbook is created using the Quarto publishing system. Elements of this workbook are partially partially based on and modified from:\n\nThe GEOG0030: Geocomputation 2023-2024 workbook by Justin van Dijk\nThe Mapping and Modelling Geographic Data in R course by Richard Harris\n\nThe workbooks contains data sourced from the 2011 South African Census of Population:\n\nStatistics South Africa. South African Census Community Profiles 2011 [dataset]. Version 1. Pretoria: Statistics SA [producer], 2014. Cape Town: DataFirst [distributor], 2015. DOI: https://doi.org/10.25828/6n0m-7m52"
  },
  {
    "objectID": "01-getting-started.html",
    "href": "01-getting-started.html",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "R is a programming language originally designed for conducting statistical analysis and creating graphics. The major advantage of using R is that it can be used on any computer operating system, and is free for anyone to use and contribute to. Because of this, it has rapidly become the statistical language of choice for many academics and has a large user community with people constantly contributing new packages to carry out all manner of statistical, graphical, and importantly for us, geographical tasks.\nInstalling R takes a few relatively simple steps involving two pieces of software. First there is the R programme itself. Follow these steps to get it installed on your computer:\n\nNavigate in your browser to the download page: [Link]\nIf you use a Windows computer, click on Download R for Windows. Then click on base. Download and install R 4.4.x for Windows. If you use a Mac computer, click on Download R for macOS and download and install R-4.4.x.arm64.pkg for Apple silicon Macs and R-4.4.x.x86_64.pkg for older Intel-based Macs.\n\nThat is it! You now have successfully installed R onto your computer. To make working with the R language a little bit easier we also need to install something called an Integrated Development Environment (IDE). We will use RStudio Desktop:\n\nNavigate to the official webpage of RStudio: [Link]\nDownload and install RStudio on your computer.\n\nAfter this, start RStudio to see if the installation was successful. Your screen should look something like what is shown in Figure 1.\n\n\n\n\n\nFigure 1: The RStudio interface.\n\n\n\n\nThe main windows that we will be using are:\n\n\n\n\n\n\n\nWindow\nPurpose\n\n\n\n\nConsole\nWhere we write one-off code such as installing packages.\n\n\nFiles\nWhere we can see where our files are stored on our computer system.\n\n\nEnvironment\nWhere our variables or objects are kept in memory.\n\n\nPlots\nWhere the outputs of our graphs, charts and maps are shown.\n\n\n\n\n\n\nNow we have installed R and RStudio, we need to customise R. Many useful R functions come in packages, these are free libraries of code written and made available by other R users. This includes packages specifically developed for data cleaning, data wrangling, visualisation, mapping, and spatial analysis. To save us some time, we will install all R packages that we will need for the workshop in one go. Start RStudio, and copy and paste the following code into the console window. You can execute the code by pressing the Return button on your keyboard. Depending on your computer’s specifications and the internet connection, this may take a short while.\n\n\n\nR code\n\n# install packages\ninstall.packages(c(\"tidyverse\", \"haven\", \"sf\", \"tmap\", \"spdep\", \"GWmodel\"))\n\n\n\n\n\n\n\n\nFor Linux and macOS users who are new to working with spatial data in R, this installation of some of these libraries may fail since additional (non-R) libraries are required (which are automatically installed for Windows users). If this is the case, please refer to the information pages of the sf library for instructions on how to install these additional libraries.\n\n\n\nOnce you have installed the packages, we need to check whether we can in fact load them into R. Copy and paste the following code into the console, and execute by pressing Return on your keyboard again.\n\n\n\nR code\n\n# load packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\nlibrary(GWmodel)\n\n\nYou will see some information printed to your console but as long as you do not get any of the messages below, the installation was successful. If you do get any of the messages below it means that the package was not properly installed, so try to install the package in question again.\n\nError: package or namespace load failed for &lt;packagename&gt;\nError: package '&lt;packagename&gt;' could not be loaded\nError in library(&lt;packagename&gt;) : there is no package called '&lt;packagename&gt;'\n\n\n\n\n\n\n\nMany packages require additional software components, known as dependencies, to function properly. Occasionally, when you install a package, some of these dependencies are not installed automatically. When you then try to load a package, you might encounter error messages that relate to a package that you did not explicitly loaded or installed. If this is the case, it is likely due to a missing dependency. To resolve this, identify the missing dependency and install it using the command install.packages('&lt;dependencyname&gt;'). Afterwards, try loading your packages again.\n\n\n\n\n\n\nUnlike traditional statistical analysis software like Microsoft Excel or Stata, which often rely on point-and-click interfaces, R requires users to input commands to perform tasks such as loading datasets and fitting models. This command-based approach is typically done by writing scripts, which not only document your workflow but also allow for easy repetition of tasks.\n\n\n\n\n\n\nIf you are familiar with Stata’s do-file you will find that the approach to programming in R is comparable.\n\n\n\nLet us begin by exploring some of R’s built-in functionality through a simple exercise: creating a few variables and performing basic mathematical operations.\n\n\n\n\n\n\nIn your RStudio console, you will notice a prompt sign &gt; on the left-hand side. This is where you can directly interact with R. If any text appears in red, it indicates an error or warning. When you see the &gt;, it means R is ready for your next command. However, if you see a +, it means you have not completed the previous line of code. This often happens when brackets are left open or a command is not properly finished as R expected.\n\n\n\nAt its core, every programming language can be used as a powerful calculator. Type in 10 * 12 into the console and execute.\n\n\n\nR code\n\n# multiplication\n10 * 12\n\n\n[1] 120\n\n\nOnce you press return, you should see the answer of 120 returned below.\n\n\nInstead of using raw numbers or standalone values, it is more effective to store these values in variables, which allows for easy reference later. In R, this process is known as creating an object, and the object is stored as a variable. To assign a value to a variable, use the &lt;- symbol. Let us create two variables to experiment with this concept.\nType in ten &lt;- 10 into the console and execute.\n\n\n\nR code\n\n# store a variable\nten &lt;- 10\n\n\nYou will see nothing is returned in the console. However, if you check your environment window, you will see that a new variable has appeared, containing the value you assigned.\nType in twelve &lt;- 12 into the console and execute.\n\n\n\nR code\n\n# store a variable\ntwelve &lt;- 12\n\n\nAgain, nothing will be returned to the console, but be sure to check your environment window for the newly created variable. We have now stored two numbers in our environment and assigned them variable names for easy reference. R stores these objects as variables in your computer’s RAM memory, allowing for quick processing. Keep in mind that without saving your environment, these variables will be lost when you close R and you would need to run your code again.\nNow that we have our variables, let us proceed with a simple multiplication. Type in ten * twelve into the console and execute.\n\n\n\nR code\n\n# using variables\nten * twelve\n\n\n[1] 120\n\n\nYou should see the output in the console of 120. While this calculation may seem trivial, it demonstrates a powerful concept: these variables can be treated just like the values they contain.\nNext, type in ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# using variables and values\nten * twelve * 8\n\n\n[1] 960\n\n\nYou should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable.\nType output &lt;- ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# store output\noutput &lt;- ten * twelve * 8\n\n\nBecause we are storing the output of our maths to a new variable, the answer is not returned to the screen but is kept in memory.\n\n\n\nWe can ask R to return the value of the output variable by simply typing its name into the console. You should see that it returns the same value as the earlier calculation.\n\n\n\nR code\n\n# return value\noutput\n\n\n[1] 960\n\n\n\n\n\nWe can also store variables of different data types, not just numbers but text as well.\nType in str_variable &lt;- \"Hello Pretoria\" into the console and execute.\n\n\n\nR code\n\n# store a variable\nstr_variable &lt;- \"Hello Pretoria\"\n\n\nWe have just stored our sentence made from a combination of characters. A variable that stores text is known as a string. A string is always denoted by the use of single ('') or double (\"\") quotation marks.\nType in str_variable into the console and execute.\n\n\n\nR code\n\n# return variable\nstr_variable\n\n\n[1] \"Hello Pretoria\"\n\n\nYou should see our entire sentence returned, enclosed in quotation marks (\"\").\n\n\n\nWe can also call a function on our variable. For example, we can ask R to print our variable, which will give us the same output as accessing it directly via the console.\nType in print(str_variable) into the console and execute.\n\n\n\nR code\n\n# printing a variable\nprint(str_variable)\n\n\n[1] \"Hello Pretoria\"\n\n\nYou can type ?print into the console to learn more about the print() function. This method works with any function, giving you access to its documentation. Understanding this documentation is crucial for using the function correctly and interpreting its output.\n\n\n\nR code\n\n# open documentation of the print function\n?print\n\n\n\n\n\n\n\n\nIn many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one required argument although most functions will also have several optional or default parameters.\n\n\n\n\n\n\nWithin the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is.\nType in typeof(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(str_variable)\n\n\n[1] \"character\"\n\n\nYou should see the answer: character. As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables too.\nType in typeof(ten) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(ten)\n\n\n[1] \"double\"\n\n\nYou should see the answer: double. Alternatively, we can check the class of a variable by using the class() function.\nType in class(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(str_variable)\n\n\n[1] \"character\"\n\n\nIn this case, you will see the same result as before because, in R, both the class and type of a string are character. Other programming languages might use the term string instead, but it essentially means the same thing.\nType in class(ten) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(ten)\n\n\n[1] \"numeric\"\n\n\nIn this case, you will get a different result because the class of this variable is numeric. Numeric objects in R can be either doubles (decimals) or integers (whole numbers). You can test whether the ten variable is an integer by using specific functions designed for this purpose.\nType in is.integer(ten) into the console and execute.\n\n\n\nR code\n\n# call the integer() function\nis.integer(ten)\n\n\n[1] FALSE\n\n\nYou should see the result FALSE. As we know from the typeof() function, the ten variable is stored as a double, so it cannot be an integer.\n\n\n\n\n\n\nWhilst knowing how to distinguish between different data types might not seem important now, the difference betwee a double and an integer can quite easily lead to unexpected errors.\n\n\n\nWe can also check the length of our a variable.\nType in length(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the length() function\nlength(str_variable)\n\n\n[1] 1\n\n\nYou should get the answer 1 because we only have one set of characters. We can also determine the length of each set of characters, which tells us the length of the string contained in the variable.\nType in nchar(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the nchar() function\nnchar(str_variable)\n\n\n[1] 14\n\n\nYou should get an answer of 14.\n\n\n\nVariables are not constricted to one value, but can be combined to create larger objects. Type in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute.\n\n\n\nR code\n\n# store a new variable\ntwo_str_variable &lt;- c(\"This is our second string variable\", \"It has two parts to it\")\n\n\nIn this code, we have created a new variable using the c() function, which combines values into a vector or list. We provided the c() function with two sets of strings, separated by a comma and enclosed within the function’s parentheses.\nLet us now try both our length() and nchar() on our new variable and see what the results are:\n\n\n\nR code\n\n# call the length() function\nlength(two_str_variable)\n\n\n[1] 2\n\n# call the nchar() function\nnchar(two_str_variable)\n\n[1] 34 22\n\n\nYou should notice that the length() function now returned a 2 and the nchar() function returned two values of 34 and 22.\n\n\n\n\n\n\nYou may have noticed that each line of code in the examples includes a comment explaining its purpose. In R, comments are created using the hash symbol #. This symbol instructs R to ignore the commented line when executing the code. Comments are useful for understanding your code when you revisit it later or when sharing it with others.\n\n\n\nExtending the concept of multi-value objects to two dimensions results in a dataframe. A dataframe is the de facto data structure for most tabular data. We will use functions from the tidyverse library, a suite of packages—to load a data file and conduct exploratory data analysis. Within the tidyverse, dataframes are referred to as tibbles. Some of the most important and useful functions come from the tidyr and dplyr packages, including:\n\n\n\n\n\n\n\n\nPackage\nFunction\nUse to\n\n\n\n\ndplyr\nselect()\nselect columns\n\n\ndplyr\nfilter()\nselect rows\n\n\ndplyr\nmutate()\ntransform or recode variables\n\n\ndplyr\nsummarise()\nsummarise data\n\n\ndplyr\ngroup_by()\ngroup data into subgroups for further processing\n\n\ntidyr\npivot_longer()\nconvert data from wide format to long format\n\n\ntidyr\npivot_wider()\nconvert long format dataset to wide format\n\n\n\n\n\n\n\n\n\nFor more information on the tidyverse you can refer to www.tidyverse.org.\n\n\n\n\n\n\n\nIn RStudio, scripts allow us to build and save code that can be run repeatedly. We can organise these scripts into RStudio projects, which consolidate all files related to an analysis such as input data, R scripts, results, figures, and more. This organisation helps keep track of all data, input, and output, while enabling us to create standalone scripts for each part of our analysis. Additionally, it simplifies managing directories and filepaths.\nNavigate to File -&gt; New Project -&gt; New Directory, and create a folder with the name GeoSpatial-Workshop24. Click on Create Project. You should now see your main window switch to this new project and when you check your files window, you should see a new R Project called GeoSpatial-Workshop24.\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore _ or hyphen - if you like.\n\n\n\n\n\n\nWith the basics covered, let us dive into loading a real dataset, performing data cleaning, and conducting some exploratory data analysis on the isiXhosa speaking population. We will work with a dataset that contains counts of the primary languages spoken in South African, sourced from the South African Census Community Profiles 2011 and made available through the DataFirst data service. You can download a copy of the file through the link below and save it in your project folder under data/attributes.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 Language Table\ndta\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Create a folder named scripts, and save your script as 01-language-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(haven)\n\n\n\n\n\n\n\n\nIn RStudio, there are two primary ways to run a script: all at once or by executing individual lines or chunks of code. As a beginner, it is often beneficial to use the line-by-line approach, as it allows you to test your code interactively and catch errors early.\nTo run line-by-line:\n\nBy clicking: Highlight the line or chunk of code you want to run, then go to Code and select Run selected lines.\nBy key commands: Highlight the code, then press Ctl (or Cmd on Mac) + Return.\n\nTo run the whole script:\n\nBy clicking: In the scripting window, click Run in the top-right corner and choose Run All.\nBy key commands: Press Option + Ctrl (or Cmd) + R.\n\nIf a script gets stuck or you realise there is an error in your code, you may need to interrupt R. To do this, go to Session -&gt; Interrupt R. If the interruption doesn’t work, you might need to terminate and restart R.\n\n\n\n\n\nNext, we can load the sa-language.dta file into R. Using the haven library, R is capable to read a large range of different filetypes. This includes Stata dta files:\n\n\n\nR code\n\n# load data\natt &lt;- read_dta(\"data/attributes/sa-language.dta\")\n\n\n\n\n\n\n\n\nIf using a Windows machine, you may need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nLet us have a look at the dataframe:\n\n\n\nR code\n\n# inspect columns, rows\nncol(att)\n\n\n[1] 28\n\nnrow(att)\n\n[1] 84908\n\n# inspect data\nhead(att)\n\n# A tibble: 6 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1600001 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n2  1600002 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n3  1600003 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n4  1600004 160010001 Vredenda…  160010 Vreden… WC011        160 Matzik… DC1     \n5  1600005 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n6  1600006 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n# inspect column names\nnames(att)\n\n [1] \"sal_code\" \"sp_code\"  \"sp_name\"  \"mp_code\"  \"mp_name\"  \"mn_mdb_c\"\n [7] \"mn_code\"  \"mn_name\"  \"dc_mdb_c\" \"dc_code\"  \"dc_name\"  \"pr_code\" \n[13] \"pr_name\"  \"lng_1\"    \"lng_2\"    \"lng_3\"    \"lng_4\"    \"lng_5\"   \n[19] \"lng_6\"    \"lng_7\"    \"lng_8\"    \"lng_9\"    \"lng_10\"   \"lng_11\"  \n[25] \"lng_12\"   \"lng_13\"   \"lng_14\"   \"lng_15\"  \n\n\nTo access specific columns or rows in a dataframe, we can use indexing. Indexing refers to the numbering assigned to each element within a data structure, allowing us to precisely select and manipulate data.\nTo access the first row of a dataframe, you would use dataframe[1, ], and to access the first column, you would use dataframe[, 1]. The comma separates the row and column indices, with the absence of a number indicating all rows or columns respectively.\n\n\n\n\n\n\nIn R, indexing begins at 1, meaning that the first element of any data structure is accessed with the index [1]. This is different from many other programming languages, such as Python or Java, where indexing typically starts at 0.\n\n\n\n\n\n\nR code\n\n# index columns\natt[, 1]\n\n\n# A tibble: 84,908 × 1\n   sal_code\n      &lt;dbl&gt;\n 1  1600001\n 2  1600002\n 3  1600003\n 4  1600004\n 5  1600005\n 6  1600006\n 7  1600007\n 8  1600008\n 9  1600009\n10  1600010\n# ℹ 84,898 more rows\n\n# index rows\natt[1, ]\n\n# A tibble: 1 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1600001 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n# index cell\natt[1, 1]\n\n# A tibble: 1 × 1\n  sal_code\n     &lt;dbl&gt;\n1  1600001\n\n\n\n\n\n\n\n\nAlternatively, you can access the data within individual columns by referring to their names using the $ operator. This allows you to easily extract and work with a specific column without needing to know its position in the dataframe. For example, if your dataframe is named dataframe and you want to access a column named age, you would use dataframe$age. This method is especially useful when your data has many columns or when the column positions may change, because it relies on the column names rather than their index numbers.\n\n\n\n\n\n\nNow that we have loaded and inspected our data, we can examine its distribution. We will focus on the column that contains the counts of individuals who speak isiXhosa as their primary language, aggregated by Small Area Layer — the most granular geographic level available in the 2011 South African Census of Population. This data is stored in the lng_4 column of the dataframe.\n\n\n\nR code\n\n# mean\nmean(att$lng_4)\n\n\n[1] 95.99431\n\n# median\nmedian(att$lng_4)\n\n[1] 9\n\n# range\nrange(att$lng_4)\n\n[1]    0 6465\n\n# summary\nsummary(att$lng_4)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    9.00   95.99   72.00 6465.00 \n\n\nWe can also call some functions to quickly draw a boxplot, histogram, or scatterplot:\n\n\n\nR code\n\n# boxplot\nboxplot(att$lng_4, horizontal = TRUE)\n\n\n\n\n\nFigure 2: Quick boxplot.\n\n\n\n\n\n\n\nR code\n\n# histogram\nhist(att$lng_4, breaks = 50, xlab = \"Number of Small Area Layers\", main = \"Number of isiXhosa speakers\")\n\n\n\n\n\nFigure 3: Quick histogram.\n\n\n\n\n\n\n\nR code\n\n# bivariate plot\nplot(att$lng_4, att$lng_2, xlab = \"isiXhosa\", ylab = \"English\")\n\n\n\n\n\nFigure 4: Quick bivariate plot.\n\n\n\n\n\n\n\nThe dataset is quite extensive, containing counts for all 84,908 Small Area Layers. To make our analysis more manageable, let’s focus on a subset of the data, specifically zooming in on Cape Town. We can achieve this by filtering the data using the dc_name column, which corresponds to the District Municipality Name.\n\n\n\nR code\n\n# prepare data: filter out Cape Town\natt &lt;- att |&gt;\n    filter(dc_name == \"City of Cape Town\")\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might seem a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\nIn this case, we have overwritten our initial object, though this is not strictly necessary. Our object now contains a subset of the full language dataset that we initially loaded.\n\n\n\nR code\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1990001 199014025 Paarden …  199014 Milner… CPT          199 City o… CPT     \n2  1990002 199016089 Mimosa     199016 Bellvi… CPT          199 City o… CPT     \n3  1990003 199041008 Signal H…  199041 Cape T… CPT          199 City o… CPT     \n4  1990004 199017021 Durbanvi…  199017 Durban… CPT          199 City o… CPT     \n5  1990005 199053012 Pine Hav…  199053 Simon'… CPT          199 City o… CPT     \n6  1990006 199030007 Guguleth…  199030 Gugule… CPT          199 City o… CPT     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n\n\n\n\nThe next thing that we probably want to do is standardise our data. A common challenge with spatial data is that the spatial units aren not always the same size — some Small Area Layers may have larger populations, while others have fewer residents. This makes it difficult to compare absolute counts fairly. To address this, we can sum the counts across all relevant columns to determine the total number of speakers in each area. Then, we can calculate the proportion of isiXhosa speakers within each Small Area Layer:\n\n\n\nR code\n\n# prepare data: sum across\natt &lt;- att |&gt;\n    rowwise() |&gt;\n    mutate(sal_pop = sum(across(starts_with(\"lng\")), na.rm = TRUE))\n\n# prepare data: calculate percentages\natt &lt;- att |&gt;\n    mutate(sal_prop_xhosa = lng_4/sal_pop)\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 30\n# Rowwise: \n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1990001 199014025 Paarden …  199014 Milner… CPT          199 City o… CPT     \n2  1990002 199016089 Mimosa     199016 Bellvi… CPT          199 City o… CPT     \n3  1990003 199041008 Signal H…  199041 Cape T… CPT          199 City o… CPT     \n4  1990004 199017021 Durbanvi…  199017 Durban… CPT          199 City o… CPT     \n5  1990005 199053012 Pine Hav…  199053 Simon'… CPT          199 City o… CPT     \n6  1990006 199030007 Guguleth…  199030 Gugule… CPT          199 City o… CPT     \n# ℹ 21 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;, sal_pop &lt;dbl&gt;, sal_prop_xhosa &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe now have the percentage of people who speak isiXhosa as their primary language for each Small Area Layer. To provide a more meaningful representation, however, we can aggregate the data by sub places. This involves grouping our data by sp_code, which contains unique sub place codes. We will then:\n\nSum the number of isiXhosa speakers within each sub place and store this in a new variable.\nSum the total number of people within each sub place and store this in another new variable.\nExtract the distinct values for each sub place.\n\nBy using the pipe function again, we can chain these steps together efficiently as follows:\n\n\n\nR code\n\n# prepare data: aggregate small areas to sub places\natt &lt;- att |&gt;\n    group_by(sp_code) |&gt;\n    mutate(sp_pop = sum(sal_pop)) |&gt;\n    mutate(sp_xhosa = sum(lng_4)) |&gt;\n    ungroup() |&gt;\n    distinct(sp_code, sp_pop, sp_xhosa)\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 3\n    sp_code sp_pop sp_xhosa\n      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 199014025     12        0\n2 199016089   1212        0\n3 199041008     12        0\n4 199017021   7893      105\n5 199053012      9        0\n6 199030007  55242    47385\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can save this dataset so that we can easily load it the next time we want to work with this by writing it to a csv file.\n\n\n\nR code\n\n# write data\nwrite_csv(x = att, file = \"data/attributes/sa-language.csv\")\n\n\n\n\n\n\nThis concludes this session. Please try to complete the following tasks:\n\nCreate a histogram showing the distribution of the number of isiXhosa speakers in Cape Town, grouped by sub place.\nCreate a boxplot of the proportion of isiXhosa-speakers in Cape Town, grouped by sub place.\n\nNext, use the link below to download another dataset sourced from the South African Census Community Profiles 2011 that contains information on internet access. Save the file in your project folder under data/attributes.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 Internet Access Table\ndta\nDownload\n\n\n\nUse this dataset to:\n\nCreate a histogram of the distribution of the number of people with internet access at home in the City of Johannesburg, grouped by sub place.\nCreate a boxplot of the proportion of people with internet access at home in the City of Johannesburg, grouped by sub place.\n\n\n\n\nThere are several ways to achieve the above, but if you would like to see an example solution, you can find it here: [Link]"
  },
  {
    "objectID": "01-getting-started.html#installation-of-r",
    "href": "01-getting-started.html#installation-of-r",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "R is a programming language originally designed for conducting statistical analysis and creating graphics. The major advantage of using R is that it can be used on any computer operating system, and is free for anyone to use and contribute to. Because of this, it has rapidly become the statistical language of choice for many academics and has a large user community with people constantly contributing new packages to carry out all manner of statistical, graphical, and importantly for us, geographical tasks.\nInstalling R takes a few relatively simple steps involving two pieces of software. First there is the R programme itself. Follow these steps to get it installed on your computer:\n\nNavigate in your browser to the download page: [Link]\nIf you use a Windows computer, click on Download R for Windows. Then click on base. Download and install R 4.4.x for Windows. If you use a Mac computer, click on Download R for macOS and download and install R-4.4.x.arm64.pkg for Apple silicon Macs and R-4.4.x.x86_64.pkg for older Intel-based Macs.\n\nThat is it! You now have successfully installed R onto your computer. To make working with the R language a little bit easier we also need to install something called an Integrated Development Environment (IDE). We will use RStudio Desktop:\n\nNavigate to the official webpage of RStudio: [Link]\nDownload and install RStudio on your computer.\n\nAfter this, start RStudio to see if the installation was successful. Your screen should look something like what is shown in Figure 1.\n\n\n\n\n\nFigure 1: The RStudio interface.\n\n\n\n\nThe main windows that we will be using are:\n\n\n\n\n\n\n\nWindow\nPurpose\n\n\n\n\nConsole\nWhere we write one-off code such as installing packages.\n\n\nFiles\nWhere we can see where our files are stored on our computer system.\n\n\nEnvironment\nWhere our variables or objects are kept in memory.\n\n\nPlots\nWhere the outputs of our graphs, charts and maps are shown."
  },
  {
    "objectID": "01-getting-started.html#customisation-of-r",
    "href": "01-getting-started.html#customisation-of-r",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "Now we have installed R and RStudio, we need to customise R. Many useful R functions come in packages, these are free libraries of code written and made available by other R users. This includes packages specifically developed for data cleaning, data wrangling, visualisation, mapping, and spatial analysis. To save us some time, we will install all R packages that we will need for the workshop in one go. Start RStudio, and copy and paste the following code into the console window. You can execute the code by pressing the Return button on your keyboard. Depending on your computer’s specifications and the internet connection, this may take a short while.\n\n\n\nR code\n\n# install packages\ninstall.packages(c(\"tidyverse\", \"haven\", \"sf\", \"tmap\", \"spdep\", \"GWmodel\"))\n\n\n\n\n\n\n\n\nFor Linux and macOS users who are new to working with spatial data in R, this installation of some of these libraries may fail since additional (non-R) libraries are required (which are automatically installed for Windows users). If this is the case, please refer to the information pages of the sf library for instructions on how to install these additional libraries.\n\n\n\nOnce you have installed the packages, we need to check whether we can in fact load them into R. Copy and paste the following code into the console, and execute by pressing Return on your keyboard again.\n\n\n\nR code\n\n# load packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\nlibrary(GWmodel)\n\n\nYou will see some information printed to your console but as long as you do not get any of the messages below, the installation was successful. If you do get any of the messages below it means that the package was not properly installed, so try to install the package in question again.\n\nError: package or namespace load failed for &lt;packagename&gt;\nError: package '&lt;packagename&gt;' could not be loaded\nError in library(&lt;packagename&gt;) : there is no package called '&lt;packagename&gt;'\n\n\n\n\n\n\n\nMany packages require additional software components, known as dependencies, to function properly. Occasionally, when you install a package, some of these dependencies are not installed automatically. When you then try to load a package, you might encounter error messages that relate to a package that you did not explicitly loaded or installed. If this is the case, it is likely due to a missing dependency. To resolve this, identify the missing dependency and install it using the command install.packages('&lt;dependencyname&gt;'). Afterwards, try loading your packages again."
  },
  {
    "objectID": "01-getting-started.html#getting-started-with-r",
    "href": "01-getting-started.html#getting-started-with-r",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "Unlike traditional statistical analysis software like Microsoft Excel or Stata, which often rely on point-and-click interfaces, R requires users to input commands to perform tasks such as loading datasets and fitting models. This command-based approach is typically done by writing scripts, which not only document your workflow but also allow for easy repetition of tasks.\n\n\n\n\n\n\nIf you are familiar with Stata’s do-file you will find that the approach to programming in R is comparable.\n\n\n\nLet us begin by exploring some of R’s built-in functionality through a simple exercise: creating a few variables and performing basic mathematical operations.\n\n\n\n\n\n\nIn your RStudio console, you will notice a prompt sign &gt; on the left-hand side. This is where you can directly interact with R. If any text appears in red, it indicates an error or warning. When you see the &gt;, it means R is ready for your next command. However, if you see a +, it means you have not completed the previous line of code. This often happens when brackets are left open or a command is not properly finished as R expected.\n\n\n\nAt its core, every programming language can be used as a powerful calculator. Type in 10 * 12 into the console and execute.\n\n\n\nR code\n\n# multiplication\n10 * 12\n\n\n[1] 120\n\n\nOnce you press return, you should see the answer of 120 returned below.\n\n\nInstead of using raw numbers or standalone values, it is more effective to store these values in variables, which allows for easy reference later. In R, this process is known as creating an object, and the object is stored as a variable. To assign a value to a variable, use the &lt;- symbol. Let us create two variables to experiment with this concept.\nType in ten &lt;- 10 into the console and execute.\n\n\n\nR code\n\n# store a variable\nten &lt;- 10\n\n\nYou will see nothing is returned in the console. However, if you check your environment window, you will see that a new variable has appeared, containing the value you assigned.\nType in twelve &lt;- 12 into the console and execute.\n\n\n\nR code\n\n# store a variable\ntwelve &lt;- 12\n\n\nAgain, nothing will be returned to the console, but be sure to check your environment window for the newly created variable. We have now stored two numbers in our environment and assigned them variable names for easy reference. R stores these objects as variables in your computer’s RAM memory, allowing for quick processing. Keep in mind that without saving your environment, these variables will be lost when you close R and you would need to run your code again.\nNow that we have our variables, let us proceed with a simple multiplication. Type in ten * twelve into the console and execute.\n\n\n\nR code\n\n# using variables\nten * twelve\n\n\n[1] 120\n\n\nYou should see the output in the console of 120. While this calculation may seem trivial, it demonstrates a powerful concept: these variables can be treated just like the values they contain.\nNext, type in ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# using variables and values\nten * twelve * 8\n\n\n[1] 960\n\n\nYou should get an answer of 960. As you can see, we can mix variables with raw values without any problems. We can also store the output of variable calculations as a new variable.\nType output &lt;- ten * twelve * 8 into the console and execute.\n\n\n\nR code\n\n# store output\noutput &lt;- ten * twelve * 8\n\n\nBecause we are storing the output of our maths to a new variable, the answer is not returned to the screen but is kept in memory.\n\n\n\nWe can ask R to return the value of the output variable by simply typing its name into the console. You should see that it returns the same value as the earlier calculation.\n\n\n\nR code\n\n# return value\noutput\n\n\n[1] 960\n\n\n\n\n\nWe can also store variables of different data types, not just numbers but text as well.\nType in str_variable &lt;- \"Hello Pretoria\" into the console and execute.\n\n\n\nR code\n\n# store a variable\nstr_variable &lt;- \"Hello Pretoria\"\n\n\nWe have just stored our sentence made from a combination of characters. A variable that stores text is known as a string. A string is always denoted by the use of single ('') or double (\"\") quotation marks.\nType in str_variable into the console and execute.\n\n\n\nR code\n\n# return variable\nstr_variable\n\n\n[1] \"Hello Pretoria\"\n\n\nYou should see our entire sentence returned, enclosed in quotation marks (\"\").\n\n\n\nWe can also call a function on our variable. For example, we can ask R to print our variable, which will give us the same output as accessing it directly via the console.\nType in print(str_variable) into the console and execute.\n\n\n\nR code\n\n# printing a variable\nprint(str_variable)\n\n\n[1] \"Hello Pretoria\"\n\n\nYou can type ?print into the console to learn more about the print() function. This method works with any function, giving you access to its documentation. Understanding this documentation is crucial for using the function correctly and interpreting its output.\n\n\n\nR code\n\n# open documentation of the print function\n?print\n\n\n\n\n\n\n\n\nIn many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one required argument although most functions will also have several optional or default parameters.\n\n\n\n\n\n\nWithin the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the typeof() function to check what data type our variable is.\nType in typeof(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(str_variable)\n\n\n[1] \"character\"\n\n\nYou should see the answer: character. As evident, our str_variable is a character data type. We can try testing this out on one of our earlier variables too.\nType in typeof(ten) into the console and execute.\n\n\n\nR code\n\n# call the typeof() function\ntypeof(ten)\n\n\n[1] \"double\"\n\n\nYou should see the answer: double. Alternatively, we can check the class of a variable by using the class() function.\nType in class(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(str_variable)\n\n\n[1] \"character\"\n\n\nIn this case, you will see the same result as before because, in R, both the class and type of a string are character. Other programming languages might use the term string instead, but it essentially means the same thing.\nType in class(ten) into the console and execute.\n\n\n\nR code\n\n# call the class() function\nclass(ten)\n\n\n[1] \"numeric\"\n\n\nIn this case, you will get a different result because the class of this variable is numeric. Numeric objects in R can be either doubles (decimals) or integers (whole numbers). You can test whether the ten variable is an integer by using specific functions designed for this purpose.\nType in is.integer(ten) into the console and execute.\n\n\n\nR code\n\n# call the integer() function\nis.integer(ten)\n\n\n[1] FALSE\n\n\nYou should see the result FALSE. As we know from the typeof() function, the ten variable is stored as a double, so it cannot be an integer.\n\n\n\n\n\n\nWhilst knowing how to distinguish between different data types might not seem important now, the difference betwee a double and an integer can quite easily lead to unexpected errors.\n\n\n\nWe can also check the length of our a variable.\nType in length(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the length() function\nlength(str_variable)\n\n\n[1] 1\n\n\nYou should get the answer 1 because we only have one set of characters. We can also determine the length of each set of characters, which tells us the length of the string contained in the variable.\nType in nchar(str_variable) into the console and execute.\n\n\n\nR code\n\n# call the nchar() function\nnchar(str_variable)\n\n\n[1] 14\n\n\nYou should get an answer of 14.\n\n\n\nVariables are not constricted to one value, but can be combined to create larger objects. Type in two_str_variable &lt;- c(\"This is our second variable\", \"It has two parts to it\") into the console and execute.\n\n\n\nR code\n\n# store a new variable\ntwo_str_variable &lt;- c(\"This is our second string variable\", \"It has two parts to it\")\n\n\nIn this code, we have created a new variable using the c() function, which combines values into a vector or list. We provided the c() function with two sets of strings, separated by a comma and enclosed within the function’s parentheses.\nLet us now try both our length() and nchar() on our new variable and see what the results are:\n\n\n\nR code\n\n# call the length() function\nlength(two_str_variable)\n\n\n[1] 2\n\n# call the nchar() function\nnchar(two_str_variable)\n\n[1] 34 22\n\n\nYou should notice that the length() function now returned a 2 and the nchar() function returned two values of 34 and 22.\n\n\n\n\n\n\nYou may have noticed that each line of code in the examples includes a comment explaining its purpose. In R, comments are created using the hash symbol #. This symbol instructs R to ignore the commented line when executing the code. Comments are useful for understanding your code when you revisit it later or when sharing it with others.\n\n\n\nExtending the concept of multi-value objects to two dimensions results in a dataframe. A dataframe is the de facto data structure for most tabular data. We will use functions from the tidyverse library, a suite of packages—to load a data file and conduct exploratory data analysis. Within the tidyverse, dataframes are referred to as tibbles. Some of the most important and useful functions come from the tidyr and dplyr packages, including:\n\n\n\n\n\n\n\n\nPackage\nFunction\nUse to\n\n\n\n\ndplyr\nselect()\nselect columns\n\n\ndplyr\nfilter()\nselect rows\n\n\ndplyr\nmutate()\ntransform or recode variables\n\n\ndplyr\nsummarise()\nsummarise data\n\n\ndplyr\ngroup_by()\ngroup data into subgroups for further processing\n\n\ntidyr\npivot_longer()\nconvert data from wide format to long format\n\n\ntidyr\npivot_wider()\nconvert long format dataset to wide format\n\n\n\n\n\n\n\n\n\nFor more information on the tidyverse you can refer to www.tidyverse.org."
  },
  {
    "objectID": "01-getting-started.html#rstudio-projects",
    "href": "01-getting-started.html#rstudio-projects",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "In RStudio, scripts allow us to build and save code that can be run repeatedly. We can organise these scripts into RStudio projects, which consolidate all files related to an analysis such as input data, R scripts, results, figures, and more. This organisation helps keep track of all data, input, and output, while enabling us to create standalone scripts for each part of our analysis. Additionally, it simplifies managing directories and filepaths.\nNavigate to File -&gt; New Project -&gt; New Directory, and create a folder with the name GeoSpatial-Workshop24. Click on Create Project. You should now see your main window switch to this new project and when you check your files window, you should see a new R Project called GeoSpatial-Workshop24.\n\n\n\n\n\n\nPlease ensure that folder names and file names do not contain spaces or special characters such as * . \" / \\ [ ] : ; | = , &lt; ? &gt; & $ # ! ' { } ( ). Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore _ or hyphen - if you like."
  },
  {
    "objectID": "01-getting-started.html#bathetha-isixhosa",
    "href": "01-getting-started.html#bathetha-isixhosa",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "With the basics covered, let us dive into loading a real dataset, performing data cleaning, and conducting some exploratory data analysis on the isiXhosa speaking population. We will work with a dataset that contains counts of the primary languages spoken in South African, sourced from the South African Census Community Profiles 2011 and made available through the DataFirst data service. You can download a copy of the file through the link below and save it in your project folder under data/attributes.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 Language Table\ndta\nDownload\n\n\n\nTo get started, let us create our first script. File -&gt; New File -&gt; R Script. Create a folder named scripts, and save your script as 01-language-analysis.r.\nWe will start by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(haven)\n\n\n\n\n\n\n\n\nIn RStudio, there are two primary ways to run a script: all at once or by executing individual lines or chunks of code. As a beginner, it is often beneficial to use the line-by-line approach, as it allows you to test your code interactively and catch errors early.\nTo run line-by-line:\n\nBy clicking: Highlight the line or chunk of code you want to run, then go to Code and select Run selected lines.\nBy key commands: Highlight the code, then press Ctl (or Cmd on Mac) + Return.\n\nTo run the whole script:\n\nBy clicking: In the scripting window, click Run in the top-right corner and choose Run All.\nBy key commands: Press Option + Ctrl (or Cmd) + R.\n\nIf a script gets stuck or you realise there is an error in your code, you may need to interrupt R. To do this, go to Session -&gt; Interrupt R. If the interruption doesn’t work, you might need to terminate and restart R.\n\n\n\n\n\nNext, we can load the sa-language.dta file into R. Using the haven library, R is capable to read a large range of different filetypes. This includes Stata dta files:\n\n\n\nR code\n\n# load data\natt &lt;- read_dta(\"data/attributes/sa-language.dta\")\n\n\n\n\n\n\n\n\nIf using a Windows machine, you may need to substitute your forward-slashes (/) with two backslashes (\\\\) whenever you are dealing with file paths.\n\n\n\nLet us have a look at the dataframe:\n\n\n\nR code\n\n# inspect columns, rows\nncol(att)\n\n\n[1] 28\n\nnrow(att)\n\n[1] 84908\n\n# inspect data\nhead(att)\n\n# A tibble: 6 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1600001 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n2  1600002 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n3  1600003 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n4  1600004 160010001 Vredenda…  160010 Vreden… WC011        160 Matzik… DC1     \n5  1600005 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n6  1600006 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n# inspect column names\nnames(att)\n\n [1] \"sal_code\" \"sp_code\"  \"sp_name\"  \"mp_code\"  \"mp_name\"  \"mn_mdb_c\"\n [7] \"mn_code\"  \"mn_name\"  \"dc_mdb_c\" \"dc_code\"  \"dc_name\"  \"pr_code\" \n[13] \"pr_name\"  \"lng_1\"    \"lng_2\"    \"lng_3\"    \"lng_4\"    \"lng_5\"   \n[19] \"lng_6\"    \"lng_7\"    \"lng_8\"    \"lng_9\"    \"lng_10\"   \"lng_11\"  \n[25] \"lng_12\"   \"lng_13\"   \"lng_14\"   \"lng_15\"  \n\n\nTo access specific columns or rows in a dataframe, we can use indexing. Indexing refers to the numbering assigned to each element within a data structure, allowing us to precisely select and manipulate data.\nTo access the first row of a dataframe, you would use dataframe[1, ], and to access the first column, you would use dataframe[, 1]. The comma separates the row and column indices, with the absence of a number indicating all rows or columns respectively.\n\n\n\n\n\n\nIn R, indexing begins at 1, meaning that the first element of any data structure is accessed with the index [1]. This is different from many other programming languages, such as Python or Java, where indexing typically starts at 0.\n\n\n\n\n\n\nR code\n\n# index columns\natt[, 1]\n\n\n# A tibble: 84,908 × 1\n   sal_code\n      &lt;dbl&gt;\n 1  1600001\n 2  1600002\n 3  1600003\n 4  1600004\n 5  1600005\n 6  1600006\n 7  1600007\n 8  1600008\n 9  1600009\n10  1600010\n# ℹ 84,898 more rows\n\n# index rows\natt[1, ]\n\n# A tibble: 1 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1600001 160002001 Matzikam…  160002 Matzik… WC011        160 Matzik… DC1     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n# index cell\natt[1, 1]\n\n# A tibble: 1 × 1\n  sal_code\n     &lt;dbl&gt;\n1  1600001\n\n\n\n\n\n\n\n\nAlternatively, you can access the data within individual columns by referring to their names using the $ operator. This allows you to easily extract and work with a specific column without needing to know its position in the dataframe. For example, if your dataframe is named dataframe and you want to access a column named age, you would use dataframe$age. This method is especially useful when your data has many columns or when the column positions may change, because it relies on the column names rather than their index numbers.\n\n\n\n\n\n\nNow that we have loaded and inspected our data, we can examine its distribution. We will focus on the column that contains the counts of individuals who speak isiXhosa as their primary language, aggregated by Small Area Layer — the most granular geographic level available in the 2011 South African Census of Population. This data is stored in the lng_4 column of the dataframe.\n\n\n\nR code\n\n# mean\nmean(att$lng_4)\n\n\n[1] 95.99431\n\n# median\nmedian(att$lng_4)\n\n[1] 9\n\n# range\nrange(att$lng_4)\n\n[1]    0 6465\n\n# summary\nsummary(att$lng_4)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    9.00   95.99   72.00 6465.00 \n\n\nWe can also call some functions to quickly draw a boxplot, histogram, or scatterplot:\n\n\n\nR code\n\n# boxplot\nboxplot(att$lng_4, horizontal = TRUE)\n\n\n\n\n\nFigure 2: Quick boxplot.\n\n\n\n\n\n\n\nR code\n\n# histogram\nhist(att$lng_4, breaks = 50, xlab = \"Number of Small Area Layers\", main = \"Number of isiXhosa speakers\")\n\n\n\n\n\nFigure 3: Quick histogram.\n\n\n\n\n\n\n\nR code\n\n# bivariate plot\nplot(att$lng_4, att$lng_2, xlab = \"isiXhosa\", ylab = \"English\")\n\n\n\n\n\nFigure 4: Quick bivariate plot.\n\n\n\n\n\n\n\nThe dataset is quite extensive, containing counts for all 84,908 Small Area Layers. To make our analysis more manageable, let’s focus on a subset of the data, specifically zooming in on Cape Town. We can achieve this by filtering the data using the dc_name column, which corresponds to the District Municipality Name.\n\n\n\nR code\n\n# prepare data: filter out Cape Town\natt &lt;- att |&gt;\n    filter(dc_name == \"City of Cape Town\")\n\n\n\n\n\n\n\n\nThe code above uses a pipe function: |&gt;. The pipe operator allows you to pass the output of one function directly into the next, streamlining your code. While it might seem a bit confusing at first, you will find that it makes your code faster to write and easier to read. More importantly, it reduces the need to create multiple intermediate variables to store outputs.\n\n\n\nIn this case, we have overwritten our initial object, though this is not strictly necessary. Our object now contains a subset of the full language dataset that we initially loaded.\n\n\n\nR code\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 28\n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1990001 199014025 Paarden …  199014 Milner… CPT          199 City o… CPT     \n2  1990002 199016089 Mimosa     199016 Bellvi… CPT          199 City o… CPT     \n3  1990003 199041008 Signal H…  199041 Cape T… CPT          199 City o… CPT     \n4  1990004 199017021 Durbanvi…  199017 Durban… CPT          199 City o… CPT     \n5  1990005 199053012 Pine Hav…  199053 Simon'… CPT          199 City o… CPT     \n6  1990006 199030007 Guguleth…  199030 Gugule… CPT          199 City o… CPT     \n# ℹ 19 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;\n\n\n\n\n\nThe next thing that we probably want to do is standardise our data. A common challenge with spatial data is that the spatial units aren not always the same size — some Small Area Layers may have larger populations, while others have fewer residents. This makes it difficult to compare absolute counts fairly. To address this, we can sum the counts across all relevant columns to determine the total number of speakers in each area. Then, we can calculate the proportion of isiXhosa speakers within each Small Area Layer:\n\n\n\nR code\n\n# prepare data: sum across\natt &lt;- att |&gt;\n    rowwise() |&gt;\n    mutate(sal_pop = sum(across(starts_with(\"lng\")), na.rm = TRUE))\n\n# prepare data: calculate percentages\natt &lt;- att |&gt;\n    mutate(sal_prop_xhosa = lng_4/sal_pop)\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 30\n# Rowwise: \n  sal_code   sp_code sp_name   mp_code mp_name mn_mdb_c mn_code mn_name dc_mdb_c\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1  1990001 199014025 Paarden …  199014 Milner… CPT          199 City o… CPT     \n2  1990002 199016089 Mimosa     199016 Bellvi… CPT          199 City o… CPT     \n3  1990003 199041008 Signal H…  199041 Cape T… CPT          199 City o… CPT     \n4  1990004 199017021 Durbanvi…  199017 Durban… CPT          199 City o… CPT     \n5  1990005 199053012 Pine Hav…  199053 Simon'… CPT          199 City o… CPT     \n6  1990006 199030007 Guguleth…  199030 Gugule… CPT          199 City o… CPT     \n# ℹ 21 more variables: dc_code &lt;dbl&gt;, dc_name &lt;chr&gt;, pr_code &lt;dbl&gt;,\n#   pr_name &lt;chr&gt;, lng_1 &lt;dbl&gt;, lng_2 &lt;dbl&gt;, lng_3 &lt;dbl&gt;, lng_4 &lt;dbl&gt;,\n#   lng_5 &lt;dbl&gt;, lng_6 &lt;dbl&gt;, lng_7 &lt;dbl&gt;, lng_8 &lt;dbl&gt;, lng_9 &lt;dbl&gt;,\n#   lng_10 &lt;dbl&gt;, lng_11 &lt;dbl&gt;, lng_12 &lt;dbl&gt;, lng_13 &lt;dbl&gt;, lng_14 &lt;dbl&gt;,\n#   lng_15 &lt;dbl&gt;, sal_pop &lt;dbl&gt;, sal_prop_xhosa &lt;dbl&gt;\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe now have the percentage of people who speak isiXhosa as their primary language for each Small Area Layer. To provide a more meaningful representation, however, we can aggregate the data by sub places. This involves grouping our data by sp_code, which contains unique sub place codes. We will then:\n\nSum the number of isiXhosa speakers within each sub place and store this in a new variable.\nSum the total number of people within each sub place and store this in another new variable.\nExtract the distinct values for each sub place.\n\nBy using the pipe function again, we can chain these steps together efficiently as follows:\n\n\n\nR code\n\n# prepare data: aggregate small areas to sub places\natt &lt;- att |&gt;\n    group_by(sp_code) |&gt;\n    mutate(sp_pop = sum(sal_pop)) |&gt;\n    mutate(sp_xhosa = sum(lng_4)) |&gt;\n    ungroup() |&gt;\n    distinct(sp_code, sp_pop, sp_xhosa)\n\n# inspect data\nhead(att)\n\n\n# A tibble: 6 × 3\n    sp_code sp_pop sp_xhosa\n      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 199014025     12        0\n2 199016089   1212        0\n3 199041008     12        0\n4 199017021   7893      105\n5 199053012      9        0\n6 199030007  55242    47385\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nWe can save this dataset so that we can easily load it the next time we want to work with this by writing it to a csv file.\n\n\n\nR code\n\n# write data\nwrite_csv(x = att, file = \"data/attributes/sa-language.csv\")"
  },
  {
    "objectID": "01-getting-started.html#internet-at-home",
    "href": "01-getting-started.html#internet-at-home",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "This concludes this session. Please try to complete the following tasks:\n\nCreate a histogram showing the distribution of the number of isiXhosa speakers in Cape Town, grouped by sub place.\nCreate a boxplot of the proportion of isiXhosa-speakers in Cape Town, grouped by sub place.\n\nNext, use the link below to download another dataset sourced from the South African Census Community Profiles 2011 that contains information on internet access. Save the file in your project folder under data/attributes.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 Internet Access Table\ndta\nDownload\n\n\n\nUse this dataset to:\n\nCreate a histogram of the distribution of the number of people with internet access at home in the City of Johannesburg, grouped by sub place.\nCreate a boxplot of the proportion of people with internet access at home in the City of Johannesburg, grouped by sub place."
  },
  {
    "objectID": "01-getting-started.html#solutions-optional",
    "href": "01-getting-started.html#solutions-optional",
    "title": "1 R for Data Analysis",
    "section": "",
    "text": "There are several ways to achieve the above, but if you would like to see an example solution, you can find it here: [Link]"
  },
  {
    "objectID": "02-mapping-data.html",
    "href": "02-mapping-data.html",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "Open a new script within your Geospatial-Workshop24 project and save this as 02-language-maps.r. We will start again by loading the libraries that we will need. You have been introduced to the tidyverse library last session, but now we are adding the sf library to read and load our spatial data as well as the tmap library to visualise our spatial data:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\nWe will continue working with the csv dataset that we prepared and saved in the previous session, so let us make sure it is loaded properly:\n\n\n\nR code\n\n# load data\natt &lt;- read_csv(\"data/attributes/sa-language.csv\")\n\n\nRows: 878 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): sp_code, sp_pop, sp_xhosa\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nNext, we need a corresponding spatial dataset that contains the Cape Town’s sub places and save it in your data/spatial folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nCape Town Sub Places\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nYou may have used spatial data before and noticed that we did not download a collection of files known as a shapefile but a GeoPackage instead. Whilst shapefiles are still being used, GeoPackage is a more modern and portable file format. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles where possible: [Link]\n\n\n\nLet us load the file and store it into an object called cpt. We can do this as follows:\n\n\n\nR code\n\n# load data\ncpt &lt;- st_read(\"data/spatial/subplace-cape-town-2013.gpkg\")\n\n\nReading layer `subplace-cape-town-2013' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/SA-TIED/data/spatial/subplace-cape-town-2013.gpkg' \n  using driver `GPKG'\nSimple feature collection with 921 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -64020.67 ymin: -3803551 xmax: 430.9835 ymax: -3705149\nProjected CRS: WGS_1984_Transverse_Mercator\n\n\nYou should also see the cpt variable appear in your environment window.\n\n\n\nAs this is the first time we have loaded spatial data into R, let’s go for a little exploration of how we can interact with our spatial dataframe. The first thing we want to do when we load spatial data is to quickly plot the data to check whether everything is in order. To do this, we can use the same function we used before: plot().\n\n\n\nR code\n\n# plot data\nplot(cpt, max.plot = 1)\n\n\n\n\n\nYou should see your cpt plot appear in your Plots window.\n\n\n\n\n\n\nThe plot() function should not to be used to make publishable maps but can be used as a quick way of inspecting your spatial data.\n\n\n\nJust as with a tabular dataframe, we can inspect the spatial data frame:\n\n\n\nR code\n\n# inspect columns\nncol(cpt)\n\n\n[1] 17\n\n# inspect rows\nnrow(cpt)\n\n[1] 921\n\n# inspect data\nhead(cpt)\n\nSimple feature collection with 6 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -28165.99 ymin: -3763492 xmax: -25397.68 ymax: -3761237\nProjected CRS: WGS_1984_Transverse_Mercator\n    sp_code      sp_name mp_code    mp_name mn_mdb_c mn_code           mn_name\n1 199035011   Greenfield  199035 Blue Downs      CPT     199 City of Cape Town\n2 199035012      Wesbank  199035 Blue Downs      CPT     199 City of Cape Town\n3 199035013    Kleinvlei  199035 Blue Downs      CPT     199 City of Cape Town\n4 199035014    Palm Park  199035 Blue Downs      CPT     199 City of Cape Town\n5 199035015 Park Village  199035 Blue Downs      CPT     199 City of Cape Town\n6 199035016    Hill View  199035 Blue Downs      CPT     199 City of Cape Town\n  dc_mdb_c dc_mn_c           dc_name pr_mdb_c pr_code      pr_name albers_are\n1      CPT     199 City of Cape Town       WC       1 Western Cape  0.3061520\n2      CPT     199 City of Cape Town       WC       1 Western Cape  1.0037679\n3      CPT     199 City of Cape Town       WC       1 Western Cape  0.7925992\n4      CPT     199 City of Cape Town       WC       1 Western Cape  0.6029142\n5      CPT     199 City of Cape Town       WC       1 Western Cape  0.4050055\n6      CPT     199 City of Cape Town       WC       1 Western Cape  0.1587553\n  shape_leng   shape_area                           geom\n1 0.02507468 2.986940e-05 MULTIPOLYGON (((-27674.79 -...\n2 0.05302850 9.794070e-05 MULTIPOLYGON (((-27165.44 -...\n3 0.04533365 7.733431e-05 MULTIPOLYGON (((-26251.72 -...\n4 0.03277084 5.883012e-05 MULTIPOLYGON (((-25852.91 -...\n5 0.02499933 3.951954e-05 MULTIPOLYGON (((-26300.9 -3...\n6 0.01980995 1.549123e-05 MULTIPOLYGON (((-26748.16 -...\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"geom\"      \n\n\nWe can also again establish the class of our data:\n\n\n\nR code\n\n# inspect\nclass(cpt)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe should see our data is an sf dataframe, which is what we want and we can move on.\n\n\n\nWe now have our language dataset (att) with the number of isiXhosa speakers in Cape Town, organised by sub place, as well as a spatial dataset containing the boundaries of these sub places (cpt). We can now join this table data to our spatial data using an Attribute Join.\n\n\n\n\n\n\nAn attribute join links two datasets based on a common attribute, enabling the ‘matching’ of rows between them.\n\n\n\n\n\nFigure 1: Attribute Joins.\n\n\n\n\nTo perform a successful join, each dataset must contain a unique identifying (UID) field. This could be a code, a name, or any other consistent identifier. It is crucial that the ID field is accurate across both datasets, with no typos or inconsistencies (e.g., “City of Cape Town” is not the same as “The City of Cape Town”). Whenever possible, it is preferable to use unique codes rather than names, as codes reduce the likelihood of errors and mismatches.\n\n\n\nBefore proceeding with the join, we need to verify that a matching UID exists in both datasets. Let’s look at the column names in our datasets again:\n\n\n\nR code\n\n# inspect column names\nnames(att)\n\n\n[1] \"sp_code\"  \"sp_pop\"   \"sp_xhosa\"\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"geom\"      \n\n\nThe sp_code columns looks promising as it features in both datasets. We can quickly sort both columns and have a peek at the data:\n\n\n\nR code\n\n# inspect att\nhead(sort(att$sp_code))\n\n\n[1] 199001001 199002001 199002002 199002003 199003001 199004001\n\n# inspect cpt\nhead(sort(cpt$sp_code))\n\n[1] \"199001001\" \"199002001\" \"199002002\" \"199002003\" \"199003001\" \"199004001\"\n\n\nThey seem to contain similar values, so that is promising. Let us try to join the attribute data onto the spatial data:\n\n\n\nR code\n\n# join attribute data onto spatial data\ncpt &lt;- cpt |&gt; \n  left_join(att, by = c(\"sp_code\" = \"sp_code\"))\n\n\nYou will notice that the join results in an error.\nWhere the sub place codes in the att object are stored as numbers, the sub place codes in the cpt object are stored as strings. We can fix this by casting the number to characters:\n\n\n\nR code\n\n# change data type\natt &lt;- att |&gt;\n    mutate(sp_code = as.character(sp_code))\n\n# inspect\ntypeof(att$sp_code)\n\n\n[1] \"character\"\n\n\nWe can now try to join the datasets together again:\n\n\n\nR code\n\n# join attribute data onto spatial data\ncpt &lt;- cpt |&gt; \n  left_join(att, by = c(\"sp_code\" = \"sp_code\"))\n\n\nWe can explore the joined data in usual fashion:\n\n\n\nR code\n\n# inspect columns\nncol(cpt)\n\n\n[1] 19\n\n# inspect rows\nnrow(cpt)\n\n[1] 921\n\n# inspect data\nhead(cpt)\n\nSimple feature collection with 6 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -28165.99 ymin: -3763492 xmax: -25397.68 ymax: -3761237\nProjected CRS: WGS_1984_Transverse_Mercator\n    sp_code      sp_name mp_code    mp_name mn_mdb_c mn_code           mn_name\n1 199035011   Greenfield  199035 Blue Downs      CPT     199 City of Cape Town\n2 199035012      Wesbank  199035 Blue Downs      CPT     199 City of Cape Town\n3 199035013    Kleinvlei  199035 Blue Downs      CPT     199 City of Cape Town\n4 199035014    Palm Park  199035 Blue Downs      CPT     199 City of Cape Town\n5 199035015 Park Village  199035 Blue Downs      CPT     199 City of Cape Town\n6 199035016    Hill View  199035 Blue Downs      CPT     199 City of Cape Town\n  dc_mdb_c dc_mn_c           dc_name pr_mdb_c pr_code      pr_name albers_are\n1      CPT     199 City of Cape Town       WC       1 Western Cape  0.3061520\n2      CPT     199 City of Cape Town       WC       1 Western Cape  1.0037679\n3      CPT     199 City of Cape Town       WC       1 Western Cape  0.7925992\n4      CPT     199 City of Cape Town       WC       1 Western Cape  0.6029142\n5      CPT     199 City of Cape Town       WC       1 Western Cape  0.4050055\n6      CPT     199 City of Cape Town       WC       1 Western Cape  0.1587553\n  shape_leng   shape_area sp_pop sp_xhosa                           geom\n1 0.02507468 2.986940e-05   2694       69 MULTIPOLYGON (((-27674.79 -...\n2 0.05302850 9.794070e-05   4164       45 MULTIPOLYGON (((-27165.44 -...\n3 0.04533365 7.733431e-05   6756       24 MULTIPOLYGON (((-26251.72 -...\n4 0.03277084 5.883012e-05   5415      297 MULTIPOLYGON (((-25852.91 -...\n5 0.02499933 3.951954e-05   2604       75 MULTIPOLYGON (((-26300.9 -3...\n6 0.01980995 1.549123e-05   1407       42 MULTIPOLYGON (((-26748.16 -...\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"sp_pop\"     \"sp_xhosa\"   \"geom\"      \n\n\nAlways inspect your join to ensure everything looks as expected. A good way to do this is by using the View() function to check for any unexpected missing values, which are marked as NA. We can further compare the total number of rows in the spatial dataset with the total number of non-NA values in the joined columns:\n\n\n\nR code\n\n# inspect\nnrow(cpt)\n\n\n[1] 921\n\n# inspect attribute data: sp_pop\nsum(!is.na(cpt$sp_pop))\n\n[1] 878\n\n# inspect attribute data: sp_xhosas\nsum(!is.na(cpt$sp_xhosa))\n\n[1] 878\n\n\nOut of the 921 sub places, only 878 have data on the number of isiXhosa speakers and the total population. However, this discrepancy may not be due to the join; it could simply be that the original dataset lacks data for some units. Let’s confirm this:\n\n\n\nR code\n\n# inspect\nnrow(att)\n\n\n[1] 878\n\n\nThis confirms that all our full attribute dataset has been linked to the spatial dataset.\n\n\n\n\n\n\nIt is important to confirm the source of any NA values that are introduced to the dataset to ensure these are genuine. In our case the South African Census Community Profiles 2011 dataset includes sub places with no population, which typically correspond to industrial areas.\n\n\n\nSince we know these NA values are not truly missing but represent structural zeroes, we can replace them with 0. However, this step is not strictly necessary.\n\n\n\nR code\n\n# replace NAs\ncpt &lt;- cpt |&gt;\n    mutate(sp_pop = if_else(is.na(sp_pop), 0, sp_pop), sp_xhosa = if_else(is.na(sp_xhosa),\n        0, sp_xhosa))\n\n\nWe are almost ready to map the data. We only need to add the proportion of isiXhosa speakers within each sub place to the data frame:\n\n\n\nR code\n\n# calculate percentages\ncpt &lt;- cpt |&gt;\n    mutate(sp_prop_xhosa = sp_xhosa/sp_pop)\n\n\nWe can save this dataset so that we can easily load it the next time we want to work with this by writing it to a GeoPackage:\n\n\n\nR code\n\n# write data\nst_write(obj = cpt, dsn = \"data/spatial/subplace-cape-town-2013-xhosa.gpkg\")\n\n\n\n\n\nFor our map-making, we will use one of the two primary visualisation libraries for spatial data: tmap. tmap offers a flexible, layer-based approach that makes it easy to create various types of thematic maps, such as choropleths and bubble maps. One of the standout features of tmap is its quick plotting function, qtm(), which allows you to generate basic maps with minimal effort.\n\n\n\nR code\n\n# quick thematic map\nqtm(cpt, fill = \"sp_prop_xhosa\")\n\n\n\n\n\nFigure 2: Quick thematic map.\n\n\n\n\nIn this case, the fill() argument in tmap is how we instruct the library to create a choropleth map based on the values in the specified column. If we set fill() to NULL, only the borders of our polygons will be drawn, without any colour fill. The qtm() function in tmap is versatile, allowing us to pass various parameters to customise the aesthetics of our map. By checking the function’s documentation, you can explore the full list of available parameters. For instance, to set the borders of our Cape Town polygons to white, we can use the borders parameter:\n\n\n\nR code\n\n# quick thematic map\nqtm(cpt, fill = \"sp_prop_xhosa\", borders = \"white\")\n\n\n\n\n\nFigure 3: Quick thematic map with white borders.\n\n\n\n\nThe map does not look quite right yet. While we can continue tweaking parameters in the qtm() function to improve it, qtm() is somewhat limited in its functionality and is primarily intended for quickly inspecting your data and creating basic maps. For more complex and refined map-making with the tmap library, it is better to use the main plotting method that starts with the tm_shape() function.\n\n\n\n\n\n\nThe primary approach to creating maps in tmap involves using a layered grammar of graphics to build up your map, starting with the tm_shape() function. This function, when provided with a spatial dataframe, captures the spatial information of your data, including its projection and geometry, and creates a spatial object. While you can override certain aspects of the spatial data (such as its projection) using the function’s parameters, the essential role of tm_shape() is to instruct R to “use this object as the basis for drawing the shapes.”\nTo actually render the shapes, you need to add a layer that specifies the type of shape you want R to draw from this spatial information—such as polygons for our data. This layer function tells R to “draw my spatial object as X,” where X represents the type of shape. Within this layer, you can also provide additional details to control how R draws your shapes. Further, you can add more layers to include other spatial objects and their corresponding shapes on your map. Finally, layout options can be specified through a layout layer, allowing you to customise the overall appearance and arrangement of your map.\n\n\n\nLet us build a map using tmap:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) + tm_polygons()\n\n\n\n\n\nFigure 4: Building up a map layer by layer.\n\n\n\n\nAs you can now see, we have mapped the spatial polygons of our cpt spatial dataframe. However, this is not quite the map we want; we need a choropleth map where the polygons are colored based on the proportion of isiXhosa speakers. To achieve this, we use the col parameter within the tm_polygons() function.\n\n\n\n\n\n\nThe col parameter within tm_polygons() allows you to fill polygons with colours based on:\n\nA single colour value (e.g. red).\nThe name of a data variable within the spatial data file. This variable can either contain specific colour values or numeric/categorical values that will be mapped to a colour palette.\n\n\n\n\nLet us go ahead and pass our sp_prop_xhosa variable within the col() parameter and see what we get:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n  # specify column\n  tm_polygons(\n    col = \"sp_prop_xhosa\"\n  )\n\n\n\n\n\nFigure 5: Building up a map layer by layer.\n\n\n\n\nWe are making progress, but there are two immediate issues with our map. First, the classification breaks do not adequately reflect the variation in our dataset. By default, tmap uses pretty breaks, which may not be the most effective for our data. An alternative, such as natural breaks (or jenks), might better reveal the data’s variation.\nTo customise the classification breaks, refer to the tm_polygons() documentation. The following parameters are relevant:\n\nn: Specifies the number of classification breaks.\nstyle: Defines the method for classification breaks, such as fixed, standard deviation, equal, or quantile.\nbreaks: Allows you to set specific numeric breaks when using the fixed style.\n\nFor example, if we want to adjust our choropleth map to use five classes determined by the natural breaks method, we need to add the n and style parameters to our tm_polygons() layer:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n  # specify column, classes\n  tm_polygons(\n    col = \"sp_prop_xhosa\",\n    n = 5,\n    style = \"jenks\"\n  )\n\n\n\n\n\nFigure 6: Building up a map layer by layer.\n\n\n\n\n\n\n\nStyling a map in tmap requires a deeper understanding and familiarity with the library, which is something you will develop best through hands-on practice. Here are the key functions to be aware of:\n\ntm_layout(): Customise titles, fonts, legends, and other layout elements.\ntm_compass(): Add and style a North arrow or compass.\ntm_scale_bar(): Add and style a scale bar.\n\nTo begin styling your map, explore each of these functions and their parameters. Through trial and error, you can tweak and refine the map until you achieve the desired look:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"sp_prop_xhosa\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Largest share\", \"2nd largest\", \"3rd largest\", \"4th largest\", \"Smallest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population speaking isiXhosa\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  )\n\n\n\n\n\nFigure 7: Building up a map layer by layer.\n\n\n\n\nWe can also have some map labels, if we want, by extracting centroids from selected polygons and adding these as seperate map layer:\n\n\n\nR code\n\n# map labels\ncbd &lt;- cpt |&gt;\n  filter(sp_code == \"199041011\") |&gt;\n  st_centroid()\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# map object\ncpt_xhosa &lt;-\n  # shape, polygons\n  tm_shape(cpt) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"sp_prop_xhosa\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Largest share\", \"2nd largest\", \"3rd largest\", \"4th largest\", \"Smallest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # cbd centroid\n  tm_shape(cbd) +\n\n  # add points\n  tm_dots(size = 0.4, col = \"#000000\") +\n\n  # add labels\n  tm_text(text = \"sp_name\", xmod = 0, ymod = -0.6, col = \"#000000\", size = 0.8) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population speaking isi-Xhosa\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  ) +\n\n  # add credits\n  tm_credits(\"Data source: Census 2011, StatsSA\",\n    fontface = \"italic\",\n    position = c(\"left\", \"bottom\"),\n    size = 0.4\n  )\n\n# plot\ncpt_xhosa\n\n\n\n\nFigure 8: Building up a map layer by layer.\n\n\n\n\nIn the code above, we stored the full map definition as an object. This makes it easy to export the map and save it as a .jpg, .png or .pdf file:\n\n\n\nR code\n\n# write map\ntmap_save(tm = xhosa_map, filename = \"cpt-xhosa.jpg\", width = 15, height = 15, units = c(\"cm\"))\n\n\n\n\n\nThis concludes this session. Please try to complete the following tasks:\n\nIf you have not already done so in the previous session, aggregate the internet access dataset by the internet at home variable for sub places in the City of Johannesburg.\nDownload the spatial boundaries for sub places in the City of Johannesburg using the link below, and save the file in your data/spatial folder.\nCreate a map that shows the proportion of individuals with internet access at home as part of the overall population.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nJohannesburg Sub Places\nGeoPackage\nDownload\n\n\n\n\n\n\nThere are several ways to achieve the above, but if you would like to see an example solution, you can find it here: [Link]"
  },
  {
    "objectID": "02-mapping-data.html#loading-spatial-data",
    "href": "02-mapping-data.html#loading-spatial-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "Open a new script within your Geospatial-Workshop24 project and save this as 02-language-maps.r. We will start again by loading the libraries that we will need. You have been introduced to the tidyverse library last session, but now we are adding the sf library to read and load our spatial data as well as the tmap library to visualise our spatial data:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\n\n\n\n\n\nWe will continue working with the csv dataset that we prepared and saved in the previous session, so let us make sure it is loaded properly:\n\n\n\nR code\n\n# load data\natt &lt;- read_csv(\"data/attributes/sa-language.csv\")\n\n\nRows: 878 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): sp_code, sp_pop, sp_xhosa\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYou can further inspect the results using the View() function.\n\n\n\nNext, we need a corresponding spatial dataset that contains the Cape Town’s sub places and save it in your data/spatial folder.\n\n\n\nFile\nType\nLink\n\n\n\n\nCape Town Sub Places\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nYou may have used spatial data before and noticed that we did not download a collection of files known as a shapefile but a GeoPackage instead. Whilst shapefiles are still being used, GeoPackage is a more modern and portable file format. Have a look at this article on towardsdatascience.com for an excellent explanation on why one should use GeoPackage files over shapefiles where possible: [Link]\n\n\n\nLet us load the file and store it into an object called cpt. We can do this as follows:\n\n\n\nR code\n\n# load data\ncpt &lt;- st_read(\"data/spatial/subplace-cape-town-2013.gpkg\")\n\n\nReading layer `subplace-cape-town-2013' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/SA-TIED/data/spatial/subplace-cape-town-2013.gpkg' \n  using driver `GPKG'\nSimple feature collection with 921 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -64020.67 ymin: -3803551 xmax: 430.9835 ymax: -3705149\nProjected CRS: WGS_1984_Transverse_Mercator\n\n\nYou should also see the cpt variable appear in your environment window."
  },
  {
    "objectID": "02-mapping-data.html#exploring-spatial-data",
    "href": "02-mapping-data.html#exploring-spatial-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "As this is the first time we have loaded spatial data into R, let’s go for a little exploration of how we can interact with our spatial dataframe. The first thing we want to do when we load spatial data is to quickly plot the data to check whether everything is in order. To do this, we can use the same function we used before: plot().\n\n\n\nR code\n\n# plot data\nplot(cpt, max.plot = 1)\n\n\n\n\n\nYou should see your cpt plot appear in your Plots window.\n\n\n\n\n\n\nThe plot() function should not to be used to make publishable maps but can be used as a quick way of inspecting your spatial data.\n\n\n\nJust as with a tabular dataframe, we can inspect the spatial data frame:\n\n\n\nR code\n\n# inspect columns\nncol(cpt)\n\n\n[1] 17\n\n# inspect rows\nnrow(cpt)\n\n[1] 921\n\n# inspect data\nhead(cpt)\n\nSimple feature collection with 6 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -28165.99 ymin: -3763492 xmax: -25397.68 ymax: -3761237\nProjected CRS: WGS_1984_Transverse_Mercator\n    sp_code      sp_name mp_code    mp_name mn_mdb_c mn_code           mn_name\n1 199035011   Greenfield  199035 Blue Downs      CPT     199 City of Cape Town\n2 199035012      Wesbank  199035 Blue Downs      CPT     199 City of Cape Town\n3 199035013    Kleinvlei  199035 Blue Downs      CPT     199 City of Cape Town\n4 199035014    Palm Park  199035 Blue Downs      CPT     199 City of Cape Town\n5 199035015 Park Village  199035 Blue Downs      CPT     199 City of Cape Town\n6 199035016    Hill View  199035 Blue Downs      CPT     199 City of Cape Town\n  dc_mdb_c dc_mn_c           dc_name pr_mdb_c pr_code      pr_name albers_are\n1      CPT     199 City of Cape Town       WC       1 Western Cape  0.3061520\n2      CPT     199 City of Cape Town       WC       1 Western Cape  1.0037679\n3      CPT     199 City of Cape Town       WC       1 Western Cape  0.7925992\n4      CPT     199 City of Cape Town       WC       1 Western Cape  0.6029142\n5      CPT     199 City of Cape Town       WC       1 Western Cape  0.4050055\n6      CPT     199 City of Cape Town       WC       1 Western Cape  0.1587553\n  shape_leng   shape_area                           geom\n1 0.02507468 2.986940e-05 MULTIPOLYGON (((-27674.79 -...\n2 0.05302850 9.794070e-05 MULTIPOLYGON (((-27165.44 -...\n3 0.04533365 7.733431e-05 MULTIPOLYGON (((-26251.72 -...\n4 0.03277084 5.883012e-05 MULTIPOLYGON (((-25852.91 -...\n5 0.02499933 3.951954e-05 MULTIPOLYGON (((-26300.9 -3...\n6 0.01980995 1.549123e-05 MULTIPOLYGON (((-26748.16 -...\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"geom\"      \n\n\nWe can also again establish the class of our data:\n\n\n\nR code\n\n# inspect\nclass(cpt)\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nWe should see our data is an sf dataframe, which is what we want and we can move on."
  },
  {
    "objectID": "02-mapping-data.html#joining-attribute-data",
    "href": "02-mapping-data.html#joining-attribute-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "We now have our language dataset (att) with the number of isiXhosa speakers in Cape Town, organised by sub place, as well as a spatial dataset containing the boundaries of these sub places (cpt). We can now join this table data to our spatial data using an Attribute Join.\n\n\n\n\n\n\nAn attribute join links two datasets based on a common attribute, enabling the ‘matching’ of rows between them.\n\n\n\n\n\nFigure 1: Attribute Joins.\n\n\n\n\nTo perform a successful join, each dataset must contain a unique identifying (UID) field. This could be a code, a name, or any other consistent identifier. It is crucial that the ID field is accurate across both datasets, with no typos or inconsistencies (e.g., “City of Cape Town” is not the same as “The City of Cape Town”). Whenever possible, it is preferable to use unique codes rather than names, as codes reduce the likelihood of errors and mismatches.\n\n\n\nBefore proceeding with the join, we need to verify that a matching UID exists in both datasets. Let’s look at the column names in our datasets again:\n\n\n\nR code\n\n# inspect column names\nnames(att)\n\n\n[1] \"sp_code\"  \"sp_pop\"   \"sp_xhosa\"\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"geom\"      \n\n\nThe sp_code columns looks promising as it features in both datasets. We can quickly sort both columns and have a peek at the data:\n\n\n\nR code\n\n# inspect att\nhead(sort(att$sp_code))\n\n\n[1] 199001001 199002001 199002002 199002003 199003001 199004001\n\n# inspect cpt\nhead(sort(cpt$sp_code))\n\n[1] \"199001001\" \"199002001\" \"199002002\" \"199002003\" \"199003001\" \"199004001\"\n\n\nThey seem to contain similar values, so that is promising. Let us try to join the attribute data onto the spatial data:\n\n\n\nR code\n\n# join attribute data onto spatial data\ncpt &lt;- cpt |&gt; \n  left_join(att, by = c(\"sp_code\" = \"sp_code\"))\n\n\nYou will notice that the join results in an error.\nWhere the sub place codes in the att object are stored as numbers, the sub place codes in the cpt object are stored as strings. We can fix this by casting the number to characters:\n\n\n\nR code\n\n# change data type\natt &lt;- att |&gt;\n    mutate(sp_code = as.character(sp_code))\n\n# inspect\ntypeof(att$sp_code)\n\n\n[1] \"character\"\n\n\nWe can now try to join the datasets together again:\n\n\n\nR code\n\n# join attribute data onto spatial data\ncpt &lt;- cpt |&gt; \n  left_join(att, by = c(\"sp_code\" = \"sp_code\"))\n\n\nWe can explore the joined data in usual fashion:\n\n\n\nR code\n\n# inspect columns\nncol(cpt)\n\n\n[1] 19\n\n# inspect rows\nnrow(cpt)\n\n[1] 921\n\n# inspect data\nhead(cpt)\n\nSimple feature collection with 6 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -28165.99 ymin: -3763492 xmax: -25397.68 ymax: -3761237\nProjected CRS: WGS_1984_Transverse_Mercator\n    sp_code      sp_name mp_code    mp_name mn_mdb_c mn_code           mn_name\n1 199035011   Greenfield  199035 Blue Downs      CPT     199 City of Cape Town\n2 199035012      Wesbank  199035 Blue Downs      CPT     199 City of Cape Town\n3 199035013    Kleinvlei  199035 Blue Downs      CPT     199 City of Cape Town\n4 199035014    Palm Park  199035 Blue Downs      CPT     199 City of Cape Town\n5 199035015 Park Village  199035 Blue Downs      CPT     199 City of Cape Town\n6 199035016    Hill View  199035 Blue Downs      CPT     199 City of Cape Town\n  dc_mdb_c dc_mn_c           dc_name pr_mdb_c pr_code      pr_name albers_are\n1      CPT     199 City of Cape Town       WC       1 Western Cape  0.3061520\n2      CPT     199 City of Cape Town       WC       1 Western Cape  1.0037679\n3      CPT     199 City of Cape Town       WC       1 Western Cape  0.7925992\n4      CPT     199 City of Cape Town       WC       1 Western Cape  0.6029142\n5      CPT     199 City of Cape Town       WC       1 Western Cape  0.4050055\n6      CPT     199 City of Cape Town       WC       1 Western Cape  0.1587553\n  shape_leng   shape_area sp_pop sp_xhosa                           geom\n1 0.02507468 2.986940e-05   2694       69 MULTIPOLYGON (((-27674.79 -...\n2 0.05302850 9.794070e-05   4164       45 MULTIPOLYGON (((-27165.44 -...\n3 0.04533365 7.733431e-05   6756       24 MULTIPOLYGON (((-26251.72 -...\n4 0.03277084 5.883012e-05   5415      297 MULTIPOLYGON (((-25852.91 -...\n5 0.02499933 3.951954e-05   2604       75 MULTIPOLYGON (((-26300.9 -3...\n6 0.01980995 1.549123e-05   1407       42 MULTIPOLYGON (((-26748.16 -...\n\n# inspect column names\nnames(cpt)\n\n [1] \"sp_code\"    \"sp_name\"    \"mp_code\"    \"mp_name\"    \"mn_mdb_c\"  \n [6] \"mn_code\"    \"mn_name\"    \"dc_mdb_c\"   \"dc_mn_c\"    \"dc_name\"   \n[11] \"pr_mdb_c\"   \"pr_code\"    \"pr_name\"    \"albers_are\" \"shape_leng\"\n[16] \"shape_area\" \"sp_pop\"     \"sp_xhosa\"   \"geom\"      \n\n\nAlways inspect your join to ensure everything looks as expected. A good way to do this is by using the View() function to check for any unexpected missing values, which are marked as NA. We can further compare the total number of rows in the spatial dataset with the total number of non-NA values in the joined columns:\n\n\n\nR code\n\n# inspect\nnrow(cpt)\n\n\n[1] 921\n\n# inspect attribute data: sp_pop\nsum(!is.na(cpt$sp_pop))\n\n[1] 878\n\n# inspect attribute data: sp_xhosas\nsum(!is.na(cpt$sp_xhosa))\n\n[1] 878\n\n\nOut of the 921 sub places, only 878 have data on the number of isiXhosa speakers and the total population. However, this discrepancy may not be due to the join; it could simply be that the original dataset lacks data for some units. Let’s confirm this:\n\n\n\nR code\n\n# inspect\nnrow(att)\n\n\n[1] 878\n\n\nThis confirms that all our full attribute dataset has been linked to the spatial dataset.\n\n\n\n\n\n\nIt is important to confirm the source of any NA values that are introduced to the dataset to ensure these are genuine. In our case the South African Census Community Profiles 2011 dataset includes sub places with no population, which typically correspond to industrial areas.\n\n\n\nSince we know these NA values are not truly missing but represent structural zeroes, we can replace them with 0. However, this step is not strictly necessary.\n\n\n\nR code\n\n# replace NAs\ncpt &lt;- cpt |&gt;\n    mutate(sp_pop = if_else(is.na(sp_pop), 0, sp_pop), sp_xhosa = if_else(is.na(sp_xhosa),\n        0, sp_xhosa))\n\n\nWe are almost ready to map the data. We only need to add the proportion of isiXhosa speakers within each sub place to the data frame:\n\n\n\nR code\n\n# calculate percentages\ncpt &lt;- cpt |&gt;\n    mutate(sp_prop_xhosa = sp_xhosa/sp_pop)\n\n\nWe can save this dataset so that we can easily load it the next time we want to work with this by writing it to a GeoPackage:\n\n\n\nR code\n\n# write data\nst_write(obj = cpt, dsn = \"data/spatial/subplace-cape-town-2013-xhosa.gpkg\")"
  },
  {
    "objectID": "02-mapping-data.html#mapping-spatial-data",
    "href": "02-mapping-data.html#mapping-spatial-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "For our map-making, we will use one of the two primary visualisation libraries for spatial data: tmap. tmap offers a flexible, layer-based approach that makes it easy to create various types of thematic maps, such as choropleths and bubble maps. One of the standout features of tmap is its quick plotting function, qtm(), which allows you to generate basic maps with minimal effort.\n\n\n\nR code\n\n# quick thematic map\nqtm(cpt, fill = \"sp_prop_xhosa\")\n\n\n\n\n\nFigure 2: Quick thematic map.\n\n\n\n\nIn this case, the fill() argument in tmap is how we instruct the library to create a choropleth map based on the values in the specified column. If we set fill() to NULL, only the borders of our polygons will be drawn, without any colour fill. The qtm() function in tmap is versatile, allowing us to pass various parameters to customise the aesthetics of our map. By checking the function’s documentation, you can explore the full list of available parameters. For instance, to set the borders of our Cape Town polygons to white, we can use the borders parameter:\n\n\n\nR code\n\n# quick thematic map\nqtm(cpt, fill = \"sp_prop_xhosa\", borders = \"white\")\n\n\n\n\n\nFigure 3: Quick thematic map with white borders.\n\n\n\n\nThe map does not look quite right yet. While we can continue tweaking parameters in the qtm() function to improve it, qtm() is somewhat limited in its functionality and is primarily intended for quickly inspecting your data and creating basic maps. For more complex and refined map-making with the tmap library, it is better to use the main plotting method that starts with the tm_shape() function.\n\n\n\n\n\n\nThe primary approach to creating maps in tmap involves using a layered grammar of graphics to build up your map, starting with the tm_shape() function. This function, when provided with a spatial dataframe, captures the spatial information of your data, including its projection and geometry, and creates a spatial object. While you can override certain aspects of the spatial data (such as its projection) using the function’s parameters, the essential role of tm_shape() is to instruct R to “use this object as the basis for drawing the shapes.”\nTo actually render the shapes, you need to add a layer that specifies the type of shape you want R to draw from this spatial information—such as polygons for our data. This layer function tells R to “draw my spatial object as X,” where X represents the type of shape. Within this layer, you can also provide additional details to control how R draws your shapes. Further, you can add more layers to include other spatial objects and their corresponding shapes on your map. Finally, layout options can be specified through a layout layer, allowing you to customise the overall appearance and arrangement of your map.\n\n\n\nLet us build a map using tmap:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) + tm_polygons()\n\n\n\n\n\nFigure 4: Building up a map layer by layer.\n\n\n\n\nAs you can now see, we have mapped the spatial polygons of our cpt spatial dataframe. However, this is not quite the map we want; we need a choropleth map where the polygons are colored based on the proportion of isiXhosa speakers. To achieve this, we use the col parameter within the tm_polygons() function.\n\n\n\n\n\n\nThe col parameter within tm_polygons() allows you to fill polygons with colours based on:\n\nA single colour value (e.g. red).\nThe name of a data variable within the spatial data file. This variable can either contain specific colour values or numeric/categorical values that will be mapped to a colour palette.\n\n\n\n\nLet us go ahead and pass our sp_prop_xhosa variable within the col() parameter and see what we get:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n  # specify column\n  tm_polygons(\n    col = \"sp_prop_xhosa\"\n  )\n\n\n\n\n\nFigure 5: Building up a map layer by layer.\n\n\n\n\nWe are making progress, but there are two immediate issues with our map. First, the classification breaks do not adequately reflect the variation in our dataset. By default, tmap uses pretty breaks, which may not be the most effective for our data. An alternative, such as natural breaks (or jenks), might better reveal the data’s variation.\nTo customise the classification breaks, refer to the tm_polygons() documentation. The following parameters are relevant:\n\nn: Specifies the number of classification breaks.\nstyle: Defines the method for classification breaks, such as fixed, standard deviation, equal, or quantile.\nbreaks: Allows you to set specific numeric breaks when using the fixed style.\n\nFor example, if we want to adjust our choropleth map to use five classes determined by the natural breaks method, we need to add the n and style parameters to our tm_polygons() layer:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n  # specify column, classes\n  tm_polygons(\n    col = \"sp_prop_xhosa\",\n    n = 5,\n    style = \"jenks\"\n  )\n\n\n\n\n\nFigure 6: Building up a map layer by layer."
  },
  {
    "objectID": "02-mapping-data.html#styling-spatial-data",
    "href": "02-mapping-data.html#styling-spatial-data",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "Styling a map in tmap requires a deeper understanding and familiarity with the library, which is something you will develop best through hands-on practice. Here are the key functions to be aware of:\n\ntm_layout(): Customise titles, fonts, legends, and other layout elements.\ntm_compass(): Add and style a North arrow or compass.\ntm_scale_bar(): Add and style a scale bar.\n\nTo begin styling your map, explore each of these functions and their parameters. Through trial and error, you can tweak and refine the map until you achieve the desired look:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(cpt) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"sp_prop_xhosa\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Largest share\", \"2nd largest\", \"3rd largest\", \"4th largest\", \"Smallest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population speaking isiXhosa\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  )\n\n\n\n\n\nFigure 7: Building up a map layer by layer.\n\n\n\n\nWe can also have some map labels, if we want, by extracting centroids from selected polygons and adding these as seperate map layer:\n\n\n\nR code\n\n# map labels\ncbd &lt;- cpt |&gt;\n  filter(sp_code == \"199041011\") |&gt;\n  st_centroid()\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# map object\ncpt_xhosa &lt;-\n  # shape, polygons\n  tm_shape(cpt) +\n\n  # specify column, classes, labels, title\n  tm_polygons(\n    col = \"sp_prop_xhosa\", n = 5, style = \"jenks\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\"#feebe2\", \"#fbb4b9\", \"#f768a1\", \"#c51b8a\", \"#7a0177\"),\n    labels = c(\"Largest share\", \"2nd largest\", \"3rd largest\", \"4th largest\", \"Smallest share\"),\n    title = \"Share of population\",\n    textNA = \"No population\"\n  ) +\n\n  # cbd centroid\n  tm_shape(cbd) +\n\n  # add points\n  tm_dots(size = 0.4, col = \"#000000\") +\n\n  # add labels\n  tm_text(text = \"sp_name\", xmod = 0, ymod = -0.6, col = \"#000000\", size = 0.8) +\n\n  # set layout\n  tm_layout(\n    main.title = \"Share of population speaking isi-Xhosa\",\n    main.title.size = 0.9,\n    main.title.position = c(\"right\", \"top\"),\n    legend.outside = FALSE,\n    legend.position = c(\"right\", \"top\"),\n    legend.title.size = 0.7,\n    legend.title.fontface = \"bold\",\n    legend.text.size = 0.5,\n    frame = FALSE,\n    inner.margins = c(0.05, 0.05, 0.05, 0.05),\n    fontfamily = \"Helvetica\"\n  ) +\n\n  # add North arrow\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"left\", \"top\"),\n    size = 1,\n    text.size = 0.7\n  ) +\n\n  # add scale bar\n  tm_scale_bar(\n    breaks = c(0, 5, 10, 15, 20),\n    position = c(\"right\", \"bottom\"),\n    text.size = 0.4\n  ) +\n\n  # add credits\n  tm_credits(\"Data source: Census 2011, StatsSA\",\n    fontface = \"italic\",\n    position = c(\"left\", \"bottom\"),\n    size = 0.4\n  )\n\n# plot\ncpt_xhosa\n\n\n\n\nFigure 8: Building up a map layer by layer.\n\n\n\n\nIn the code above, we stored the full map definition as an object. This makes it easy to export the map and save it as a .jpg, .png or .pdf file:\n\n\n\nR code\n\n# write map\ntmap_save(tm = xhosa_map, filename = \"cpt-xhosa.jpg\", width = 15, height = 15, units = c(\"cm\"))"
  },
  {
    "objectID": "02-mapping-data.html#assignment-optional",
    "href": "02-mapping-data.html#assignment-optional",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "This concludes this session. Please try to complete the following tasks:\n\nIf you have not already done so in the previous session, aggregate the internet access dataset by the internet at home variable for sub places in the City of Johannesburg.\nDownload the spatial boundaries for sub places in the City of Johannesburg using the link below, and save the file in your data/spatial folder.\nCreate a map that shows the proportion of individuals with internet access at home as part of the overall population.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nJohannesburg Sub Places\nGeoPackage\nDownload"
  },
  {
    "objectID": "02-mapping-data.html#solutions-optional",
    "href": "02-mapping-data.html#solutions-optional",
    "title": "1 R for Spatial Analysis",
    "section": "",
    "text": "There are several ways to achieve the above, but if you would like to see an example solution, you can find it here: [Link]"
  },
  {
    "objectID": "03-spatial-autocorrelation.html",
    "href": "03-spatial-autocorrelation.html",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "Start your Geospatial-Workshop24 project and open a new script. Save this as 03-autocorrelation.r. We will start again by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\n\n\n\n\n\nIn this session, we will be looking at education at the municipal level, focusing on the number of people with no schooling aggregated from the South African Census Community Profiles 2011. Along with this dataset, we also have access to a GeoPackage that contains the spatial boundaries of these municipalities. You can download both files below and save them in your project folder under data/attributes and data/spatial, respectively.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 No Schooling Variable\ncsv\nDownload\n\n\nSA Municipalities\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nTo download the csv file containing the mn_no_school variable that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\nOnce downloaded, we can load both files into memory:\n\n\n\nR code\n\n# load spatial data\nsa_municipality &lt;- st_read(\"data/spatial/municipality-south-africa-2013.gpkg\")\n\n\nReading layer `municipality-south-africa-2013' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/SA-TIED/data/spatial/municipality-south-africa-2013.gpkg' \n  using driver `GPKG'\nSimple feature collection with 234 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1831416 ymin: -4141363 xmax: 3667419 ymax: -2526543\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# load attribute data\nsa_no_schooling &lt;- read_csv(\"data/attributes/sa-no-schooling.csv\")\n\nRows: 234 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): mn_name\ndbl (3): mn_code, mn_pop, mn_no_school\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function.\n\n\n\n\n\n\nWith this dataset, we are interested in analysing the proportion of people without schooling across the country and visualising this information on a map. Let us start by preparing the data for mapping:\n\n\n\nR code\n\n# calculate proportions\nsa_no_schooling &lt;- sa_no_schooling |&gt;\n  mutate(mn_prop_no_schooling = mn_no_school / mn_pop)\n\n# join attribute data onto spatial data\nsa_municipality &lt;- sa_municipality |&gt; \n  left_join(sa_no_schooling, by = c(\"mn_code\" = \"mn_code\"))\n\n\nWe can now create a simple map:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_prop_no_schooling\",\n    n = 5,\n    style = \"jenks\"\n  ) +\n\n  # no legend\n  tm_layout(\n    legend.show = FALSE\n  )\n\n\n\n\n\nFigure 1: Proportions of people having no schooling by municipality.\n\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population that does not have any schooling appears to be neither random nor uniform, with a tendency for similar values to be found in closely located municipalities. Let us compare our map to a map with the same values which have been randomly permutated:\n\n\n\nR code\n\n# seed for reproducibility of random permutation\nset.seed(99)\n\n# random permutation\nsa_municipality &lt;- sa_municipality |&gt;\n  mutate(mn_prop_no_schooling_random = sample(sa_municipality$mn_prop_no_schooling, replace = FALSE))\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_prop_no_schooling_random\",\n    n = 5,\n    style = \"jenks\"\n  ) +\n\n  # no legend\n  tm_layout(\n    legend.show = FALSE\n  )\n\n\n\n\n\nFigure 2: Proportions of people having no schooling by municipality with randomly permutated values.\n\n\n\n\nLooking at Figure 2, even with the values being randomly permuted, certain patterns seem to emerge. This observation raises an important question: to what extent are the patterns that we see in the actual data actually present? A widely used method to quantify the similarity between neighbouring locations is by calculating Moran’s I statistic. This measure assesses spatial autocorrelation, indicating the degree to which values of a variable cluster spatially — either through similar (positive spatial autocorrelation) or contrasting values (negative spatial autocorrelation).\nUnderlying our Moran’s I test is the concept of a spatial lag. A spatial lag refers to a concept in spatial analysis where the value of a variable at a given location is influenced by the values of the same variable at neighboring locations. Essentially, it captures the idea that observations in close proximity are likely to be correlated, meaning that what happens in one area can ‘lag’ into or affect nearby areas. The Moran’s I statistic tries to capture the relationship between a value and its spatial lag. An Ordinary Least Squares (OLS) regression is applied, after both variables have been transformed to z-scores, to fit the data and produce a slope, which determines the Moran’s I statistic.\n\n\n\n\n\nFigure 3: Scatter plot of spatially lagged income (neighboring income) versus each areas income. Source: Manuel Gimond.\n\n\n\n\n\n\n\n\n\n\nMoran’s I values typically range from \\(-1\\) to \\(1\\):\n\n+1: Indicates perfect positive spatial autocorrelation. High values cluster near other high values, and low values near other low values.\n0: Suggests no spatial autocorrelation, meaning the spatial distribution of the variable is random.\n-1: Indicates perfect negative spatial autocorrelation. High values cluster near low values, and vice versa (a checkerboard pattern).\n\n\n\n\nThere are two approaches to estimating the significance of the Moran’s I statistic: an analytical method and a computational method. The analytical method relies on assumptions about the data, such as normality, which can sometimes limit its reliability. In contrast, the computational method, which is preferred here, does not make such assumptions and offers a more flexible and robust evaluation of significance.\nThe computational approach is based on a repeated random permutation of the observed values. The Moran’s I statistic is then calculated for each of these randomly reshuffled data sets, generating a reference distribution. By comparing the observed Moran’s I value to this reference distribution, we can assess whether our observed statistic is typical or an outlier and calculate a psuedo \\(p\\)-value (see Figure 4). If the observed Moran’s I value is an outlier, meaning it falls outside the range expected from random data distribution, it suggests a significant degree of clustering in the data.\n\n\n\n\n\nFigure 4: Determining significance using a Monte Carlo simulation. Source: Manuel Gimond.\n\n\n\n\nWe can derive a pseudo-\\(p\\) value from these simulation results as follows:\n\\[\n\\frac{N_{extreme} + 1}{N + 1}\n\\]\nwhere \\({N_{extreme}}\\) is the number of simulated Moran’s I values that were more extreme than our observed statistic and \\({N}\\) is the total number of simulations. In the example shown in Figure 4, only 1 out the 199 simulations was more extreme than the observed local Moran’s I statistic. Therefore \\({N_{extreme}}\\) = 1 , so \\(p\\) is equal to \\((1+1) / (199 + 1) = 0.01\\). This means that there is a one percent probability that we would be wrong in rejecting the null hypothesis of spatial randomness.\n\n\n\nIf the purpose of a Moran’s I test is to quantify how similar places are to their neighbours, the first step is to define what constitutes a neighbour. This definition is not necessarily straightforward, because ‘neighbouring’ observations can be determined in various ways, based on either geometry or proximity. The most common methods include:\n\nContiguity: Spatial units are considered neighbours if their polygon boundaries touch.\nFixed Distance: Spatial units are considered neighbours if they fall within a specified distance.\n\\(k\\) Nearest Neighbours: Spatial units are considered neighbours if they are among the closest neighbours.\n\nTo capture this information, we need to formalise the spatial relationships within our data by constructing a spatial weights matrix (\\(W_{ij}\\)). This matrix defines which units are neighbours based on our chosen criteria.\n\n\n\n\n\n\nIn the following example, neighbours are defined as places that share a border (i.e., they are contiguous). Currently, it is sufficient for them to meet at a single point — so if two places are triangular, touching corners would count them as neighbours. If, however, you require them to share an edge, rather than just a corner, you can modify the default argument by setting queen = FALSE.\n\n\n\n\n\n\nR code\n\n# create neighbour list\nsa_mn_nb &lt;- poly2nb(sa_municipality, queen = TRUE)\n\n# inspect\nsummary(sa_mn_nb)\n\n\nNeighbour list object:\nNumber of regions: 234 \nNumber of nonzero links: 1244 \nPercentage nonzero weights: 2.271897 \nAverage number of links: 5.316239 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 1  7 22 35 56 64 40  3  5  1 \n1 least connected region:\n102 with 1 link\n1 most connected region:\n193 with 10 links\n\n\nThe neighbour list object is a sparse matrix that lists the neighboring polygons for each municipality. This matrix represents the spatial relationships between municipalities, where each entry indicates which polygons share boundaries. These neighborhood relationships can be visualised as a graph by extracting the coordinate points of the centroids of the polygons representing each municipality:\n\n\n\n\n\n\nRegardless of the neighborhood definition you choose, it is important to verify the results, particularly when using contiguity-based approaches. If your spatial file has issues such as polygons that appear adjacent but do not actually share a border, your results may be inaccurate. You could increase the default value of the snap distance parameter in the poly2nb() function to include these polygons only separated by small gaps.\n\n\n\n\n\n\nR code\n\n# extract centroids from polygons\nsa_mn_cent &lt;- st_centroid(sa_municipality, of_largest_polygon = TRUE)\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# plot graph\npar(mai = c(0, 0, 0, 0))\nplot(st_geometry(sa_municipality), border = \"#cccccc\")\nplot(sa_mn_nb, st_geometry(sa_mn_cent), add = TRUE)\n\n\n\n\nFigure 5: Neighbourhood graph using queen contiguity.\n\n\n\n\n\n\n\nThe neighbourhood list simply identifies which areas (polygons) are neighbours, but spatial weights take this a step further by assigning a weight to each neighbourhood connection. This is important because not all polygons have the same number of neighbours. To ensure that our spatially lagged values are comparable across neighbourhoods of different sizes, standardisation is required. The code below uses style = 'W' to row-standardise the values: if a municipality has five neighbours, the value of the spatially lagged variable will be the average of that variable across those five neighbours, with each neighbour receiving equal weight.\n\n\n\nR code\n\n# create spatial weights matrix\nsa_mn_nb_weights &lt;- sa_mn_nb |&gt;\n    nb2listw(style = \"W\")\n\n# inspect - neigbhours of polygon '10'\nsa_mn_nb_weights$neighbours[[10]]\n\n\n[1]   3   9  11  44  45 124 165\n\n# inspect - weights of neighbours of polygon '10'\nsa_mn_nb_weights$weights[[10]]\n\n[1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571\n\n\n\n\n\n\n\n\nNot all places have neighbours. Islands, by definition, will not be considered as neighbours using a contiguity approach. If you attempt to create spatial weights using the nb2listw() function with a neighbourhood list that includes places without neighbours, you will encounter an error message. Potential solutions include using a different neighbourhood definition (e.g. \\(k\\)-nearest neighbours) or manually editing the neighbourhood file if you wish to include these polygons. Alternatively, you can leave it as is but then you must specify the argument zero.policy = TRUE in nb2listw() to allow for empty sets.\n\n\n\n\n\n\nNow that everything is in place, we can begin by plotting the proportion of people without schooling against the spatially lagged values:\n\n\n\nR code\n\n# moran's plot\nmoran.plot(sa_municipality$mn_prop_no_schooling, listw = sa_mn_nb_weights)\n\n\n\n\n\nFigure 6: Plot of lagged values versus polygon values.\n\n\n\n\nWe observe a positive relationship between our mn_prop_no_schooling variable and the spatially lagged values, suggesting that our global Moran’s I test will likely yield a statistic reflective of the slope visible in the scatter plot.\n\n\n\nR code\n\n# moran's test\nmoran &lt;- moran.mc(sa_municipality$mn_prop_no_schooling, listw = sa_mn_nb_weights,\n    nsim = 999)\n\n# results\nmoran\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  sa_municipality$mn_prop_no_schooling \nweights: sa_mn_nb_weights  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.52666, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe results of the Monte Carlo simulation, visualised in Figure 7, suggest that there is statistically significant positive autocorrelation in our variable. This indicates that municipalities with higher percentages of people without schooling tend to be surrounded by other municipalities with similarly high percentages. Likewise, municipalities with lower percentages of people without schooling are generally surrounded by municipalities with similarly low values.\n\n\n\nR code\n\n# permutation distribution\nplot(moran, main = \"\")\n\n\n\n\n\nFigure 7: Density plot of permutation outcomes.\n\n\n\n\n\n\n\nAlthough we have established that there is positive spatial autocorrelation in our data, we still need to identify the specific spatial patterns. Looking back at Figure 3, you will notice that the plot is divided into four quadrants.\n\nTop-right quadrant: This area represents municipalities that have a higher-than-average share of the population without schooling and are surrounded by other municipalities with similarly high shares of the population without schooling. These are known as high-high clusters.\nBottom-left quadrant: This area represents municipalities with a lower-than-average share of the population without schooling, surrounded by other municipalities with similarly low shares. These are low-low clusters.\nTop-left quadrant: Municipalities with a higher-than-average share of the population without schooling surrounded by municipalities with a lower-than-average share. These are high-low clusters.\nBottom-right quadrant: Municipalities with a lower-than-average share of the population without schooling surrounded by municipalities with a higher-than-average share. These are low-high clusters.\n\nWe can show these area on a map by deconstructing the Moran’s I into a series of local Moran values, each measuring how similar each place is (individually) to its neighbours.\n\n\n\nR code\n\n# local moran's test\nlmoran &lt;- localmoran_perm(sa_municipality$mn_prop_no_schooling, listw = sa_mn_nb_weights,\n    nsim = 999)\n\n# results\nhead(lmoran)\n\n\n            Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1 -0.050379105 -5.007116e-04 0.0055169108 -0.6715285      0.5018839\n2  0.076088895  8.806626e-03 0.0358989035  0.3551077      0.7225089\n3  0.294045408 -6.371539e-04 0.1058262803  0.9058529      0.3650137\n4  0.452492637  4.444755e-03 0.4386494317  0.6764966      0.4987254\n5  0.111902772 -1.280624e-03 0.0195226887  0.8100520      0.4179103\n6  0.002486432  9.338536e-05 0.0005451827  0.1024897      0.9183680\n  Pr(z != E(Ii)) Sim Pr(folded) Sim   Skewness    Kurtosis\n1              0.506          0.253 -0.1147855 -0.23093560\n2              0.714          0.357  0.1969864  0.03091263\n3              0.356          0.178  0.2355106  0.06633588\n4              0.502          0.251  0.1855721 -0.35342466\n5              0.438          0.219  0.1767742 -0.31332065\n6              0.948          0.474 -0.1729386 -0.13325485\n\n\nWe are not given a single statistic as we did with our global Moran’s I, but rather we get a table of different statistics that are all related back to each of the municipalities in our dataset. If we refer to the help page for the localmoran() function, we can find detailed explanations of these statistics. The most relevant ones include:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nIi\nLocal Moran’s I statistic.\n\n\nE.Ii\nExpectation (mean) of the local Moran’s I statistic.\n\n\nVar.Ii\nVariance of local Moran’s I statistic\n\n\nZ.Ii\nStandard deviation (z-score) of the local Moran’s I statistic.\n\n\nPr()\nPseudo \\(p\\)-value of local Moran’s I statistic based on standard deviations and means from the permutation sample.\n\n\nPr() Sim\nPseudo \\(p\\)-value of local Moran’s I statistic based on the rank within the permutation sample, assuming a uniform distribution.\n\n\nPr(Folded) Sim\nPseudo \\(p\\)-value of local Moran’s I statistic based on the rank within the permutation sample using a one-sided test, assuming a uniform distribution.\n\n\n\nWe can further extract the quadrants to which of all these polygons have been assigned:\n\n\n\nR code\n\n# extract quadrants\nlmoran_quadrants &lt;- attr(lmoran, \"quadr\")\n\n# inspect\nhead(lmoran_quadrants)\n\n\n       mean    median     pysal\n1  Low-High  Low-High  Low-High\n2 High-High High-High High-High\n3 High-High High-High High-High\n4 High-High High-High High-High\n5 High-High High-High High-High\n6   Low-Low   Low-Low   Low-Low\n\n\nWe can now link these values back to our spatial dataframe and make a map using the tmap library:\n\n\n\nR code\n\n# replace names\nnames(lmoran_quadrants) &lt;- c(\"lmoran_mean\", \"lmoran_median\", \"lmoran_pysal\")\n\n# bind results\nsa_municipality &lt;- sa_municipality |&gt;\n  cbind(lmoran_quadrants)\n\n# shape, polygons\ntm_shape(sa_municipality) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"lmoran_mean\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\n      \"Low-Low\" = \"#0571b0\",\n      \"Low-High\" = \"#92c5de\",\n      \"High-Low\" = \"#f4a582\",\n      \"High-High\" = \"#ca0020\"\n    ),\n    title = \"Cluster type\",\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n  )\n\n\n\n\n\nFigure 8: Mapping the Local Moran’s I clusters.\n\n\n\n\nThis type of map is called a LISA map and is a great way of showing how a variable is actually clustering over space. However, we can improve on this further by only mapping the statistically significant clusters:\n\n\n\nR code\n\n# replace values if not significant\nlmoran_quadrants[lmoran[, 6] &gt; 0.05, ] &lt;- NA\n\n# replace names\nnames(lmoran_quadrants) &lt;- c(\"lmoran_mean_sig\", \"lmoran_median_sig\", \"lmoran_pysal_sig\")\n\n# bind results\nsa_municipality &lt;- sa_municipality |&gt;\n  cbind(lmoran_quadrants)\n\n# shape, polygons\ntm_shape(sa_municipality) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"lmoran_mean_sig\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\n      \"Low-Low\" = \"#0571b0\",\n      \"Low-High\" = \"#92c5de\",\n      \"High-Low\" = \"#f4a582\",\n      \"High-High\" = \"#ca0020\"\n    ),\n    title = \"Cluster type\",\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n  )\n\n\n\n\n\nFigure 9: Mapping the significant Local Moran’s I clusters.\n\n\n\n\n\n\n\n\n\n\nThis new map may still not fully address the issue of statistical significance due to repeated testing, and some values may appear significant purely by chance. To correct for this, you can adjust the \\(p\\)-values using R’s p.adjust() function. For further details, refer to Manual Gimond’s explanation of the multiple comparison problem in the context of the pseudo-\\(p\\) values.\n\n\n\n\n\n\nThis concludes this session. Any statistic that includes spatial weights is dependent upon how those weights are defined. We have so far used first order contiguity, i.e. polygons that share a boundary, but there is no particular reason why we should not include second order contiguity polygons (i.e. neighbours of neighbours), use a fixed distance neighbours definitions, or adopt a \\(k\\) nearest neighbours definition.\nPlease try to complete the following tasks:\n\nExtract the centroids from the sa_municipality file.\nIdentify the 5 nearest neighbours for each municipalities, using the knearneigh() function.\nCreate a neigbhour list of these nearest neighbours, using the knn2nb() function.\nCompute the Global Moran’s I of the mn_prop_no_schooling variable using this new neighbourhood definition.\nMap the statistically significant clusters of Local Moran’s I based on this new neighbourhood definition.\n\n\n\n\nThere are several ways to achieve the above, but if you would like to see an example solution, you can find it here: [Link]"
  },
  {
    "objectID": "03-spatial-autocorrelation.html#loading-spatial-data",
    "href": "03-spatial-autocorrelation.html#loading-spatial-data",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "Start your Geospatial-Workshop24 project and open a new script. Save this as 03-autocorrelation.r. We will start again by loading the libraries that we will need:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\n\n\n\n\n\nIn this session, we will be looking at education at the municipal level, focusing on the number of people with no schooling aggregated from the South African Census Community Profiles 2011. Along with this dataset, we also have access to a GeoPackage that contains the spatial boundaries of these municipalities. You can download both files below and save them in your project folder under data/attributes and data/spatial, respectively.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 No Schooling Variable\ncsv\nDownload\n\n\nSA Municipalities\nGeoPackage\nDownload\n\n\n\n\n\n\n\n\n\nTo download the csv file containing the mn_no_school variable that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\nOnce downloaded, we can load both files into memory:\n\n\n\nR code\n\n# load spatial data\nsa_municipality &lt;- st_read(\"data/spatial/municipality-south-africa-2013.gpkg\")\n\n\nReading layer `municipality-south-africa-2013' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/SA-TIED/data/spatial/municipality-south-africa-2013.gpkg' \n  using driver `GPKG'\nSimple feature collection with 234 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1831416 ymin: -4141363 xmax: 3667419 ymax: -2526543\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# load attribute data\nsa_no_schooling &lt;- read_csv(\"data/attributes/sa-no-schooling.csv\")\n\nRows: 234 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): mn_name\ndbl (3): mn_code, mn_pop, mn_no_school\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYou can inspect both objects using the View() function."
  },
  {
    "objectID": "03-spatial-autocorrelation.html#spatial-dependency",
    "href": "03-spatial-autocorrelation.html#spatial-dependency",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "With this dataset, we are interested in analysing the proportion of people without schooling across the country and visualising this information on a map. Let us start by preparing the data for mapping:\n\n\n\nR code\n\n# calculate proportions\nsa_no_schooling &lt;- sa_no_schooling |&gt;\n  mutate(mn_prop_no_schooling = mn_no_school / mn_pop)\n\n# join attribute data onto spatial data\nsa_municipality &lt;- sa_municipality |&gt; \n  left_join(sa_no_schooling, by = c(\"mn_code\" = \"mn_code\"))\n\n\nWe can now create a simple map:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_prop_no_schooling\",\n    n = 5,\n    style = \"jenks\"\n  ) +\n\n  # no legend\n  tm_layout(\n    legend.show = FALSE\n  )\n\n\n\n\n\nFigure 1: Proportions of people having no schooling by municipality.\n\n\n\n\nLooking at the map, the geographical patterning of the percentage of the population that does not have any schooling appears to be neither random nor uniform, with a tendency for similar values to be found in closely located municipalities. Let us compare our map to a map with the same values which have been randomly permutated:\n\n\n\nR code\n\n# seed for reproducibility of random permutation\nset.seed(99)\n\n# random permutation\nsa_municipality &lt;- sa_municipality |&gt;\n  mutate(mn_prop_no_schooling_random = sample(sa_municipality$mn_prop_no_schooling, replace = FALSE))\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_prop_no_schooling_random\",\n    n = 5,\n    style = \"jenks\"\n  ) +\n\n  # no legend\n  tm_layout(\n    legend.show = FALSE\n  )\n\n\n\n\n\nFigure 2: Proportions of people having no schooling by municipality with randomly permutated values.\n\n\n\n\nLooking at Figure 2, even with the values being randomly permuted, certain patterns seem to emerge. This observation raises an important question: to what extent are the patterns that we see in the actual data actually present? A widely used method to quantify the similarity between neighbouring locations is by calculating Moran’s I statistic. This measure assesses spatial autocorrelation, indicating the degree to which values of a variable cluster spatially — either through similar (positive spatial autocorrelation) or contrasting values (negative spatial autocorrelation).\nUnderlying our Moran’s I test is the concept of a spatial lag. A spatial lag refers to a concept in spatial analysis where the value of a variable at a given location is influenced by the values of the same variable at neighboring locations. Essentially, it captures the idea that observations in close proximity are likely to be correlated, meaning that what happens in one area can ‘lag’ into or affect nearby areas. The Moran’s I statistic tries to capture the relationship between a value and its spatial lag. An Ordinary Least Squares (OLS) regression is applied, after both variables have been transformed to z-scores, to fit the data and produce a slope, which determines the Moran’s I statistic.\n\n\n\n\n\nFigure 3: Scatter plot of spatially lagged income (neighboring income) versus each areas income. Source: Manuel Gimond.\n\n\n\n\n\n\n\n\n\n\nMoran’s I values typically range from \\(-1\\) to \\(1\\):\n\n+1: Indicates perfect positive spatial autocorrelation. High values cluster near other high values, and low values near other low values.\n0: Suggests no spatial autocorrelation, meaning the spatial distribution of the variable is random.\n-1: Indicates perfect negative spatial autocorrelation. High values cluster near low values, and vice versa (a checkerboard pattern).\n\n\n\n\nThere are two approaches to estimating the significance of the Moran’s I statistic: an analytical method and a computational method. The analytical method relies on assumptions about the data, such as normality, which can sometimes limit its reliability. In contrast, the computational method, which is preferred here, does not make such assumptions and offers a more flexible and robust evaluation of significance.\nThe computational approach is based on a repeated random permutation of the observed values. The Moran’s I statistic is then calculated for each of these randomly reshuffled data sets, generating a reference distribution. By comparing the observed Moran’s I value to this reference distribution, we can assess whether our observed statistic is typical or an outlier and calculate a psuedo \\(p\\)-value (see Figure 4). If the observed Moran’s I value is an outlier, meaning it falls outside the range expected from random data distribution, it suggests a significant degree of clustering in the data.\n\n\n\n\n\nFigure 4: Determining significance using a Monte Carlo simulation. Source: Manuel Gimond.\n\n\n\n\nWe can derive a pseudo-\\(p\\) value from these simulation results as follows:\n\\[\n\\frac{N_{extreme} + 1}{N + 1}\n\\]\nwhere \\({N_{extreme}}\\) is the number of simulated Moran’s I values that were more extreme than our observed statistic and \\({N}\\) is the total number of simulations. In the example shown in Figure 4, only 1 out the 199 simulations was more extreme than the observed local Moran’s I statistic. Therefore \\({N_{extreme}}\\) = 1 , so \\(p\\) is equal to \\((1+1) / (199 + 1) = 0.01\\). This means that there is a one percent probability that we would be wrong in rejecting the null hypothesis of spatial randomness."
  },
  {
    "objectID": "03-spatial-autocorrelation.html#defining-neighbours",
    "href": "03-spatial-autocorrelation.html#defining-neighbours",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "If the purpose of a Moran’s I test is to quantify how similar places are to their neighbours, the first step is to define what constitutes a neighbour. This definition is not necessarily straightforward, because ‘neighbouring’ observations can be determined in various ways, based on either geometry or proximity. The most common methods include:\n\nContiguity: Spatial units are considered neighbours if their polygon boundaries touch.\nFixed Distance: Spatial units are considered neighbours if they fall within a specified distance.\n\\(k\\) Nearest Neighbours: Spatial units are considered neighbours if they are among the closest neighbours.\n\nTo capture this information, we need to formalise the spatial relationships within our data by constructing a spatial weights matrix (\\(W_{ij}\\)). This matrix defines which units are neighbours based on our chosen criteria.\n\n\n\n\n\n\nIn the following example, neighbours are defined as places that share a border (i.e., they are contiguous). Currently, it is sufficient for them to meet at a single point — so if two places are triangular, touching corners would count them as neighbours. If, however, you require them to share an edge, rather than just a corner, you can modify the default argument by setting queen = FALSE.\n\n\n\n\n\n\nR code\n\n# create neighbour list\nsa_mn_nb &lt;- poly2nb(sa_municipality, queen = TRUE)\n\n# inspect\nsummary(sa_mn_nb)\n\n\nNeighbour list object:\nNumber of regions: 234 \nNumber of nonzero links: 1244 \nPercentage nonzero weights: 2.271897 \nAverage number of links: 5.316239 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 1  7 22 35 56 64 40  3  5  1 \n1 least connected region:\n102 with 1 link\n1 most connected region:\n193 with 10 links\n\n\nThe neighbour list object is a sparse matrix that lists the neighboring polygons for each municipality. This matrix represents the spatial relationships between municipalities, where each entry indicates which polygons share boundaries. These neighborhood relationships can be visualised as a graph by extracting the coordinate points of the centroids of the polygons representing each municipality:\n\n\n\n\n\n\nRegardless of the neighborhood definition you choose, it is important to verify the results, particularly when using contiguity-based approaches. If your spatial file has issues such as polygons that appear adjacent but do not actually share a border, your results may be inaccurate. You could increase the default value of the snap distance parameter in the poly2nb() function to include these polygons only separated by small gaps.\n\n\n\n\n\n\nR code\n\n# extract centroids from polygons\nsa_mn_cent &lt;- st_centroid(sa_municipality, of_largest_polygon = TRUE)\n\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# plot graph\npar(mai = c(0, 0, 0, 0))\nplot(st_geometry(sa_municipality), border = \"#cccccc\")\nplot(sa_mn_nb, st_geometry(sa_mn_cent), add = TRUE)\n\n\n\n\nFigure 5: Neighbourhood graph using queen contiguity."
  },
  {
    "objectID": "03-spatial-autocorrelation.html#defining-weights",
    "href": "03-spatial-autocorrelation.html#defining-weights",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "The neighbourhood list simply identifies which areas (polygons) are neighbours, but spatial weights take this a step further by assigning a weight to each neighbourhood connection. This is important because not all polygons have the same number of neighbours. To ensure that our spatially lagged values are comparable across neighbourhoods of different sizes, standardisation is required. The code below uses style = 'W' to row-standardise the values: if a municipality has five neighbours, the value of the spatially lagged variable will be the average of that variable across those five neighbours, with each neighbour receiving equal weight.\n\n\n\nR code\n\n# create spatial weights matrix\nsa_mn_nb_weights &lt;- sa_mn_nb |&gt;\n    nb2listw(style = \"W\")\n\n# inspect - neigbhours of polygon '10'\nsa_mn_nb_weights$neighbours[[10]]\n\n\n[1]   3   9  11  44  45 124 165\n\n# inspect - weights of neighbours of polygon '10'\nsa_mn_nb_weights$weights[[10]]\n\n[1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571\n\n\n\n\n\n\n\n\nNot all places have neighbours. Islands, by definition, will not be considered as neighbours using a contiguity approach. If you attempt to create spatial weights using the nb2listw() function with a neighbourhood list that includes places without neighbours, you will encounter an error message. Potential solutions include using a different neighbourhood definition (e.g. \\(k\\)-nearest neighbours) or manually editing the neighbourhood file if you wish to include these polygons. Alternatively, you can leave it as is but then you must specify the argument zero.policy = TRUE in nb2listw() to allow for empty sets."
  },
  {
    "objectID": "03-spatial-autocorrelation.html#global-morans-i",
    "href": "03-spatial-autocorrelation.html#global-morans-i",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "Now that everything is in place, we can begin by plotting the proportion of people without schooling against the spatially lagged values:\n\n\n\nR code\n\n# moran's plot\nmoran.plot(sa_municipality$mn_prop_no_schooling, listw = sa_mn_nb_weights)\n\n\n\n\n\nFigure 6: Plot of lagged values versus polygon values.\n\n\n\n\nWe observe a positive relationship between our mn_prop_no_schooling variable and the spatially lagged values, suggesting that our global Moran’s I test will likely yield a statistic reflective of the slope visible in the scatter plot.\n\n\n\nR code\n\n# moran's test\nmoran &lt;- moran.mc(sa_municipality$mn_prop_no_schooling, listw = sa_mn_nb_weights,\n    nsim = 999)\n\n# results\nmoran\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  sa_municipality$mn_prop_no_schooling \nweights: sa_mn_nb_weights  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.52666, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe results of the Monte Carlo simulation, visualised in Figure 7, suggest that there is statistically significant positive autocorrelation in our variable. This indicates that municipalities with higher percentages of people without schooling tend to be surrounded by other municipalities with similarly high percentages. Likewise, municipalities with lower percentages of people without schooling are generally surrounded by municipalities with similarly low values.\n\n\n\nR code\n\n# permutation distribution\nplot(moran, main = \"\")\n\n\n\n\n\nFigure 7: Density plot of permutation outcomes."
  },
  {
    "objectID": "03-spatial-autocorrelation.html#local-morans-i",
    "href": "03-spatial-autocorrelation.html#local-morans-i",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "Although we have established that there is positive spatial autocorrelation in our data, we still need to identify the specific spatial patterns. Looking back at Figure 3, you will notice that the plot is divided into four quadrants.\n\nTop-right quadrant: This area represents municipalities that have a higher-than-average share of the population without schooling and are surrounded by other municipalities with similarly high shares of the population without schooling. These are known as high-high clusters.\nBottom-left quadrant: This area represents municipalities with a lower-than-average share of the population without schooling, surrounded by other municipalities with similarly low shares. These are low-low clusters.\nTop-left quadrant: Municipalities with a higher-than-average share of the population without schooling surrounded by municipalities with a lower-than-average share. These are high-low clusters.\nBottom-right quadrant: Municipalities with a lower-than-average share of the population without schooling surrounded by municipalities with a higher-than-average share. These are low-high clusters.\n\nWe can show these area on a map by deconstructing the Moran’s I into a series of local Moran values, each measuring how similar each place is (individually) to its neighbours.\n\n\n\nR code\n\n# local moran's test\nlmoran &lt;- localmoran_perm(sa_municipality$mn_prop_no_schooling, listw = sa_mn_nb_weights,\n    nsim = 999)\n\n# results\nhead(lmoran)\n\n\n            Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1 -0.050379105 -5.007116e-04 0.0055169108 -0.6715285      0.5018839\n2  0.076088895  8.806626e-03 0.0358989035  0.3551077      0.7225089\n3  0.294045408 -6.371539e-04 0.1058262803  0.9058529      0.3650137\n4  0.452492637  4.444755e-03 0.4386494317  0.6764966      0.4987254\n5  0.111902772 -1.280624e-03 0.0195226887  0.8100520      0.4179103\n6  0.002486432  9.338536e-05 0.0005451827  0.1024897      0.9183680\n  Pr(z != E(Ii)) Sim Pr(folded) Sim   Skewness    Kurtosis\n1              0.506          0.253 -0.1147855 -0.23093560\n2              0.714          0.357  0.1969864  0.03091263\n3              0.356          0.178  0.2355106  0.06633588\n4              0.502          0.251  0.1855721 -0.35342466\n5              0.438          0.219  0.1767742 -0.31332065\n6              0.948          0.474 -0.1729386 -0.13325485\n\n\nWe are not given a single statistic as we did with our global Moran’s I, but rather we get a table of different statistics that are all related back to each of the municipalities in our dataset. If we refer to the help page for the localmoran() function, we can find detailed explanations of these statistics. The most relevant ones include:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nIi\nLocal Moran’s I statistic.\n\n\nE.Ii\nExpectation (mean) of the local Moran’s I statistic.\n\n\nVar.Ii\nVariance of local Moran’s I statistic\n\n\nZ.Ii\nStandard deviation (z-score) of the local Moran’s I statistic.\n\n\nPr()\nPseudo \\(p\\)-value of local Moran’s I statistic based on standard deviations and means from the permutation sample.\n\n\nPr() Sim\nPseudo \\(p\\)-value of local Moran’s I statistic based on the rank within the permutation sample, assuming a uniform distribution.\n\n\nPr(Folded) Sim\nPseudo \\(p\\)-value of local Moran’s I statistic based on the rank within the permutation sample using a one-sided test, assuming a uniform distribution.\n\n\n\nWe can further extract the quadrants to which of all these polygons have been assigned:\n\n\n\nR code\n\n# extract quadrants\nlmoran_quadrants &lt;- attr(lmoran, \"quadr\")\n\n# inspect\nhead(lmoran_quadrants)\n\n\n       mean    median     pysal\n1  Low-High  Low-High  Low-High\n2 High-High High-High High-High\n3 High-High High-High High-High\n4 High-High High-High High-High\n5 High-High High-High High-High\n6   Low-Low   Low-Low   Low-Low\n\n\nWe can now link these values back to our spatial dataframe and make a map using the tmap library:\n\n\n\nR code\n\n# replace names\nnames(lmoran_quadrants) &lt;- c(\"lmoran_mean\", \"lmoran_median\", \"lmoran_pysal\")\n\n# bind results\nsa_municipality &lt;- sa_municipality |&gt;\n  cbind(lmoran_quadrants)\n\n# shape, polygons\ntm_shape(sa_municipality) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"lmoran_mean\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\n      \"Low-Low\" = \"#0571b0\",\n      \"Low-High\" = \"#92c5de\",\n      \"High-Low\" = \"#f4a582\",\n      \"High-High\" = \"#ca0020\"\n    ),\n    title = \"Cluster type\",\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n  )\n\n\n\n\n\nFigure 8: Mapping the Local Moran’s I clusters.\n\n\n\n\nThis type of map is called a LISA map and is a great way of showing how a variable is actually clustering over space. However, we can improve on this further by only mapping the statistically significant clusters:\n\n\n\nR code\n\n# replace values if not significant\nlmoran_quadrants[lmoran[, 6] &gt; 0.05, ] &lt;- NA\n\n# replace names\nnames(lmoran_quadrants) &lt;- c(\"lmoran_mean_sig\", \"lmoran_median_sig\", \"lmoran_pysal_sig\")\n\n# bind results\nsa_municipality &lt;- sa_municipality |&gt;\n  cbind(lmoran_quadrants)\n\n# shape, polygons\ntm_shape(sa_municipality) +\n\n  # specify column, colours\n  tm_polygons(\n    col = \"lmoran_mean_sig\",\n    border.col = \"#ffffff\",\n    border.alpha = 0.3,\n    palette = c(\n      \"Low-Low\" = \"#0571b0\",\n      \"Low-High\" = \"#92c5de\",\n      \"High-Low\" = \"#f4a582\",\n      \"High-High\" = \"#ca0020\"\n    ),\n    title = \"Cluster type\",\n  ) +\n\n  # set layout\n  tm_layout(\n    legend.outside = FALSE,\n    legend.position = c(\"left\", \"top\"),\n  )\n\n\n\n\n\nFigure 9: Mapping the significant Local Moran’s I clusters.\n\n\n\n\n\n\n\n\n\n\nThis new map may still not fully address the issue of statistical significance due to repeated testing, and some values may appear significant purely by chance. To correct for this, you can adjust the \\(p\\)-values using R’s p.adjust() function. For further details, refer to Manual Gimond’s explanation of the multiple comparison problem in the context of the pseudo-\\(p\\) values."
  },
  {
    "objectID": "03-spatial-autocorrelation.html#autocorrelation",
    "href": "03-spatial-autocorrelation.html#autocorrelation",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "This concludes this session. Any statistic that includes spatial weights is dependent upon how those weights are defined. We have so far used first order contiguity, i.e. polygons that share a boundary, but there is no particular reason why we should not include second order contiguity polygons (i.e. neighbours of neighbours), use a fixed distance neighbours definitions, or adopt a \\(k\\) nearest neighbours definition.\nPlease try to complete the following tasks:\n\nExtract the centroids from the sa_municipality file.\nIdentify the 5 nearest neighbours for each municipalities, using the knearneigh() function.\nCreate a neigbhour list of these nearest neighbours, using the knn2nb() function.\nCompute the Global Moran’s I of the mn_prop_no_schooling variable using this new neighbourhood definition.\nMap the statistically significant clusters of Local Moran’s I based on this new neighbourhood definition."
  },
  {
    "objectID": "03-spatial-autocorrelation.html#solutions-optional",
    "href": "03-spatial-autocorrelation.html#solutions-optional",
    "title": "1 Spatial Autocorrelation",
    "section": "",
    "text": "There are several ways to achieve the above, but if you would like to see an example solution, you can find it here: [Link]"
  },
  {
    "objectID": "04-spatial-models.html",
    "href": "04-spatial-models.html",
    "title": "1 Spatial Models",
    "section": "",
    "text": "Open a new script within your Geospatial-Workshop24 project and save this as 04-spatial-models.r. As usual, we will begin by loading the necessary libraries. This time, you should load:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(GWmodel)\n\n\n\n\n\n\n\n\nIn the previous session, we explored identifying and measuring patterns of spatial autocorrelation (clustering) in data. If such patterns exist, we can potentially exploit them by creating local summary statistics for geographical areas using the GWmodel library.\nIn this session, we will continue to look at the mn_no_school variable at the municipal level. We will further use a second dataset containing the average age by municipality (mn_avg_age), aggregated from the South African Census Community Profiles 2011. You can download this files below and save it in your project folder under data/attributes.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 Average Age Variable\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nTo download the csv file containing the mn_avg_age variable that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\nOnce downloaded, we can load both files into memory together with our spatial data file:\n\n\n\nR code\n\n# load spatial data\nsa_municipality &lt;- st_read(\"data/spatial/municipality-south-africa-2013.gpkg\")\n\n\nReading layer `municipality-south-africa-2013' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/SA-TIED/data/spatial/municipality-south-africa-2013.gpkg' \n  using driver `GPKG'\nSimple feature collection with 234 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1831416 ymin: -4141363 xmax: 3667419 ymax: -2526543\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# load attribute data\nsa_no_schooling &lt;- read_csv(\"data/attributes/sa-no-schooling.csv\")\n\nRows: 234 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): mn_name\ndbl (3): mn_code, mn_pop, mn_no_school\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load attribute data\nsa_average_age &lt;- read_csv(\"data/attributes/sa-average-age.csv\")\n\nRows: 234 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): mn_name\ndbl (3): mn_code, mn_pop, mn_avg_age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYou can inspect all objects using the View() function.\n\n\n\nLet us start by combining all three datasets:\n\n\n\nR code\n\n# calculate proportions\nsa_no_schooling &lt;- sa_no_schooling |&gt; \n  mutate(mn_no_school_prop = round(mn_no_school/mn_pop, 3)) |&gt;\n  select(mn_code, mn_no_school_prop)\n\n# join attribute data onto spatial data\nsa_municipality &lt;- sa_municipality |&gt;\n  left_join(sa_no_schooling, by = c(\"mn_code\" = \"mn_code\")) |&gt;\n  left_join(sa_average_age, by = c(\"mn_code\" = \"mn_code\"))\n\n\nThe GWmodel library uses the older sp data format for handling spatial data. We therefore need to covnert our current sf object to sp before continuing:\n\n\n\nR code\n\n# to sp\nsa_municipality_sp &lt;- as_Spatial(sa_municipality)\n\n\n\n\n\n\n\n\nThe sf package in R is now favoured over the sp package due to its more modern and efficient handling of spatial data. sf uses simple features, which are a standardised way to store and manipulate spatial geometries, making it more compatible with other geospatial tools and software. It also integrates more seamlessly with the tidyverse ecosystem, allowing for easier data manipulation and analysis. Additionally, sf offers better performance, simpler syntax, and enhanced support for spatial operations compared to sp.\n\n\n\nWe can use the GWmodel to calculate statistics such as local means, local standard deviations, and local variances. As with autocorrelation, this raises the question: what is considered local? One approach is to use a kernel function to determine which values should contribute to the local estimates. These kernels operate on point location data (e.g. polygon centroids) and overlay a window of a specific shape and bandwidth on each point to derive local estimates. The bandwidth, which refers to the kernel’s size, can be measured in absolute terms (e.g. including all polygon centroids within 10km) or relative terms (e.g. including the 10 nearest centroids), with the latter known as an adaptive kernel. While bandwidth typically has a greater impact on density estimation than the kernel type, the choice of kernel can also influence the results by weighting the points within the kernel differently.\n\n\n\n\n\nFigure 1: Kernel types and their distributions.\n\n\n\n\nLet’s calculate the geographically weighted mean value for the mn_no_school_prop and mn_avg_age variables using an adaptive bandwidth of 25 neighbours and a bisquare kernel:\n\n\n\nR code\n\n# geographically weighted statistics: no schooling\nsa_gwss_no_schooling &lt;- gwss(sa_municipality_sp, vars = \"mn_no_school_prop\", bw = 25,\n    kernel = \"bisquare\", adaptive = TRUE, longlat = TRUE)\n\n# geographically weighted statistics: average age\nsa_gwss_average_age &lt;- gwss(sa_municipality_sp, vars = \"mn_avg_age\", bw = 25, kernel = \"bisquare\",\n    adaptive = TRUE, longlat = TRUE)\n\n\n\n\n\n\n\n\nThe results of the outcomes of gwss function can be accessed through the $SDF data frame.\n\n\n\nWe can extract the values and bind these to our original sf dataset as follows:\n\n\n\nR code\n\n# names\nnames(sa_gwss_no_schooling$SDF)\n\n\n[1] \"mn_no_school_prop_LM\"   \"mn_no_school_prop_LSD\"  \"mn_no_school_prop_LVar\"\n[4] \"mn_no_school_prop_LSKe\" \"mn_no_school_prop_LCV\" \n\nnames(sa_gwss_average_age$SDF)\n\n[1] \"mn_avg_age_LM\"   \"mn_avg_age_LSD\"  \"mn_avg_age_LVar\" \"mn_avg_age_LSKe\"\n[5] \"mn_avg_age_LCV\" \n\n# extract local means\nsa_municipality$mn_no_schooling_LM25 &lt;- sa_gwss_no_schooling$SDF$mn_no_school_prop_LM\nsa_municipality$mn_average_age_LM25 &lt;- sa_gwss_average_age$SDF$mn_avg_age_LM\n\nWe can now create a simple map of the local mean values of the mn_no_school variable:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_no_schooling_LM25\",\n    n = 5,\n    style = \"jenks\",\n    title = \"Local Mean\"\n  )\n\n\n\n\n\nFigure 2: Local mean values mn_no_school variable.\n\n\n\n\nAnd we can do the same for the mn_average_age variable:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_average_age_LM25\",\n    n = 5,\n    style = \"jenks\",\n    title = \"Local Mean\"\n  )\n\n\n\n\n\nFigure 3: Local mean values mn_average age variable.\n\n\n\n\nBoth maps show that there is some clear variation in both mean values across the country.\n\n\n\n\n\n\nAs in the previous session, the resulting maps are influenced by the geographical weighting applied, which is largely determined by the bandwidth. There is no definitive answer for selecting the optimal bandwidth, though automatic selectors, such as those based on cross-validation, can help guide the choice (e.g. bw.gwr(mn_no_school_prop ~ 1, data = sa_municipality_sp, adaptive = TRUE, kernel = \"bisquare\", longlat = TRUE)). If you were to run the code above on our current variables, the automatic procedure suggests to consider rather large bandwidths. This suggests that the area we are analysing could be too extensive for detailed local analysis.\n\n\n\n\n\n\nWe can also decompose bivariate relationship by geographical areas. For instance, we can compare the global correlation and break down to what extent the association between two variables is consistent over space. Let us first look at the association between our mn_no_school_prop and mn_avg_age variables:\n\n\n\nR code\n\n# bivariate plot\nplot(sa_municipality$mn_no_school_prop, sa_municipality$mn_avg_age, xlab = \"No schooling\",\n    ylab = \"Average Age\")\n\n\n\n\n\nFigure 4: Scatterplot between the mn_no_school_prop and mn_avg_age variables.\n\n\n\n\n\n\n\nR code\n\n# correlation\ncor(sa_municipality$mn_no_school_prop, sa_municipality$mn_avg_age)\n\n\n[1] -0.5705525\n\n\nFigure 4 and the Pearson correlation coefficient suggests a moderate negative association between the proportion of people without schooling an the average age at the municipal level.\n\n\n\n\n\n\nSometimes heteroscedasticity is indicative of a geographically varying relationship and it is worth considering calculating a geographically weighted correlation.\n\n\n\nUsing the same bandwidth as before, we can derive a localised correlation as follows:\n\n\n\nR code\n\n# geographically weighted statistics: correlation\nsa_gwss_cor &lt;- gwss(sa_municipality_sp, vars = c(\"mn_no_school_prop\", \"mn_avg_age\"),\n    bw = 25, kernel = \"bisquare\", adaptive = TRUE, longlat = TRUE)\n\n\nWe can now extract the values and bind these back to our original sf object:\n\n\n\nR code\n\n# extract correlation\nsa_municipality$mn_noschool_age_cor &lt;- sa_gwss_cor$SDF$Corr_mn_no_school_prop.mn_avg_age\n\n# inspect\nsummary(sa_municipality$mn_noschool_age_cor)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.90804 -0.71461 -0.63113 -0.58882 -0.48593 -0.01354 \n\n\nThe summary shows that there indeed seems to be variation of this relationship across the country, with the local Pearson correlation ranging from very weak to very strong, althoug the direction seems to be consistent. Of course, we can map this in the usual way:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_noschool_age_cor\",\n    n = 5,\n    style = \"jenks\",\n    title = \"Local Correlation\"\n  )\n\n\n\n\n\nFigure 5: Local correlation values mn_no_school and mn_avg_age variable.\n\n\n\n\nWhat the map does not tell us is whether the correlations are significant. We can test for this using a Monte Carlo simulation:\n\n\n\nR code\n\n# test for significance, only select relevant column\nsa_gwss_cor_sig &lt;- gwss.montecarlo(sa_municipality_sp, vars = c(\"mn_no_school_prop\",\n    \"mn_avg_age\"), bw = 25, kernel = \"bisquare\", adaptive = TRUE, longlat = TRUE) |&gt;\n    as_tibble() |&gt;\n    select(Corr_mn_no_school_prop.mn_avg_age)\n\n# replace names\nnames(sa_gwss_cor_sig) &lt;- \"mn_no_school_age_cor_p\"\n\n\nWe can bind the results back to the original sf dataframe. We then can create a column containing only the correlations that show as significant and use tmap to plot these:\n\n\n\nR code\n\n# bind results\nsa_municipality &lt;- sa_municipality |&gt;\n  cbind(sa_gwss_cor_sig) |&gt;\n  mutate(sa_gwss_cor = if_else(mn_no_school_age_cor_p &lt; 0.025, mn_noschool_age_cor,\n    if_else(mn_no_school_age_cor_p &gt; 0.975, mn_noschool_age_cor, NA)\n  ))\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"sa_gwss_cor\",\n    n = 5,\n    style = \"jenks\",\n    title = \"Local Correlation\"\n  )\n\n\n\n\n\nFigure 6: Significant local correlation values mn_no_school and mn_avg_age variable.\n\n\n\n\n\n\n\nWe looked at geographically weighted statistics, including geographically weighted correlation, which examines whether the correlation between two variables varies across space. However, whilst out of the scope of this workshop, these concepts can be extended to regression modelling. Examples of such models are the spatial error model, spatial lag model, and geographically weighted regression.\n\n\nThe spatial error model is used when the error terms in a regression model exhibit spatial autocorrelation, meaning the error terms are not independent across space. This can happen due to omitted variables that have a spatial pattern or unmeasured factors that affect the dependent variable similarly across nearby locations.\nThe model adjusts for spatial autocorrelation by adding a spatially lagged error term (a weighted sum of the errors from neighbouring locations) to the regression equation:\n\\[\ny = X\\beta + \\upsilon, \\upsilon = \\lambda W \\upsilon + \\epsilon\n\\]\nwhere \\(X\\beta\\) represents the standard regression components, \\(\\lambda\\) is a spatial autoregressive parameter, \\(W\\) is a spatial weights matrix, and \\(\\upsilon\\) is a vector of spatially autocorrelated errors.\n\n\n\n\n\n\nSpatial error models can be fitted using R’s spatialreg package.\n\n\n\n\n\n\nThe spatial lag model is appropriate when the dependent variable itself exhibits spatial dependence. This means the value of the dependent variable in one location depends on the values in neighbouring locations. This model is used to capture the spillover effects or diffusion processes, where the outcome in one area is influenced by outcomes in nearby areas (e.g. house prices, crime rates).\nThe model incorporates a spatially lagged dependent variable, which is the weighted sum of the dependent variable values in neighbouring locations, into the regression equation:\n\\[\ny = \\rho Wy + X\\beta + \\epsilon\n\\]\nwhere \\(\\rho\\) is the spatial autoregressive coefficient, \\(Wy\\) represents the spatially lagged dependent variable, and \\(X\\beta\\) represents the standard regression components.\n\n\n\n\n\n\nSpatial lag models can be fitted using R’s spatialreg package.\n\n\n\n\n\n\nGeographically weighted regression (GWR) is used when the relationship between the dependent and independent variables is not constant across space, meaning the model coefficients vary by location. This is useful when you suspect that the relationship between variables may change depending on the geographic context. GWR provides a localised understanding of the relationships by allowing each observation to have its own set of regression coefficients, which can provide insights into how relationships differ across the study area.\nGWR fits a separate regression equation at each location in the study area, weighting nearby observations more heavily than those farther away. The weighting is typically based on a kernel function. The basic GWR equation is:\n\\[\ny_{i} = \\beta_{0}(\\upsilon_{i}, v_{i}) + \\sum_{k=1}^{p}\\beta_{k}(\\upsilon_{i}, v_{i})x_{ik} + \\epsilon_{i}\n\\]\nwhere \\((\\upsilon_{i}, v_{i})\\) are the coordinates of location \\(i\\) and \\(\\beta_{k}(\\upsilon_{i}, v_{i})\\) are the location-specific coefficients. The geographically weighted correlation that we calculated is therefore essentially a univariate GWR.\n\n\n\n\n\n\nThe GWmodel R package can be used to run a GWR, including a multiscale geographically weighted regression (GWRs) that accommodates different bandwidths for each variable. However, these models can be computationally intensive, especially with large datasets.\n\n\n\n\n\n\n\n\n\nTo get started with Geographically Weighted Regression, the Spatial Modelling for Data Scientists course by Liverpool-based Professors Francisco Rowe and Dani Arribas-Bel provides an excellent introduction.\n\n\n\n\n\n\nThe spatial error model adjusts for spatial autocorrelation in the error terms, whereas the spatial lag model adjusts for spatial dependence in the dependent variable itself. The spatial error model does not alter the interpretation of the coefficients of the independent variables, while the spatial lag model introduces a feedback loop where changes in one area can influence neighbouring areas.\nBoth the spatial error and spatial lag models assume that the relationships between variables are the same across the study area, with adjustments made only for spatial dependencies. GWR, on the other hand, allows the relationships themselves to vary across space. GWR is more flexible but also more complex and computationally intensive, providing local instead of global estimates of coefficients.\n\n\n\n\nThis concludes this session. Please try to complete the following tasks:\n\nDownload the three datasets below to your computer. These include a GeoPackage of the 2011 electoral ward boundaries for the Western Cape, a csv file detailing the percentage of the population within each ward at the highest level of education, and a csv file showing the percentage of the population within each ward by income category.\nCreate a new variable in the education dataset representing the percentage of the population within each ward that has at least a higher diploma.\nCreate another variable in the income dataset representing the percentage of the population within each ward that earns at least ZAR 25,601.\nJoin all the data together and then:\n\nCalculate the geographically weighted means for your new education variable, selecting the optimal bandwidth using an automatic selector.\nCalculate the geographically weighted means for your new income variable, selecting the optimal bandwidth using an automatic selector.\nCalculate the geographically weighted correlation between the education and income variables, again selecting the optimal bandwidth using an automatic selector.\n\nMap the geographically weighted correlation coefficients.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nWestern Cape Electoral Wards\nGeoPackage\nDownload\n\n\nSA Census 2011 Income Table\ncsv\nDownload\n\n\nSA Census 2011 Education Table\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nThese datasets have been sourced from the StatsSA Superweb table builder and have subsequently been slightly cleaned.\n\n\n\n\n\n\nThere are several ways to achieve the above, but if you would like to see an example solution, you can find it here: [Link]"
  },
  {
    "objectID": "04-spatial-models.html#loading-spatial-data",
    "href": "04-spatial-models.html#loading-spatial-data",
    "title": "1 Spatial Models",
    "section": "",
    "text": "Open a new script within your Geospatial-Workshop24 project and save this as 04-spatial-models.r. As usual, we will begin by loading the necessary libraries. This time, you should load:\n\n\n\nR code\n\n# load libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(GWmodel)"
  },
  {
    "objectID": "04-spatial-models.html#geographically-weighted-statistics",
    "href": "04-spatial-models.html#geographically-weighted-statistics",
    "title": "1 Spatial Models",
    "section": "",
    "text": "In the previous session, we explored identifying and measuring patterns of spatial autocorrelation (clustering) in data. If such patterns exist, we can potentially exploit them by creating local summary statistics for geographical areas using the GWmodel library.\nIn this session, we will continue to look at the mn_no_school variable at the municipal level. We will further use a second dataset containing the average age by municipality (mn_avg_age), aggregated from the South African Census Community Profiles 2011. You can download this files below and save it in your project folder under data/attributes.\n\n\n\nFile\nType\nLink\n\n\n\n\nSA Census 2011 Average Age Variable\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nTo download the csv file containing the mn_avg_age variable that is hosted on GitHub, click on the Download raw file button on the top right of your screen and it should download directly to your computer.\n\n\n\nOnce downloaded, we can load both files into memory together with our spatial data file:\n\n\n\nR code\n\n# load spatial data\nsa_municipality &lt;- st_read(\"data/spatial/municipality-south-africa-2013.gpkg\")\n\n\nReading layer `municipality-south-africa-2013' from data source \n  `/Users/justinvandijk/Library/CloudStorage/Dropbox/UCL/Web/jtvandijk.github.io/SA-TIED/data/spatial/municipality-south-africa-2013.gpkg' \n  using driver `GPKG'\nSimple feature collection with 234 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1831416 ymin: -4141363 xmax: 3667419 ymax: -2526543\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n# load attribute data\nsa_no_schooling &lt;- read_csv(\"data/attributes/sa-no-schooling.csv\")\n\nRows: 234 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): mn_name\ndbl (3): mn_code, mn_pop, mn_no_school\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# load attribute data\nsa_average_age &lt;- read_csv(\"data/attributes/sa-average-age.csv\")\n\nRows: 234 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): mn_name\ndbl (3): mn_code, mn_pop, mn_avg_age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\nYou can inspect all objects using the View() function.\n\n\n\nLet us start by combining all three datasets:\n\n\n\nR code\n\n# calculate proportions\nsa_no_schooling &lt;- sa_no_schooling |&gt; \n  mutate(mn_no_school_prop = round(mn_no_school/mn_pop, 3)) |&gt;\n  select(mn_code, mn_no_school_prop)\n\n# join attribute data onto spatial data\nsa_municipality &lt;- sa_municipality |&gt;\n  left_join(sa_no_schooling, by = c(\"mn_code\" = \"mn_code\")) |&gt;\n  left_join(sa_average_age, by = c(\"mn_code\" = \"mn_code\"))\n\n\nThe GWmodel library uses the older sp data format for handling spatial data. We therefore need to covnert our current sf object to sp before continuing:\n\n\n\nR code\n\n# to sp\nsa_municipality_sp &lt;- as_Spatial(sa_municipality)\n\n\n\n\n\n\n\n\nThe sf package in R is now favoured over the sp package due to its more modern and efficient handling of spatial data. sf uses simple features, which are a standardised way to store and manipulate spatial geometries, making it more compatible with other geospatial tools and software. It also integrates more seamlessly with the tidyverse ecosystem, allowing for easier data manipulation and analysis. Additionally, sf offers better performance, simpler syntax, and enhanced support for spatial operations compared to sp.\n\n\n\nWe can use the GWmodel to calculate statistics such as local means, local standard deviations, and local variances. As with autocorrelation, this raises the question: what is considered local? One approach is to use a kernel function to determine which values should contribute to the local estimates. These kernels operate on point location data (e.g. polygon centroids) and overlay a window of a specific shape and bandwidth on each point to derive local estimates. The bandwidth, which refers to the kernel’s size, can be measured in absolute terms (e.g. including all polygon centroids within 10km) or relative terms (e.g. including the 10 nearest centroids), with the latter known as an adaptive kernel. While bandwidth typically has a greater impact on density estimation than the kernel type, the choice of kernel can also influence the results by weighting the points within the kernel differently.\n\n\n\n\n\nFigure 1: Kernel types and their distributions.\n\n\n\n\nLet’s calculate the geographically weighted mean value for the mn_no_school_prop and mn_avg_age variables using an adaptive bandwidth of 25 neighbours and a bisquare kernel:\n\n\n\nR code\n\n# geographically weighted statistics: no schooling\nsa_gwss_no_schooling &lt;- gwss(sa_municipality_sp, vars = \"mn_no_school_prop\", bw = 25,\n    kernel = \"bisquare\", adaptive = TRUE, longlat = TRUE)\n\n# geographically weighted statistics: average age\nsa_gwss_average_age &lt;- gwss(sa_municipality_sp, vars = \"mn_avg_age\", bw = 25, kernel = \"bisquare\",\n    adaptive = TRUE, longlat = TRUE)\n\n\n\n\n\n\n\n\nThe results of the outcomes of gwss function can be accessed through the $SDF data frame.\n\n\n\nWe can extract the values and bind these to our original sf dataset as follows:\n\n\n\nR code\n\n# names\nnames(sa_gwss_no_schooling$SDF)\n\n\n[1] \"mn_no_school_prop_LM\"   \"mn_no_school_prop_LSD\"  \"mn_no_school_prop_LVar\"\n[4] \"mn_no_school_prop_LSKe\" \"mn_no_school_prop_LCV\" \n\nnames(sa_gwss_average_age$SDF)\n\n[1] \"mn_avg_age_LM\"   \"mn_avg_age_LSD\"  \"mn_avg_age_LVar\" \"mn_avg_age_LSKe\"\n[5] \"mn_avg_age_LCV\" \n\n# extract local means\nsa_municipality$mn_no_schooling_LM25 &lt;- sa_gwss_no_schooling$SDF$mn_no_school_prop_LM\nsa_municipality$mn_average_age_LM25 &lt;- sa_gwss_average_age$SDF$mn_avg_age_LM\n\nWe can now create a simple map of the local mean values of the mn_no_school variable:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_no_schooling_LM25\",\n    n = 5,\n    style = \"jenks\",\n    title = \"Local Mean\"\n  )\n\n\n\n\n\nFigure 2: Local mean values mn_no_school variable.\n\n\n\n\nAnd we can do the same for the mn_average_age variable:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_average_age_LM25\",\n    n = 5,\n    style = \"jenks\",\n    title = \"Local Mean\"\n  )\n\n\n\n\n\nFigure 3: Local mean values mn_average age variable.\n\n\n\n\nBoth maps show that there is some clear variation in both mean values across the country.\n\n\n\n\n\n\nAs in the previous session, the resulting maps are influenced by the geographical weighting applied, which is largely determined by the bandwidth. There is no definitive answer for selecting the optimal bandwidth, though automatic selectors, such as those based on cross-validation, can help guide the choice (e.g. bw.gwr(mn_no_school_prop ~ 1, data = sa_municipality_sp, adaptive = TRUE, kernel = \"bisquare\", longlat = TRUE)). If you were to run the code above on our current variables, the automatic procedure suggests to consider rather large bandwidths. This suggests that the area we are analysing could be too extensive for detailed local analysis."
  },
  {
    "objectID": "04-spatial-models.html#geographically-weighted-correlation",
    "href": "04-spatial-models.html#geographically-weighted-correlation",
    "title": "1 Spatial Models",
    "section": "",
    "text": "We can also decompose bivariate relationship by geographical areas. For instance, we can compare the global correlation and break down to what extent the association between two variables is consistent over space. Let us first look at the association between our mn_no_school_prop and mn_avg_age variables:\n\n\n\nR code\n\n# bivariate plot\nplot(sa_municipality$mn_no_school_prop, sa_municipality$mn_avg_age, xlab = \"No schooling\",\n    ylab = \"Average Age\")\n\n\n\n\n\nFigure 4: Scatterplot between the mn_no_school_prop and mn_avg_age variables.\n\n\n\n\n\n\n\nR code\n\n# correlation\ncor(sa_municipality$mn_no_school_prop, sa_municipality$mn_avg_age)\n\n\n[1] -0.5705525\n\n\nFigure 4 and the Pearson correlation coefficient suggests a moderate negative association between the proportion of people without schooling an the average age at the municipal level.\n\n\n\n\n\n\nSometimes heteroscedasticity is indicative of a geographically varying relationship and it is worth considering calculating a geographically weighted correlation.\n\n\n\nUsing the same bandwidth as before, we can derive a localised correlation as follows:\n\n\n\nR code\n\n# geographically weighted statistics: correlation\nsa_gwss_cor &lt;- gwss(sa_municipality_sp, vars = c(\"mn_no_school_prop\", \"mn_avg_age\"),\n    bw = 25, kernel = \"bisquare\", adaptive = TRUE, longlat = TRUE)\n\n\nWe can now extract the values and bind these back to our original sf object:\n\n\n\nR code\n\n# extract correlation\nsa_municipality$mn_noschool_age_cor &lt;- sa_gwss_cor$SDF$Corr_mn_no_school_prop.mn_avg_age\n\n# inspect\nsummary(sa_municipality$mn_noschool_age_cor)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.90804 -0.71461 -0.63113 -0.58882 -0.48593 -0.01354 \n\n\nThe summary shows that there indeed seems to be variation of this relationship across the country, with the local Pearson correlation ranging from very weak to very strong, althoug the direction seems to be consistent. Of course, we can map this in the usual way:\n\n\n\nR code\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"mn_noschool_age_cor\",\n    n = 5,\n    style = \"jenks\",\n    title = \"Local Correlation\"\n  )\n\n\n\n\n\nFigure 5: Local correlation values mn_no_school and mn_avg_age variable.\n\n\n\n\nWhat the map does not tell us is whether the correlations are significant. We can test for this using a Monte Carlo simulation:\n\n\n\nR code\n\n# test for significance, only select relevant column\nsa_gwss_cor_sig &lt;- gwss.montecarlo(sa_municipality_sp, vars = c(\"mn_no_school_prop\",\n    \"mn_avg_age\"), bw = 25, kernel = \"bisquare\", adaptive = TRUE, longlat = TRUE) |&gt;\n    as_tibble() |&gt;\n    select(Corr_mn_no_school_prop.mn_avg_age)\n\n# replace names\nnames(sa_gwss_cor_sig) &lt;- \"mn_no_school_age_cor_p\"\n\n\nWe can bind the results back to the original sf dataframe. We then can create a column containing only the correlations that show as significant and use tmap to plot these:\n\n\n\nR code\n\n# bind results\nsa_municipality &lt;- sa_municipality |&gt;\n  cbind(sa_gwss_cor_sig) |&gt;\n  mutate(sa_gwss_cor = if_else(mn_no_school_age_cor_p &lt; 0.025, mn_noschool_age_cor,\n    if_else(mn_no_school_age_cor_p &gt; 0.975, mn_noschool_age_cor, NA)\n  ))\n\n# shape, polygons\ntm_shape(sa_municipality) +\n  # specify column, classes\n  tm_polygons(\n    col = \"sa_gwss_cor\",\n    n = 5,\n    style = \"jenks\",\n    title = \"Local Correlation\"\n  )\n\n\n\n\n\nFigure 6: Significant local correlation values mn_no_school and mn_avg_age variable."
  },
  {
    "objectID": "04-spatial-models.html#spatial-regression",
    "href": "04-spatial-models.html#spatial-regression",
    "title": "1 Spatial Models",
    "section": "",
    "text": "We looked at geographically weighted statistics, including geographically weighted correlation, which examines whether the correlation between two variables varies across space. However, whilst out of the scope of this workshop, these concepts can be extended to regression modelling. Examples of such models are the spatial error model, spatial lag model, and geographically weighted regression.\n\n\nThe spatial error model is used when the error terms in a regression model exhibit spatial autocorrelation, meaning the error terms are not independent across space. This can happen due to omitted variables that have a spatial pattern or unmeasured factors that affect the dependent variable similarly across nearby locations.\nThe model adjusts for spatial autocorrelation by adding a spatially lagged error term (a weighted sum of the errors from neighbouring locations) to the regression equation:\n\\[\ny = X\\beta + \\upsilon, \\upsilon = \\lambda W \\upsilon + \\epsilon\n\\]\nwhere \\(X\\beta\\) represents the standard regression components, \\(\\lambda\\) is a spatial autoregressive parameter, \\(W\\) is a spatial weights matrix, and \\(\\upsilon\\) is a vector of spatially autocorrelated errors.\n\n\n\n\n\n\nSpatial error models can be fitted using R’s spatialreg package.\n\n\n\n\n\n\nThe spatial lag model is appropriate when the dependent variable itself exhibits spatial dependence. This means the value of the dependent variable in one location depends on the values in neighbouring locations. This model is used to capture the spillover effects or diffusion processes, where the outcome in one area is influenced by outcomes in nearby areas (e.g. house prices, crime rates).\nThe model incorporates a spatially lagged dependent variable, which is the weighted sum of the dependent variable values in neighbouring locations, into the regression equation:\n\\[\ny = \\rho Wy + X\\beta + \\epsilon\n\\]\nwhere \\(\\rho\\) is the spatial autoregressive coefficient, \\(Wy\\) represents the spatially lagged dependent variable, and \\(X\\beta\\) represents the standard regression components.\n\n\n\n\n\n\nSpatial lag models can be fitted using R’s spatialreg package.\n\n\n\n\n\n\nGeographically weighted regression (GWR) is used when the relationship between the dependent and independent variables is not constant across space, meaning the model coefficients vary by location. This is useful when you suspect that the relationship between variables may change depending on the geographic context. GWR provides a localised understanding of the relationships by allowing each observation to have its own set of regression coefficients, which can provide insights into how relationships differ across the study area.\nGWR fits a separate regression equation at each location in the study area, weighting nearby observations more heavily than those farther away. The weighting is typically based on a kernel function. The basic GWR equation is:\n\\[\ny_{i} = \\beta_{0}(\\upsilon_{i}, v_{i}) + \\sum_{k=1}^{p}\\beta_{k}(\\upsilon_{i}, v_{i})x_{ik} + \\epsilon_{i}\n\\]\nwhere \\((\\upsilon_{i}, v_{i})\\) are the coordinates of location \\(i\\) and \\(\\beta_{k}(\\upsilon_{i}, v_{i})\\) are the location-specific coefficients. The geographically weighted correlation that we calculated is therefore essentially a univariate GWR.\n\n\n\n\n\n\nThe GWmodel R package can be used to run a GWR, including a multiscale geographically weighted regression (GWRs) that accommodates different bandwidths for each variable. However, these models can be computationally intensive, especially with large datasets.\n\n\n\n\n\n\n\n\n\nTo get started with Geographically Weighted Regression, the Spatial Modelling for Data Scientists course by Liverpool-based Professors Francisco Rowe and Dani Arribas-Bel provides an excellent introduction.\n\n\n\n\n\n\nThe spatial error model adjusts for spatial autocorrelation in the error terms, whereas the spatial lag model adjusts for spatial dependence in the dependent variable itself. The spatial error model does not alter the interpretation of the coefficients of the independent variables, while the spatial lag model introduces a feedback loop where changes in one area can influence neighbouring areas.\nBoth the spatial error and spatial lag models assume that the relationships between variables are the same across the study area, with adjustments made only for spatial dependencies. GWR, on the other hand, allows the relationships themselves to vary across space. GWR is more flexible but also more complex and computationally intensive, providing local instead of global estimates of coefficients."
  },
  {
    "objectID": "04-spatial-models.html#assignment-optional",
    "href": "04-spatial-models.html#assignment-optional",
    "title": "1 Spatial Models",
    "section": "",
    "text": "This concludes this session. Please try to complete the following tasks:\n\nDownload the three datasets below to your computer. These include a GeoPackage of the 2011 electoral ward boundaries for the Western Cape, a csv file detailing the percentage of the population within each ward at the highest level of education, and a csv file showing the percentage of the population within each ward by income category.\nCreate a new variable in the education dataset representing the percentage of the population within each ward that has at least a higher diploma.\nCreate another variable in the income dataset representing the percentage of the population within each ward that earns at least ZAR 25,601.\nJoin all the data together and then:\n\nCalculate the geographically weighted means for your new education variable, selecting the optimal bandwidth using an automatic selector.\nCalculate the geographically weighted means for your new income variable, selecting the optimal bandwidth using an automatic selector.\nCalculate the geographically weighted correlation between the education and income variables, again selecting the optimal bandwidth using an automatic selector.\n\nMap the geographically weighted correlation coefficients.\n\n\n\n\nFile\nType\nLink\n\n\n\n\nWestern Cape Electoral Wards\nGeoPackage\nDownload\n\n\nSA Census 2011 Income Table\ncsv\nDownload\n\n\nSA Census 2011 Education Table\ncsv\nDownload\n\n\n\n\n\n\n\n\n\nThese datasets have been sourced from the StatsSA Superweb table builder and have subsequently been slightly cleaned."
  },
  {
    "objectID": "04-spatial-models.html#solutions-optional",
    "href": "04-spatial-models.html#solutions-optional",
    "title": "1 Spatial Models",
    "section": "",
    "text": "There are several ways to achieve the above, but if you would like to see an example solution, you can find it here: [Link]"
  }
]